2025-08-11 10:52:46.754 [main] INFO  com.yizhaoqi.smartpai.SmartPaiApplication - Starting SmartPaiApplication using Java 17.0.16 with PID 14328 (E:\JavaProject\PaiSmart-main\PaiSmart-main\target\classes started by SongYu in E:\JavaProject\PaiSmart-main)
2025-08-11 10:52:46.756 [main] INFO  com.yizhaoqi.smartpai.SmartPaiApplication - No active profile set, falling back to 1 default profile: "default"
2025-08-11 10:52:47.977 [main] INFO  o.s.boot.web.embedded.tomcat.TomcatWebServer - Tomcat initialized with port 8081 (http)
2025-08-11 10:52:47.985 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8081"]
2025-08-11 10:52:47.986 [main] INFO  org.apache.catalina.core.StandardService - Starting service [Tomcat]
2025-08-11 10:52:47.986 [main] INFO  org.apache.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.34]
2025-08-11 10:52:48.067 [main] INFO  o.a.c.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring embedded WebApplicationContext
2025-08-11 10:52:48.067 [main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1289 ms
2025-08-11 10:52:48.424 [main] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Starting...
2025-08-11 10:52:48.516 [main] INFO  com.zaxxer.hikari.pool.HikariPool - HikariPool-1 - Added connection com.mysql.cj.jdbc.ConnectionImpl@5d10df04
2025-08-11 10:52:48.518 [main] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Start completed.
2025-08-11 10:52:48.548 [main] WARN  org.hibernate.orm.deprecation - HHH90000025: MySQL8Dialect does not need to be specified explicitly using 'hibernate.dialect' (remove the property setting and it will be selected by default)
2025-08-11 10:52:48.549 [main] WARN  org.hibernate.orm.deprecation - HHH90000026: MySQL8Dialect has been deprecated; use org.hibernate.dialect.MySQLDialect instead
2025-08-11 10:52:49.951 [main] DEBUG o.s.web.filter.ServerHttpObservationFilter - Filter 'webMvcObservationFilter' configured for use
2025-08-11 10:52:50.559 [main] WARN  o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration - spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2025-08-11 10:52:50.580 [main] INFO  o.s.s.c.a.a.c.InitializeUserDetailsBeanManagerConfigurer$InitializeUserDetailsManagerConfigurer - Global AuthenticationManager configured with UserDetailsService bean with name customUserDetailsService
2025-08-11 10:52:50.637 [main] DEBUG o.s.w.s.server.support.WebSocketHandlerMapping - Patterns [/chat/{token}] in 'webSocketHandlerMapping'
2025-08-11 10:52:50.685 [main] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - 40 mappings in 'requestMappingHandlerMapping'
2025-08-11 10:52:50.710 [main] DEBUG o.s.web.servlet.handler.SimpleUrlHandlerMapping - Patterns [/webjars/**, /**, /static/**] in 'resourceHandlerMapping'
2025-08-11 10:52:50.818 [main] INFO  o.s.b.actuate.endpoint.web.EndpointLinksResolver - Exposing 1 endpoint beneath base path '/actuator'
2025-08-11 10:52:50.865 [main] INFO  com.yizhaoqi.smartpai.config.SecurityConfig - Security configuration loaded successfully.
2025-08-11 10:52:50.882 [main] DEBUG o.s.security.web.DefaultSecurityFilterChain - Will secure any request with filters: DisableEncodeUrlFilter, WebAsyncManagerIntegrationFilter, SecurityContextHolderFilter, HeaderWriterFilter, LogoutFilter, JwtAuthenticationFilter, OrgTagAuthorizationFilter, RequestCacheAwareFilter, SecurityContextHolderAwareRequestFilter, AnonymousAuthenticationFilter, SessionManagementFilter, ExceptionTranslationFilter, AuthorizationFilter
2025-08-11 10:52:50.937 [main] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerAdapter - ControllerAdvice beans: 0 @ModelAttribute, 0 @InitBinder, 1 RequestBodyAdvice, 1 ResponseBodyAdvice
2025-08-11 10:52:50.968 [main] DEBUG o.s.w.s.m.m.a.ExceptionHandlerExceptionResolver - ControllerAdvice beans: 0 @ExceptionHandler, 1 ResponseBodyAdvice
2025-08-11 10:52:51.252 [main] INFO  org.apache.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8081"]
2025-08-11 10:52:51.258 [main] INFO  o.s.boot.web.embedded.tomcat.TomcatWebServer - Tomcat started on port 8081 (http) with context path '/'
2025-08-11 10:52:51.395 [main] INFO  com.yizhaoqi.smartpai.SmartPaiApplication - Started SmartPaiApplication in 4.998 seconds (process running for 5.506)
2025-08-11 10:52:51.400 [main] INFO  com.yizhaoqi.smartpai.config.AdminUserInitializer - 检查管理员账号是否存在: admin
2025-08-11 10:52:51.475 [main] INFO  com.yizhaoqi.smartpai.config.AdminUserInitializer - 管理员账号 'admin' 已存在，跳过创建步骤
2025-08-11 10:52:51.477 [main] INFO  com.yizhaoqi.smartpai.config.OrgTagInitializer - 检查组织标签是否存在: default
2025-08-11 10:52:51.480 [main] INFO  com.yizhaoqi.smartpai.config.OrgTagInitializer - 组织标签 'default' 已存在，跳过创建步骤
2025-08-11 10:52:51.481 [main] INFO  com.yizhaoqi.smartpai.config.OrgTagInitializer - 检查组织标签是否存在: admin
2025-08-11 10:52:51.482 [main] INFO  com.yizhaoqi.smartpai.config.OrgTagInitializer - 组织标签 'admin' 已存在，跳过创建步骤
2025-08-11 10:52:51.482 [main] INFO  com.yizhaoqi.smartpai.config.OrgTagInitializer - 组织标签初始化完成
2025-08-11 10:52:51.573 [main] INFO  com.yizhaoqi.smartpai.config.EsIndexInitializer - 索引 'knowledge_base' 已存在
2025-08-11 10:52:51.784 [RMI TCP Connection(4)-192.168.1.3] INFO  o.a.c.core.ContainerBase.[Tomcat].[localhost].[/] - Initializing Spring DispatcherServlet 'dispatcherServlet'
2025-08-11 10:52:51.784 [RMI TCP Connection(4)-192.168.1.3] INFO  org.springframework.web.servlet.DispatcherServlet - Initializing Servlet 'dispatcherServlet'
2025-08-11 10:52:51.784 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - Detected StandardServletMultipartResolver
2025-08-11 10:52:51.784 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - Detected AcceptHeaderLocaleResolver
2025-08-11 10:52:51.784 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - Detected FixedThemeResolver
2025-08-11 10:52:51.785 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - Detected org.springframework.web.servlet.view.DefaultRequestToViewNameTranslator@f69187c
2025-08-11 10:52:51.785 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - Detected org.springframework.web.servlet.support.SessionFlashMapManager@2898a250
2025-08-11 10:52:51.785 [RMI TCP Connection(4)-192.168.1.3] DEBUG org.springframework.web.servlet.DispatcherServlet - enableLoggingRequestDetails='false': request parameters and headers will be masked to prevent unsafe logging of potentially sensitive data
2025-08-11 10:52:51.785 [RMI TCP Connection(4)-192.168.1.3] INFO  org.springframework.web.servlet.DispatcherServlet - Completed initialization in 1 ms
2025-08-11 10:53:33.843 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/users/login
2025-08-11 10:53:33.849 [http-nio-8081-exec-1] DEBUG o.s.s.w.a.AnonymousAuthenticationFilter - Set SecurityContextHolder to anonymous SecurityContext
2025-08-11 10:53:33.853 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/users/login
2025-08-11 10:53:33.854 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/users/login", parameters={}
2025-08-11 10:53:33.855 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#login(UserRequest)
2025-08-11 10:53:33.888 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestResponseBodyMethodProcessor - Read "application/json;charset=UTF-8" to [UserRequest[username=admin, password=admin123]]
2025-08-11 10:53:34.426 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Token cached: ab5c9c9dc87c4b9faa8ab3388a50b240 for user: admin
2025-08-11 10:53:34.426 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Token generated and cached for user: admin, tokenId: ab5c9c9dc87c4b9faa8ab3388a50b240
2025-08-11 10:53:34.429 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Refresh token cached: 08fcbe783abe4a15931f7c5e377305ac for user: 1
2025-08-11 10:53:34.429 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Refresh token generated and cached for user: admin, refreshTokenId: 08fcbe783abe4a15931f7c5e377305ac
2025-08-11 10:53:34.434 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:34.434 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={token=eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIi (truncated)...]
2025-08-11 10:53:34.439 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:34.446 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:34.461 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:34.461 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:34.462 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:34.468 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:34.468 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:34.470 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:34.784 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:34.792 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:34.792 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:34.792 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:34.796 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:34.796 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:34.798 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:34.929 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:53:34.939 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:53:34.939 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12", parameters={masked}
2025-08-11 10:53:34.940 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.ConversationController#getConversations(String, String, String)
2025-08-11 10:53:34.957 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:34.957 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{role=user, content=解释一下社区论坛项目, timestamp=2025-08-10T10:40:56}, {role=assistant, co (truncated)...]
2025-08-11 10:53:34.958 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:35.204 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM
2025-08-11 10:53:35.204 [http-nio-8081-exec-5] DEBUG o.s.s.w.a.AnonymousAuthenticationFilter - Set SecurityContextHolder to anonymous SecurityContext
2025-08-11 10:53:35.204 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM
2025-08-11 10:53:35.204 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM", parameters={}
2025-08-11 10:53:35.205 [http-nio-8081-exec-5] DEBUG o.s.w.s.server.support.WebSocketHandlerMapping - Mapped to org.springframework.web.socket.server.support.WebSocketHttpRequestHandler@4f951635
2025-08-11 10:53:35.205 [http-nio-8081-exec-5] DEBUG o.s.w.s.server.support.WebSocketHttpRequestHandler - GET /chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM
2025-08-11 10:53:35.220 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 101 SWITCHING_PROTOCOLS
2025-08-11 10:53:35.227 [http-nio-8081-exec-5] DEBUG o.s.w.s.handler.LoggingWebSocketHandlerDecorator - New StandardWebSocketSession[id=cd209248-acc0-e8ce-b521-37f7a1d71fa2, uri=ws://localhost:8081/chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM]
2025-08-11 10:53:35.228 [http-nio-8081-exec-5] INFO  c.yizhaoqi.smartpai.handler.ChatWebSocketHandler - WebSocket连接已建立，用户ID: admin，会话ID: cd209248-acc0-e8ce-b521-37f7a1d71fa2，URI路径: /chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM
2025-08-11 10:53:36.923 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:36.930 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:36.930 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:36.931 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:36.934 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:36.935 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:36.936 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:37.254 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/org-tags
2025-08-11 10:53:37.261 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/org-tags
2025-08-11 10:53:37.261 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/org-tags", parameters={}
2025-08-11 10:53:37.261 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getUserOrgTags(String)
2025-08-11 10:53:37.268 [http-nio-8081-exec-7] DEBUG com.yizhaoqi.smartpai.service.OrgTagCacheService - Cached organization tags for user: admin
2025-08-11 10:53:37.269 [http-nio-8081-exec-7] DEBUG com.yizhaoqi.smartpai.service.OrgTagCacheService - Cached primary organization for user: admin
2025-08-11 10:53:37.276 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:37.276 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={primaryOrg=default, orgTags=[default, admin], orgTagDetails=[{tagId=default, name=默认组织, descr (truncated)...]
2025-08-11 10:53:37.278 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:40.038 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:40.045 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:40.045 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:40.045 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:40.049 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:40.049 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:40.050 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:40.423 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:40.429 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:40.429 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:53:40.429 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:53:40.443 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:40.443 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:53:40.446 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:46.114 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:46.120 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:46.121 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:46.121 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:46.124 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:46.124 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:46.125 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:46.510 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:46.510 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:46.516 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:46.516 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:53:46.516 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:46.516 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:53:46.516 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:53:46.516 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:53:46.523 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:46.523 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:53:46.525 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:46.537 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:46.537 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:53:46.538 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:46.842 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:46.847 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:46.847 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:53:46.847 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:53:46.856 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:46.856 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:53:46.858 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:52.945 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:52.951 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:52.951 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:52.952 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:52.955 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:52.955 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:52.956 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:53.209 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:53.215 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:53.215 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:53:53.215 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:53:53.224 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:53.225 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:53:53.226 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:54.745 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:54.751 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:54.751 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:54.751 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:54.754 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:54.754 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:54.755 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:55.376 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/org-tags
2025-08-11 10:53:55.381 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/org-tags
2025-08-11 10:53:55.381 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/org-tags", parameters={}
2025-08-11 10:53:55.381 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getUserOrgTags(String)
2025-08-11 10:53:55.392 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:55.392 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={primaryOrg=default, orgTags=[default, admin], orgTagDetails=[{tagId=default, name=默认组织, descr (truncated)...]
2025-08-11 10:53:55.393 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:58.442 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:58.451 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:58.451 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:58.451 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:58.457 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:58.457 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:58.458 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:59.087 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:59.087 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:59.093 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:59.093 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:53:59.093 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:53:59.093 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:53:59.093 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:53:59.093 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:53:59.099 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:59.099 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:53:59.100 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:59.100 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:53:59.100 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:59.101 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:59.334 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:59.346 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:53:59.346 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:53:59.346 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:53:59.355 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:59.355 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:53:59.356 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:59.416 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:53:59.421 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:53:59.421 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:53:59.421 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:53:59.424 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:59.424 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:53:59.425 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:53:59.653 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/org-tags
2025-08-11 10:53:59.657 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/org-tags
2025-08-11 10:53:59.657 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/org-tags", parameters={}
2025-08-11 10:53:59.657 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getUserOrgTags(String)
2025-08-11 10:53:59.663 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:53:59.663 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={primaryOrg=default, orgTags=[default, admin], orgTagDetails=[{tagId=default, name=默认组织, descr (truncated)...]
2025-08-11 10:53:59.664 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:03.883 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:54:03.888 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:54:03.888 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:54:03.889 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:54:03.891 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:03.891 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:54:03.892 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:04.133 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:54:04.138 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:54:04.138 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:54:04.138 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:54:04.148 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:04.148 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:54:04.149 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:04.211 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:54:04.211 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:54:04.215 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:54:04.215 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 10:54:04.216 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:54:04.216 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 10:54:04.216 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:54:04.216 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:54:04.223 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:04.223 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:54:04.223 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:04.223 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 10:54:04.224 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:04.224 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:05.705 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:54:05.711 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:54:05.711 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:54:05.711 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:54:05.715 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:05.715 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:54:05.716 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:06.336 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/org-tags
2025-08-11 10:54:06.342 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/org-tags
2025-08-11 10:54:06.342 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/org-tags", parameters={}
2025-08-11 10:54:06.342 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getUserOrgTags(String)
2025-08-11 10:54:06.349 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:06.349 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={primaryOrg=default, orgTags=[default, admin], orgTagDetails=[{tagId=default, name=默认组织, descr (truncated)...]
2025-08-11 10:54:06.350 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:54.650 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/users/login
2025-08-11 10:54:54.650 [http-nio-8081-exec-10] DEBUG o.s.s.w.a.AnonymousAuthenticationFilter - Set SecurityContextHolder to anonymous SecurityContext
2025-08-11 10:54:54.650 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/users/login
2025-08-11 10:54:54.650 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/users/login", parameters={}
2025-08-11 10:54:54.650 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#login(UserRequest)
2025-08-11 10:54:54.651 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestResponseBodyMethodProcessor - Read "application/json;charset=UTF-8" to [UserRequest[username=admin, password=admin123]]
2025-08-11 10:54:54.709 [http-nio-8081-exec-10] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Token cached: ace70adff6684f2494a18f5b8704c0c0 for user: admin
2025-08-11 10:54:54.709 [http-nio-8081-exec-10] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Token generated and cached for user: admin, tokenId: ace70adff6684f2494a18f5b8704c0c0
2025-08-11 10:54:54.713 [http-nio-8081-exec-10] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Refresh token cached: d28a69854c334ab894f46f0ba6829e4b for user: 1
2025-08-11 10:54:54.713 [http-nio-8081-exec-10] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Refresh token generated and cached for user: admin, refreshTokenId: d28a69854c334ab894f46f0ba6829e4b
2025-08-11 10:54:54.713 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:54.713 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={token=eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIi (truncated)...]
2025-08-11 10:54:54.713 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:55.022 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:54:55.027 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:54:55.027 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:54:55.027 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:54:55.030 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:55.030 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:54:55.030 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:55.283 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:54:55.289 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:54:55.289 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:54:55.289 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:54:55.292 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:55.292 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:54:55.292 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:55.347 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/org-tags
2025-08-11 10:54:55.351 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/org-tags
2025-08-11 10:54:55.352 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/org-tags", parameters={}
2025-08-11 10:54:55.352 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getUserOrgTags(String)
2025-08-11 10:54:55.357 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:55.357 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={primaryOrg=default, orgTags=[default, admin], orgTagDetails=[{tagId=default, name=默认组织, descr (truncated)...]
2025-08-11 10:54:55.358 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:58.514 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:54:58.518 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:54:58.519 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:54:58.519 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:54:58.522 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:58.522 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:54:58.523 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:54:59.345 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/documents/uploads
2025-08-11 10:54:59.349 [http-nio-8081-exec-5] INFO  c.y.smartpai.config.OrgTagAuthorizationFilter - 处理获取用户文档请求: /api/v1/documents/uploads
2025-08-11 10:54:59.350 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/documents/uploads
2025-08-11 10:54:59.350 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/documents/uploads", parameters={}
2025-08-11 10:54:59.350 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.DocumentController#getUserUploadedFiles(String)
2025-08-11 10:54:59.351 [http-nio-8081-exec-5] INFO  com.yizhaoqi.smartpai.service.DocumentService - 获取用户上传的文件列表: userId=1
2025-08-11 10:54:59.353 [http-nio-8081-exec-5] INFO  com.yizhaoqi.smartpai.service.DocumentService - 成功获取用户上传的文件列表: userId=1, fileCount=1
2025-08-11 10:54:59.357 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:54:59.357 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{createdAt=2025-08-10T14:53:26.801858, fileName=国家电网·必会考点-计算机类.pdf, totalSize=15397 (truncated)...]
2025-08-11 10:54:59.358 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:10.515 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:55:10.522 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:55:10.522 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:55:10.522 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:55:10.525 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:10.525 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:55:10.526 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:10.846 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:55:10.851 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:55:10.851 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:55:10.852 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:55:10.858 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:10.858 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:55:10.859 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:15.710 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:55:15.715 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:55:15.715 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:55:15.715 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:55:15.717 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:15.717 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:55:15.719 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:16.051 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/documents/uploads
2025-08-11 10:55:16.055 [http-nio-8081-exec-9] INFO  c.y.smartpai.config.OrgTagAuthorizationFilter - 处理获取用户文档请求: /api/v1/documents/uploads
2025-08-11 10:55:16.056 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/documents/uploads
2025-08-11 10:55:16.056 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/documents/uploads", parameters={}
2025-08-11 10:55:16.056 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.DocumentController#getUserUploadedFiles(String)
2025-08-11 10:55:16.057 [http-nio-8081-exec-9] INFO  com.yizhaoqi.smartpai.service.DocumentService - 获取用户上传的文件列表: userId=1
2025-08-11 10:55:16.058 [http-nio-8081-exec-9] INFO  com.yizhaoqi.smartpai.service.DocumentService - 成功获取用户上传的文件列表: userId=1, fileCount=1
2025-08-11 10:55:16.061 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:16.061 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{createdAt=2025-08-10T14:53:26.801858, fileName=国家电网·必会考点-计算机类.pdf, totalSize=15397 (truncated)...]
2025-08-11 10:55:16.062 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:18.741 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 10:55:18.747 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 10:55:18.747 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 10:55:18.747 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 10:55:18.752 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:18.752 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 10:55:18.754 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:35.664 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/upload/chunk
2025-08-11 10:55:35.668 [http-nio-8081-exec-2] INFO  c.y.smartpai.config.OrgTagAuthorizationFilter - 处理分片上传请求: /api/v1/upload/chunk
2025-08-11 10:55:35.669 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/upload/chunk
2025-08-11 10:55:35.669 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/upload/chunk", parameters={multipart}
2025-08-11 10:55:35.738 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UploadController#uploadChunk(String, int, long, String, Integer, String, boolean, MultipartFile, String)
2025-08-11 10:55:35.740 [http-nio-8081-exec-2] DEBUG c.y.smartpai.service.FileTypeValidationService - 开始验证文件类型: fileName=牛客论坛项目总结.pdf
2025-08-11 10:55:35.740 [http-nio-8081-exec-2] DEBUG c.y.smartpai.service.FileTypeValidationService - 文件类型识别结果: fileName=牛客论坛项目总结.pdf, extension=pdf, fileType=PDF文档
2025-08-11 10:55:35.740 [http-nio-8081-exec-2] INFO  c.y.smartpai.service.FileTypeValidationService - 文件类型验证通过: fileName=牛客论坛项目总结.pdf, extension=pdf, fileType=PDF文档
2025-08-11 10:55:35.740 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - [uploadChunk] 开始处理分片上传请求 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0, totalSize: 3761691, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, contentType: application/octet-stream, fileSize: 3761691, orgTag: PRIVATE_sy, isPublic: true, userId: 1
2025-08-11 10:55:35.742 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 检查文件记录是否存在 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, exists: false
2025-08-11 10:55:35.742 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 创建新的文件记录 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, totalSize: 3761691, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:35.769 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件记录创建成功 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:35.769 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 检查分片是否已上传 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0
2025-08-11 10:55:35.770 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片上传状态 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0, isUploaded: false
2025-08-11 10:55:35.770 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 检查分片是否已上传 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0, isUploaded: false
2025-08-11 10:55:35.772 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 检查数据库中分片信息 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0, exists: false
2025-08-11 10:55:35.772 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 计算分片MD5 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0
2025-08-11 10:55:35.786 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片MD5计算完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0, chunkMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.786 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 构建分片存储路径 => fileName: 牛客论坛项目总结.pdf, path: chunks/c8f8cebf90c764b93d862694096a2af9/0
2025-08-11 10:55:35.786 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始上传分片到MinIO => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, chunkIndex: 0, bucket: uploads, path: chunks/c8f8cebf90c764b93d862694096a2af9/0, size: 3761691, contentType: application/octet-stream
2025-08-11 10:55:35.948 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片上传到MinIO成功 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, chunkIndex: 0
2025-08-11 10:55:35.948 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 标记分片为已上传 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0
2025-08-11 10:55:35.948 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 标记分片为已上传 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0
2025-08-11 10:55:35.950 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片已标记为已上传 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0
2025-08-11 10:55:35.950 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片标记完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0
2025-08-11 10:55:35.950 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 保存分片信息到数据库 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0, chunkMd5: c8f8cebf90c764b93d862694096a2af9, storagePath: chunks/c8f8cebf90c764b93d862694096a2af9/0
2025-08-11 10:55:35.950 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 保存分片信息到数据库 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0, chunkMd5: c8f8cebf90c764b93d862694096a2af9, storagePath: chunks/c8f8cebf90c764b93d862694096a2af9/0
2025-08-11 10:55:35.954 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片信息已保存 => fileMd5: c8f8cebf90c764b93d862694096a2af9, chunkIndex: 0
2025-08-11 10:55:35.954 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片信息已保存到数据库 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, chunkIndex: 0
2025-08-11 10:55:35.954 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片处理完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, chunkIndex: 0
2025-08-11 10:55:35.954 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 获取已上传分片列表 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.954 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 计算文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.956 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件总分片数计算结果 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalSize: 3761691, chunkSize: 5242880, totalChunks: 1
2025-08-11 10:55:35.956 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.UploadService - 文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalChunks: 1
2025-08-11 10:55:35.971 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 获取到已上传分片列表 => fileMd5: c8f8cebf90c764b93d862694096a2af9, 已上传数量: 1, 总分片数: 1, 优化方式: 一次性获取
2025-08-11 10:55:35.971 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 计算文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.972 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件总分片数计算结果 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalSize: 3761691, chunkSize: 5242880, totalChunks: 1
2025-08-11 10:55:35.974 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:35.974 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data={uploaded=[0], progress=100.0}, message=分片上传成功}]
2025-08-11 10:55:35.977 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:35.981 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/upload/merge
2025-08-11 10:55:35.985 [http-nio-8081-exec-1] INFO  c.y.smartpai.config.OrgTagAuthorizationFilter - 处理合并分片请求: /api/v1/upload/merge
2025-08-11 10:55:35.986 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/upload/merge
2025-08-11 10:55:35.986 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/upload/merge", parameters={}
2025-08-11 10:55:35.986 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UploadController#mergeFile(MergeRequest, String)
2025-08-11 10:55:35.988 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestResponseBodyMethodProcessor - Read "application/json;charset=UTF-8" to [MergeRequest[fileMd5=c8f8cebf90c764b93d862694096a2af9, fileName=牛客论坛项目总结.pdf]]
2025-08-11 10:55:35.990 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 获取已上传分片列表 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.990 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 计算文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.991 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件总分片数计算结果 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalSize: 3761691, chunkSize: 5242880, totalChunks: 1
2025-08-11 10:55:35.991 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalChunks: 1
2025-08-11 10:55:35.991 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 获取到已上传分片列表 => fileMd5: c8f8cebf90c764b93d862694096a2af9, 已上传数量: 1, 总分片数: 1, 优化方式: 一次性获取
2025-08-11 10:55:35.991 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 计算文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.992 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件总分片数计算结果 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalSize: 3761691, chunkSize: 5242880, totalChunks: 1
2025-08-11 10:55:35.992 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始合并文件分片 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:35.992 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 查询分片信息 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf
2025-08-11 10:55:35.994 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 查询到分片信息 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, 分片数量: 1
2025-08-11 10:55:35.994 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 计算文件总分片数 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:35.995 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件总分片数计算结果 => fileMd5: c8f8cebf90c764b93d862694096a2af9, totalSize: 3761691, chunkSize: 5242880, totalChunks: 1
2025-08-11 10:55:35.995 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片路径列表 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, 路径数量: 1
2025-08-11 10:55:35.995 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始检查每个分片是否存在 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:36.004 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片存在 => fileName: 牛客论坛项目总结.pdf, index: 0, path: chunks/c8f8cebf90c764b93d862694096a2af9/0, size: 3761691
2025-08-11 10:55:36.004 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片检查完成，所有分片都存在 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:36.004 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始合并分片 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, 合并后路径: merged/牛客论坛项目总结.pdf
2025-08-11 10:55:36.004 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 构建合并请求 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, targetPath: merged/牛客论坛项目总结.pdf, sourcePaths: [chunks/c8f8cebf90c764b93d862694096a2af9/0]
2025-08-11 10:55:36.036 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片合并成功 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, mergedPath: merged/牛客论坛项目总结.pdf
2025-08-11 10:55:36.043 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 合并文件信息 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, path: merged/牛客论坛项目总结.pdf, size: 3761691
2025-08-11 10:55:36.043 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始清理分片文件 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, 分片数量: 1
2025-08-11 10:55:36.048 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 分片文件已删除 => fileName: 牛客论坛项目总结.pdf, path: chunks/c8f8cebf90c764b93d862694096a2af9/0
2025-08-11 10:55:36.048 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片文件清理完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:36.048 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 删除Redis中的分片状态记录 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf
2025-08-11 10:55:36.048 [http-nio-8081-exec-1] DEBUG com.yizhaoqi.smartpai.service.UploadService - 删除文件所有分片上传标记 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:36.050 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件分片上传标记已删除 => fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:36.050 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 分片状态记录已删除 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf
2025-08-11 10:55:36.050 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 更新文件状态为已完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:36.052 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 文件状态已更新为已完成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档
2025-08-11 10:55:36.052 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 开始生成预签名URL => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, path: merged/牛客论坛项目总结.pdf
2025-08-11 10:55:36.053 [http-nio-8081-exec-1] INFO  com.yizhaoqi.smartpai.service.UploadService - 预签名URL已生成 => fileMd5: c8f8cebf90c764b93d862694096a2af9, fileName: 牛客论坛项目总结.pdf, fileType: PDF文档, URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:36.224 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Received task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
2025-08-11 10:55:36.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件权限信息: userId=1, orgTag=PRIVATE_sy, isPublic=true
2025-08-11 10:55:36.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Downloading file from storage: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:36.226 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Detected remote URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:36.231 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:55:36.231 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data={object_url=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9 (truncated)...]
2025-08-11 10:55:36.232 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:55:36.237 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Successfully connected to URL, starting download...
2025-08-11 10:55:36.237 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 开始解析文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文件元数据:
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:unmappedUnicodeCharsPerPage: 0
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:PDFVersion: 1.7
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmp:CreatorTool: WPS 文字
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXFA: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:modify_annotations: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print_degraded: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By-Full-Set: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:creator: SongYu
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:num3DAnnotations: 0
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:format: application/pdf; version=1.7
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator_tool: WPS 文字
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:overallPercentageUnmappedUnicodeChars: 0.0
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:fill_in_form: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasCollection: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:encrypted: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsNonEmbeddedFont: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:custom:SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasMarkedContent: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - Content-Type: application/pdf
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator: SongYu
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:totalUnmappedUnicodeChars: 0
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_for_accessibility: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:assemble_document: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmpTPg:NPages: 133
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXMP: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:charsPerPage: 1441
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_content: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:trapped: False
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_modify: true
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsDamagedFont: false
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 提取的文本内容长度: 121403
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆状态的检查、为游客和已登录用户展示不同界面与功能。实现不同用户的权限控制和网站数据统计(UV、DAU)，管理员可以查看网站数据统计和网站监控信息。支持用户上传头像，实现发布帖子、评论帖子、热帖排行、发送私信与敏感词过滤等功能。实现了点赞关注与系统通知功能。支持全局搜索帖子信息的功能。核心功能具体实现1. 通过对登录用户颁发登录凭证，将登陆凭证存进 Redis 中来记录登录用户登录状态，使用拦截器进行登录状态检查，使用 Spring Security 实现权限控制，解决了 http 无状态带来的缺陷，保护需登录或权限才能使用的特定资源。（登入时将生成的 Ticket存入 Redies, 然后在登入请求成功时，将 Redies中的Ticket存入新建的 Cookie 中，然后反馈给浏览器，随后在该浏览器访问其他请求时，会先经过 LoginTicketInterceptor，判断请求中是否有 Ticket，是否和 Redies中的 T
2025-08-11 10:55:37.053 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: icket一致，如果一致会将用户信息存入到 hostHolder（属于线程局部缓存）中，以便后续在请求处理过程中可以方便地获取到当前登录用户的信息，在请求处理完成后，清除当前线程中存储的用户信息。通过这种方式，确保每个请求都是独立处理的，不会因为线程复用而导致用户信息泄露或混淆。 注意可以将用户信息存入 Redis缓存中来减少 DB 的访问量，但是当用户数据更新时，必须即使删除 Redis中的用户数据，以保证数据的一致性和准确性。Spring Security 的用户认证是在自定义的过滤器中，也是获取请求 Cookie 中的 Ticket 和 Redis中的Ticket是否一致，然后将认证用户存到安全上下文中，在 Security 配置类中根据安全上下文获取用户信息，判断用户对各个资源的访问权限。Security 认证应当放到过滤器中而不是拦截器，因为过滤器比拦截器先执行，在拦截器中配置安全上下文会导致 Security 配置类获取不到用户信息，因为此时还没执行拦截器。）2. 使用 ThreadLocal 在当前线程中存储用户数据，代替 session 的功能便于分布式部署。在拦截器的 preHandle 中
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存储用户数据，在 postHandle 中将用户数据存入 Model，在 afterCompletion 中清理用户数据。（ThreadLocal 为每个线程提供独立的变量副本。每个线程操作自己的副本，互不干扰。在 Web 应用中，一个请求从开始到结束通常由同一个线程处理。因此，在拦截器的 preHandle 中存储的数据，可在整个请求链路（Controller、Service、Dao）中通过 ThreadLocal 获取。 在分布式部署中，由于 Session需要共享，使用 ThreadLocal存储用户数据，我们并不需要在多个服务器之间共享这些数据。因为每个请求都是独立的，处理完一个请求后，数据就被清除了。所以，在分布式环境下，我们只需要确保每个服务器能够独立处理请求即可，不需要考虑多个服务器之间的 Session同步问题。）3. 使用 Redis 的集合数据类型来解决踩赞、相互关注功能，采用事务管理，保证数据的正确，采用“先更新数据库，再删除缓存”策略保证数据库与缓存数据的一致性。采用 Redis 存储验证码，解决性能问题和分布式部署时的验证码需求。采用 Redis 的 HyperLogLog 存储每日
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  UV、Bitmap 存储 DAU，实现网站数据统计的需求。（使用 RedisTemplate执行一个 Redis 事务。SessionCallback 接口的 execute方法会在一个事务中执行所有操作，确保操作的原子性。通过调用 multi()方法，开启一个 Redis事务。在这个事务中执行的命令会被缓存，直到 exec()方法被调用时才会一次性提交。数据结构：HyperLogLog，12KB 内存可计算 2^64 个不重复元素 误差率仅 0.81% 数据结构：Bitmap 一连串二进制数组，可以进行二值状态统计）4. 使用 Kafka 作为消息队列，在用户被点赞、评论、关注后以系统通知的方式推送给用户，用户发布或删除帖子后向 elasticsearch 同步，对系统进行解耦、削峰。（在这个系统中 kafka 就办了三件事，一是用户在被点赞、评论、关注后会借助kafka 消费者来异步的生成系统通知，二三是在用户发布帖子和删除帖子时，将内容添加到 elasticsearch或者从 elasticsearch 中删除）5. 使用 elasticsearch + ik 分词插件实现全局搜索功能，当用户发布、修
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 改或删除帖子时，使用 Kafka 消息队列去异步将帖子信息给 elasticsearch 同步。（Elasticsearch（简称 ES）是一个开源的分布式搜索和分析引擎，它专为处理海量数据设计，提供近实时的全文搜索能力。IK Analyzer 是专为中文设计的开源分词插件，解决中文文本分析的核心难题。ik_smart：粗粒度切分 ik_max_word 细粒度切分）6. 使用分布式定时任务 Quartz 定时计算帖子分数，来实现热帖排行的业务功能。对频繁需要访问的数据，如用户信息、帖子总数、热帖的单页帖子列表，使用Caffeine 本地缓存 + Redis 分布式缓存的多级缓存，提高服务器性能，实现系统的高可用。（上面三个部分就是 Quartz的基本组成部分：调度器：Scheduler任务：JobDetail触发器：Trigger，包括 SimpleTrigger 和 CronTrigger定义一个 Quartz定时任务及其触发器。具体来说，它配置了一个名为postScoreRefreshJob的任务，该任务属于 communityJobGroup 组，并且被设置为持久化和请求恢复。同时，它还配置了一个名为
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  postScoreRefreshTrigger 的触发器，该触发器也属于 communityTriggerGroup组，并且每 5 分钟触发一次postScoreRefreshJob任务。因此，这段代码的主要目的是确保 PostScoreRefreshJob类中定义的任务每 5分钟执行一次）核心技术Spring Boot、SSMRedis、Kafka、ElasticsearchSpring Security、Quartz、Caffeine项目亮点项⽬构建在 Spring Boot+SSM 框架之上，并统⼀的进⾏了状态管理、事务管理、异常处理；利⽤ Redis 实现了点赞和关注功能，单机可达 5000TPS；利⽤ Kafka 实现了异步的站内通知，单机可达 7000TPS；利⽤ Elasticsearch 实现了全⽂搜索功能，可准确匹配搜索结果，并⾼亮显示关键词；利⽤ Caffeine+Redis 实现了两级缓存，并优化了热⻔帖⼦的访问，单机可达8000QPS。利⽤ Spring Security 实现了权限控制，实现了多重⻆⾊、URL 级别的权限管理；利⽤ HyperLogLog、Bitmap 分别实现了 
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: UV、DAU 的统计功能，100 万⽤户数据只需*M 内存空间；利⽤ Quartz 实现了任务调度功能，并实现了定时计算帖⼦分数、定时清理垃圾⽂件等功能；利⽤ Actuator 对应⽤的 Bean、缓存、⽇志、路径等多个维度进⾏了监控，并通过⾃定义的端点对数据库连接进⾏了监控。面试题：1.你提到使用 Spring Security 实现权限控制。能具体说明如何整合登录凭证（Redis存储）与 Spring Security？如何实现 URL 级别的动态权限管理？答：用户登入成功时系统会生成一个 Ticket并存入 Redis，新建一个 cookie 存入Ticket；在自定义的过滤器中验证请求携带的 Ticket与 Redis内的 Ticket是否一致，构建 Authentication对象存入 SecurityContextHolder；在 Security 的配置类中对固定 URL 如/admin/**）使用 antMatchers().hasRole("ADMIN")，即根据用户权限赋予访问资源的能力。2. 你提到点赞功能采用‘先更新 DB 再删缓存’策略。如果删除缓存失败导致不一致，如何解决？为何不用
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ‘更新缓存’方案？答：“删除失败兜底方案：设置缓存短过期时间（如 30s），容忍短期不一致异步重试：将失败操作推入 Kafka，消费者重试删除监听 MySQL Binlog（如 Canal）触发缓存删除不更新缓存的原因：写冲突： 并发更新可能导致缓存脏数据（如线程 A更新 DB 后未更新缓存时，线程 B又更新）浪费资源： 频繁更新但低读取的数据会占用带宽复杂度： 需维护缓存与 DB的强一致性逻辑（如分布式锁），而删除策略更简单可靠。”3. 系统通知使用 Kafka异步推送。如果通知发送失败（如网络抖动），如何保证用户最终能收到通知？答：“我们通过三级保障实现可靠性：生产者确认： 设置 Kafka acks=all，确保消息写入所有副本；消费者容错：开启手动提交 Offset，业务处理成功后才提交捕获异常后重试（如 3次），仍失败则存入死信队列补偿机制：定时任务扫描未通知记录（DB状态标记）重新投递死信队列消息人工介入处理此外，消息体包含唯一 ID 防重复消费。”4. 热帖列表用了 Caffeine+Redis两级缓存。如何解决缓存穿透？如何同步本地缓存（Caffeine）的数据？答：“缓存穿透防护：布隆过滤器
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ：将所有的 key 提前存入布隆过滤器，在访问缓存层之前，查询前校验 Key 是否存在，不存在返回空值。缓存空值：对不存在的帖子 ID 缓存 NULL（短过期时间）本地缓存同步过期同步： Caffeine设置 refreshAfterWrite=30s自动刷新主动推送： 当帖子更新时，通过 Redis Pub/Sub 广播失效事件，节点监听后删除本地缓存兜底策略： 本地缓存过期时间短于 Redis（如本地 60s vs Redis 300s），确保最终一致。”5. 定时计算帖子分数时，如何避免分布式环境下的重复执行？如果计算耗时过长导致阻塞，如何优化？答：“防重复执行：使用 Quartz集群模式：数据库锁（QRTZ_LOCKS 表）保证同一任务仅一个节点执行性能优化：分片处理： 按帖子 ID 范围分片（如 0-10000, 10001-20000），多线程并行计算增量计算： 仅扫描最近 X 小时变化的帖子（如 last_modified_time > now()-6h）异步化： 将计算任务拆解为多个子任务投递到 Kafka，消费者并发处理降级策略： 超时后记录断点，下次任务从断点继续。”6. 你使用 Elas
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ticsearch 实现全文搜索并高亮关键词。请说明：如何设计索引映射（Mapping）以优化搜索效率和准确性？如何实现搜索结果的高亮显示？遇到 HTML 标签转义问题如何处理？搜索性能瓶颈可能在哪里？如何优化？答: 1. 索引映射设计：分词策略： 对帖子标题和内容字段使用 ik_max_word 分词器进行细粒度分词（索引时），搜索时结合 ik_smart 提高相关性。字段类型： 标题用 text（分词） + keyword（不分词，用于精确匹配/聚合），ID 用 keyword，发布时间用 date。副本分片： 设置合理副本数（如 1-2）提高查询吞吐量和容错性。关闭不必要特性： 对不需聚合/排序的字段关闭 doc_values 节省存储。2. 高亮实现与转义：高亮请求： 在搜索请求中添加 highlight 部分，指定字段、pre_tags（如 <em>）、post_tags（如 </em>）。HTML 转义： ES 默认会转义高亮片段中的 HTML。我们确保存入 ES 的内容是纯文本（不含用户输入的原始 HTML），避免 XSS 同时解决转义混乱。前端渲染高亮片段时使用 textContent 而非 
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: innerHTML。3. 性能优化点：瓶颈： 复杂查询（多条件+聚合）、深度分页（from + size 过大）、索引设计不佳。优化措施：避免深度分页：使用 search_after + 唯一排序值（如 ID+时间戳）替代 from/size。限制查询范围： 使用 filter 缓存（如时间范围、状态）减少 query 计算量。冷热数据分离： 历史数据迁移到低性能节点或归档索引。合理硬件： SSD、充足内存（ES 堆内存 ≈ 50% 物理内存，不超过 31GB）。7. 项目用 HyperLogLog (HLL) 统计 UV，Bitmap 统计 DAU。请解释：HLL 如何用极小空间估算大基数？它的误差范围是多少？Bitmap 如何统计 DAU？如何解决用户 ID 非连续导致的空间浪费？如果某天 UV 突增，HLL 合并结果会怎样？如何验证其准确性？答：“1. HLL 原理与误差：原理： 对每个用户 ID 做哈希，计算哈希值二进制表示中 ‘1’ 的最高位位置（如 0001... 最高位=4），维护一个 ‘寄存器数组’ 记录每个桶的最大位置。最终通过调和平均数估算基数。核心是利用概率分布。误差： Redis 的 
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: PF 实现标准误差约 0.81%（使用 16384 个寄存器时）。空间仅需 12KB（固定大小）。2. Bitmap 与 DAU：实现： 每天一个 Bitmap Key（如 dau:20230702），用户 ID 作为 Offset，访问则设位为 1。BITCOUNT 获取当日活跃用户数。稀疏优化： 使用 RLE (Run-Length Encoding) 压缩的 Bitmap 库（如 RoaringBitmap）或 Redis 的 BITFIELD 命令动态管理非连续 ID，避免传统 Bitmap 的空间浪费。3. HLL 突增与验证：突增影响：HLL 是基数估计，突增时估算值会上升，误差仍在理论范围内（0.81%）。合并多个 HLL（如按小时合并成天）误差会累积但可控。验证： 定期抽样对比：对某小段时间用 SET 精确计算 UV，与 HLL 结果对比，监控误差是否符合预期。业务上接受近似值是其使用前提。8. 敏感词过滤是社区必备功能。你如何实现它？如何平衡过滤效率和敏感词库的更新？“技术选型：Trie 树 (前缀树)实现：初始化： 服务启动时将敏感词库（DB 或文件）加载到内存中的 Trie 树。节点标记
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是否为词尾。过滤过程： 对用户输入（帖子/评论）进行滑动窗口扫描。匹配到 Trie 树路径且到达词尾时，替换为 *** 或阻断提交。优化： 结合 DFA (确定有限状态自动机) 减少回溯，支持跳过无关字符（如 敏*感*词）。词库更新：热更新： 后台管理添加敏感词后，通过 ZooKeeper 配置中心 或 Redis Pub/Sub广播到所有服务节点，节点异步重建 Trie 树。降级： 更新期间短暂使用旧词库，避免服务中断。词库版本号控制。效率：Trie 树查询时间复杂度 O(n) (n=文本长度)，内存占用可控（可压缩节点）。避免正则表达式（性能差）。”9. 你提到用 Spring Boot Actuator 进行监控并自定义了数据库监控端点。请说明：暴露了哪些关键内置端点？（至少 3个）如何自定义一个端点监控数据库连接池状态（如活跃连接数、等待连接数）？如何保证这些监控接口的安全？答：“1. 关键内置端点：/health：应用健康状态（DB, Redis, Disk 等）/metrics：JVM 内存、线程、HTTP 请求指标等/loggers：动态查看/调整日志级别/threaddump：获取线程快照（排
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 查死锁）创建一个类实现 Endpoint 接口或使用 @Endpoint(id = "dbpool") 注解。注入连接池对象（如 HikariDataSource）。在 @ReadOperation 方法中返回关键指标：安全保障：访问控制：通过 management.endpoints.web.exposure.include/exclude 精确控制暴露的端点。安全加固：集成 Spring Security：只允许管理员角色访问 /actuator/** 路径。修改默认端口：management.server.port 使用与管理网络隔离的端口。HTTPS： 强制要求监控端点使用 HTTPS。10. 在“点赞后发通知”这个场景，涉及更新数据库点赞数 (DB) 和发送 Kafka 消息 (通知) 。如何保证这两个操作的原子性？如果 Kafka 发送失败，如何处理？答：“核心思路：最终一致性 + 本地事务 + 可靠消息原子性保障： 将 ‘更新点赞状态/计数’ 和 ‘写入待通知消息’ 放在同一个数据库事务中。使用 ‘本地消息表’ 方案：在业务数据库创建 message_event 表 (含业务 ID、消息体、状态
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: [PENDING, SENT])。事务内操作：更新点赞相关 DB 数据。向 message_event 插入一条 PENDING 状态的通知记录。事务提交。Kafka 发送与补偿：后台定时任务扫描 PENDING 的消息。发送消息到 Kafka，成功后将状态改为 SENT。发送失败处理：记录重试次数和错误信息，下次任务重试（指数退避）。超过最大重试则标记为失败，告警人工介入。消费者幂等： 通知消费者根据业务 ID 去重，避免重复处理。为什么不强一致？ 跨系统（DB 与 MQ）的强一致（如 2PC）成本高且降低可用性。本方案在 CAP 中优先保证 AP，通过可靠消息实现最终一致，满足业务需求。”11.你提到使用 ThreadLocal 存储用户数据以替代 Session。这在单机中可行，但分布式部署时（如多台 Tomcat 节点）会失效。如何解决分布式场景下的用户状态共享问题？业界主流方案是什么？答：“ThreadLocal 的局限： 它绑定于单个 JVM 线程，无法跨节点共享。在负载均衡（如 Nginx 轮询）下，用户请求落到不同节点会导致状态丢失。方案演进：Session 复制： 利用 Tomcat Red
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: is Session Manager 等工具将 Session 存入Redis。所有节点从 Redis 读写 Session，实现共享。无状态 Token (JWT)： 当前项目采用的核心方案。用户登录后生成包含用户 ID和权限的 JWT Token 返回客户端（通常存于 Cookie 或 Header）。后续请求携带 Token，服务端无需存储 Session，仅需验证 Token 签名和有效期并从 Token中解析用户信息（如注入到 SecurityContext）。这天然支持分布式。项目整合： 我们实际采用了 JWT + Redis 黑名单 的增强方案：JWT 本身无状态，解析快速。主动登出/失效： 将需提前失效的 Token ID 存入 Redis 并设置 TTL（作为黑名单）。校验 Token 时额外检查黑名单。安全性： Token 使用强密钥签名（如 HMAC-SHA256），防止篡改。优点： 彻底解决分布式状态问题，减轻服务端存储压力，更适合 RESTful API。12. 热帖列表使用了 Caffeine 本地缓存。请说明：你选择了哪种缓存淘汰策略（如 LRU、LFU）？依据是什么？如何配置缓
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存大小和过期时间？如何监控缓存命中率？本地缓存导致不同节点数据不一致的风险如何缓解？（例如帖子被删除）答：“1. 淘汰策略与依据：策略： 使用 Window TinyLFU (W-TinyLFU)，Caffeine 的默认算法。它结合了 LRU（近期使用）和 LFU（频率统计）的优点，对突发流量和长期热点都有良好表现。依据：论坛热帖访问模式既有突发（新热帖），也有长尾（持续热帖）。W-TinyLFU在有限空间内能最大化命中率，优于纯 LRU/LFU。2. 配置与监控：配置：javaCaffeine.newBuilder().maximumSize(10_000) // 最大条目数.expireAfterWrite(5, TimeUnit.MINUTES) // 写入后 5 分钟过期.recordStats() // 开启统计.build();监控： 通过 Cache.stats() 获取 CacheStats 对象，关键指标：hitRate()：命中率evictionCount()：淘汰数量averageLoadPenalty()：平均加载耗时可定期输出到日志或监控系统（如 Prometheus）。3. 数据
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不一致风险缓解：主动失效： 核心策略！当帖子被删除或更新时：更新数据库。删除 Redis 中的缓存 Key。通过 Redis Pub/Sub 或 专门的广播方案（如 RabbitMQ Fanout Exchange） 发布“帖子失效”事件。所有服务节点监听到事件后，删除本地 Caffeine 缓存中对应的条目。兜底： 设置较短的本地缓存过期时间（如 5分钟），确保最终一致。”13.你提到点赞功能单机 TPS 达 5000，通知单机 TPS 7000，热帖访问 QPS 8000。请说明：这些数据是如何测试得到的？（工具、场景、环境）TPS 和 QPS 的区别是什么？测试中发现了哪些性能瓶颈？如何定位和优化的？（如 GC、慢 SQL）1. 测试方法：工具： JMeter（模拟并发用户）。场景：点赞 TPS： 持续模拟用户对随机帖子点赞（高并发写）。通知 TPS： 模拟触发通知事件（评论/关注），测量 Kafka 生产者吞吐量。热帖 QPS： 持续请求热帖列表接口（高并发读）。环境： 明确标注是 单机测试（如 4C8G Linux, JDK 17, Tomcat, Redis/Kafka 同机或独立）。2. TPS
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  vs QPS：QPS (Queries Per Second)： 服务器每秒处理的查询请求数（如 HTTP 请求）。适用于读场景。TPS (Transactions Per Second)： 服务器每秒处理的事务数（一个事务可能包含多个操作/请求）。点赞（写 DB + Redis + 发 Kafka）是一个事务。此处 5000 TPS 指每秒完成 5000 次点赞事务。3. 瓶颈发现与优化：发现工具： Arthas (监控方法耗时)、JVisualVM/PerfMa (GC 分析)、Redis Slowlog、MySQL Slow Query Log。典型瓶颈 & 优化：GC 频繁 (Young GC >1s)： 优化 JVM 参数（如 -XX:+UseG1GC, 调整MaxGCPauseMillis），减少大对象分配（如缓存 DTO 复用）。慢 SQL (全表扫描)： 添加索引（如 post_id 在点赞表），优化查询（避免 SELECT*）。Redis 单线程阻塞： 避免长命令（如 KEYS *），分片（Cluster），热点 Key 本地缓存（Caffeine）。Kafka 生产瓶颈： 调优 batc
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: h.size 和 linger.ms，增加分区数，提高 acks 级别换取可靠性（需权衡）。”14.如果这个论坛用户量增长 100 倍（如日活千万级），当前架构在哪些地方可能最先遇到瓶颈？你会如何改造？（请结合你已用技术栈思考）“潜在瓶颈与改造方向：数据库 (MySQL)：瓶颈： 写压力（点赞、发帖）、复杂查询（搜索、统计）、单表数据量过大。改造：读写分离： 主库写，多个从库读（评论列表、用户信息查询）。分库分表： 按 user_id 或 post_id 分片（如 ShardingSphere）。将点赞/关注等高频写操作分离到独立库。冷热数据分离： 归档旧帖到分析型数据库（如 HBase）。Redis：瓶颈： 单机内存容量、带宽、单线程处理。改造：集群化： Redis Cluster 自动分片。区分数据类型： 热点数据（用户信息）用集群；超大 Value（如长帖缓存）考虑其他存储或压缩；统计类（UV/DAU）可保留。Elasticsearch：瓶颈： 索引过大导致查询慢、写入堆积。改造：分片策略优化： 增加主分片数（提前规划）。按时间分索引： 如 posts-202307，便于管理/查询/删除旧数据。Kafk
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: a：瓶颈： 单个 Topic 分区数限制吞吐量。改造： 增加分区数，生产者根据 Key（如 user_id）分区保证顺序性。应用层：瓶颈： 单节点处理能力。改造： 水平扩展无状态节点（更多 Tomcat 实例），通过 Nginx 负载均衡。微服务化拆分（如独立用户服务、帖子服务、消息服务），便于独立伸缩。监控与治理：加强：引入 APM（如 SkyWalking）、集中日志（ELK）、更强健的配置中心（Nacos）和熔断限流（Sentinel）。RedisIO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？”同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。举例子说一下阻塞 IO 和 IO
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  多路复用的差别？假设我是一名老师，让学生解答一道题目。我的第一种选择：按顺序逐个检查，先检查 A 同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种就是阻塞 IO，不具有并发能力我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 ARedis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。等子进
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。AOF 重写期间命令会同时写入现有 AOF 文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。缺点是可能丢失两次快照期间的所有数据变更AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。缺点是文件体积较大，恢复速度慢。在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。如果是缓存场景，可以接受一定程度的数据丢失，
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布/订阅机制通知客户端主节点已经发生变化。主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。当
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。常见的数据分区有三种：节点取余、一致性哈希和哈希槽。节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过CRC16 算法计算后对 16384 取模，确定它属于哪个槽。布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来解决 Redis 缓存穿透的问题。布隆过滤器并不支持删除操作，这是它的一个重要限制。如何保证缓存和数据库数据的一致性？具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题：线程 A 要更新用户信息，先删除了缓存线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值线程 B 将查到的旧值重新放入缓存线程 A 完成数据库更新结果就是数据库是新的值，但缓存中还是旧值当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。即使缓存删除失败，消息队列的重试机制也能保证最终一致性。第二种
2025-08-11 10:55:37.054 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。如何保证本地缓存和分布式缓存的一致性？为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pub/sub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内存压力。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis虽然不是专业的消息队列，但可以很好地实现延时队列功能。核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不存在时才能创建成功，EX 设置过期时间用以防止死锁。Kafka，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景Kafka 的设计Kafka 将消息以 topic 为单位进行归纳，发布消息的程序称为 Producer，消费消息的程序称为 Consumer。它是以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 Broker，Producer 通过网络将消息发送到 kafka 集群，集群向消费者提供消息，broker 在中间起到一个代理保存消息的中转站。Kafka 性能高原因利用了 PageCache 缓存磁盘顺序写零拷贝技术pull 拉模式优点高性能、高吞吐量、低延迟：Kafka 生产和消费消息的速度都达到每秒 10 万级高可用：所有消息持久化存储到磁盘，并支持数据备份防止数据丢失高并发：支持数千个客户端同时读写容错性：允许集群中节点失败（若副本数量为 n，则允许 n-1 个节点失败）高扩展性：Kafka 集群支持热伸缩，无须停机缺点没有完整的监控工具集不支持通配符主
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 题选择Kafka 的应用场景日志聚合：可收集各种服务的日志写入 kafka 的消息队列进行存储消息系统：广泛用于消息中间件系统解耦：在重要操作完成后，发送消息，由别的服务系统来完成其他操作流量削峰：一般用于秒杀或抢购活动中，来缓冲网站短时间内高流量带来的压力异步处理：通过异步处理机制，可以把一个消息放入队列中，但不立即处理它，在需要的时候再进行处理Kafka 为什么要把消息分区方便扩展：因为一个 topic 可以有多个 partition，每个 Partition 可用通过调整以适应它所在的机器，而一个 Topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率Kafka 中生产者运行流程一条消息发过来首先会被封装成一个 ProducerRecord 对象对该对象进行序列化处理（可以使用默认，也可以自定义序列化）对消息进行分区处理，分区的时候需要获取集群的元数据，决定这个消息会被发送到哪个主题的哪个分区分好区的消息不会直接发送到服务端，而是放入生产者的缓存区，多条消息会被封装成一个批次（
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Batch），默认一个批次的大小是 16KBSender 线程启动以后会从缓存里面去获取可以发送的批次Sender 线程把一个一个批次发送到服务端Kafka采用大部分消息系统遵循的传统模式：Producer 将消息推送到 Broker，Consumer 从 Broker 获取消息。负载均衡是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度保证系统整体运行效率与稳定性负载均衡Kakfa 的负载均衡就是每个 Broker 都有均等的机会为 Kafka 的客户端（生产者与消费者）提供服务，可以负载分散到所有集群中的机器上。Kafka 通过智能化的分区领导者选举来实现负载均衡，提供智能化的 Leader 选举算法，可在集群的所有机器上均匀分散各个 Partition的 Leader，从而整体上实现负载均衡。故障转移Kafka 的故障转移是通过使用会话机制实现的，每台 Kafka 服务器启动后会以会话的形式把自己注册到 Zookeeper 服务器上。一旦服务器运转出现问题，就会导致与 Zookeeper 的会话不能维持从而超时断连，此时 Kafka 集群会选举出另一台服务器来完全替代这台服务
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 器继续提供服务。Kafka 中 Zookeeper 的作用Kafka 是一个使用 Zookeeper 构建的分布式系统。Kafka 的各 Broker 在启动时都要在 Zookeeper上注册，由 Zookeeper统一协调管理。如果任何节点失败，可通过Zookeeper从先前提交的偏移量中恢复，因为它会做周期性提交偏移量工作。同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也是 Zookeeper在维护Kafka 中消费者与消费者组的关系与负载均衡实现Consumer Group 是 Kafka 独有的可扩展且具有容错性的消费者机制。一个组内可以有多个 Consumer，它们共享一个全局唯一的 Group ID。组内的所有 Consumer协调在一起来消费订阅主题（Topic）内的所有分区（Partition）。当然，每个 Partition只能由同一个 Consumer Group内的一个 Consumer 来消费。消费组内的消费者可以使用多线程的方式实现，消费者的数量通常不超过分区的数量，且二者最好保持整数倍的关系，这样不会造成有空
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲的消费者。Consumer 订阅的是 Topic 的 Partition，而不是 Message。所以在同一时间点上，订阅到同一个分区的 Consumer 必然属于不同的 Consumer GroupConsumer Group与 Consumer的关系是动态维护的，当一个 Consumer 进程挂掉或者是卡住时，该 Consumer 所订阅的 Partition会被重新分配到改组内的其他Consumer 上，当一个 Consumer加入到一个 Consumer Group中时，同样会从其他的 Consumer 中分配出一个或者多个 Partition到这个新加入的 Consumer。当生产者试图发送消息的速度快于 Broker 可以处理的速度时，通常会发生QueueFullException首先先进行判断生产者是否能够降低生产速率，如果生产者不能阻止这种情况，为了处理增加的负载，用户需要添加足够的 Broker。或者选择生产阻塞，设置Queue.enQueueTimeout.ms 为 -1，通过这样处理，如果队列已满的情况，生产者将组织而不是删除消息。或者容忍这种异常，进行消息丢弃。Consumer 如何
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 消费指定分区消息Cosumer 消费消息时，想 Broker 发出 fetch 请求去消费特定分区的消息，Consumer 可以通过指定消息在日志中的偏移量 offset，就可以从这个位置开始消息消息，Consumer 拥有了 offset 的控制权，也可以向后回滚去重新消费之前的消息。也可以使用 seek(Long topicPartition) 来指定消费的位置。Replica、Leader 和 Follower 三者的概念:Kafka 中的 Partition 是有序消息日志，为了实现高可用性，需要采用备份机制，将相同的数据复制到多个 Broker 上，而这些备份日志就是 Replica，目的是为了防止数据丢失。所有 Partition 的副本默认情况下都会均匀地分布到所有 Broker 上,一旦领导者副本所在的 Broker 宕机，Kafka 会从追随者副本中选举出新的领导者继续提供服务。Leader： 副本中的领导者。负责对外提供服务，与客户端进行交互。生产者总是向 Leader 副本些消息，消费者总是从 Leader 读消息Follower： 副本中的追随者。被动地追随 Leader，不能与外界进
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行交付。只是向 Leader 发送消息，请求 Leader把最新生产的消息发给它，进而保持同步。Kafka 中 AR、ISR、OSR 三者的概念AR：分区中所有副本称为 ARISR：所有与主副本保持一定程度同步的副本（包括主副本）称为 ISROSR：与主副本滞后过多的副本组成 OSR分区副本什么情况下会从 ISR 中剔出Leader 会维护一个与自己基本保持同步的 Replica列表，该列表称为 ISR，每个Partition都会有一个 ISR，而且是由 Leader 动态维护。所谓动态维护，就是说如果一个 Follower比一个 Leader 落后太多，或者超过一定时间未发起数据复制请求，则 Leader 将其从 ISR 中移除。当 ISR 中所有 Replica 都向 Leader 发送 ACK（Acknowledgement确认）时，Leader 才 commit分区副本中的 Leader 如果宕机但 ISR 却为空该如何处理可以通过配置 unclean.leader.election ：true：允许 OSR 成为 Leader，但是 OSR 的消息较为滞后，可能会出现消息不一致的问题false：会一
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直等待旧 leader 恢复正常，降低了可用性Kafka的 Producer 有三种 ack机制，参数值有 0、1 和 -10： 相当于异步操作，Producer 不需要 Leader 给予回复，发送完就认为成功，继续发送下一条（批）Message。此机制具有最低延迟，但是持久性可靠性也最差，当服务器发生故障时，很可能发生数据丢失。1： Kafka 默认的设置。表示 Producer 要 Leader 确认已成功接收数据才发送下一条（批）Message。不过 Leader 宕机，Follower 尚未复制的情况下，数据就会丢失。此机制提供了较好的持久性和较低的延迟性。-1： Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些Follower都确认消息已同步，Producer 才发送下一条（批）Message。此机制持久性可靠性最好，但延时性最差Kafka 的 consumer 如何消费数据在 Kafka中，Producers 将消息推送给 Broker 端，在 Consumer 和 Broker 建立连接之后，会主动去 Pull（或者说 Fetch）消息。这种模式有些优点
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，首先 Consumer端可以根据自己的消费能力适时的去 fetch消息并处理，且可以控制消息消费的进度（offset）；此外，消费者可以控制每次消费的数，实现批量消费。Kafka 的 Topic 中 Partition 数据是怎么存储到磁盘的用磁盘顺序写+内存页缓存+稀疏索引Topic 中的多个 Partition 以文件夹的形式保存到 Broker，每个分区序号从 0递增，且消息有序。Partition 文件下有多个 Segment（xxx.index，xxx.log），Segment文件里的大小和配置文件大小一致。默认为 1GB，但可以根据实际需要修改。如果大小大于 1GB时，会滚动一个新的 Segment并且以上一个 Segment 最后一条消息的偏移量命名。生产者发送消息│▼Leader Partition│▼ (追加写入)当前活跃 Segment: [00000000000000.log]│▼ (每隔 4KB 数据)更新 .index/.timeindex│▼ (Segment 满 1GB)创建新 Segment: [00000000000015.log]消费者请求 offset=520│▼查 .
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: index 文件 → 找到 offset=500 → position=10240│▼从 .log 文件 10240 位置顺序扫描│▼找到 offset=520 的消息返回Kafka 创建 Topic 后如何将分区放置到不同的 Broker 中Kafka创建 Topic 将分区放置到不同的 Broker 时遵循以下规则：副本因子不能大于 Broker 的个数。第一个分区（编号为 0）的第一个副本放置位置是随机从 Broker List 中选择的。其他分区的第一个副本放置位置相对于第 0个分区依次往后移。也就是如果有 3个 Broker，3 个分区，假设第一个分区放在第二个 Broker 上，那么第二个分区将会放在第三个 Broker 上；第三个分区将会放在第一个 Broker 上，更多 Broker 与更多分区依此类推。剩余的副本相对于第一个副本放置位置其实是由nextReplicaShift决定的，而这个数也是随机产生的。Kafka 中如何进行主从同步Kafka动态维护了一个同步状态的副本的集合（a set of In-SyncReplicas），简称 ISR，在这个集合中的结点都是和 Leader 保持高
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度一致的，任何一条消息只有被这个集合中的每个结点读取并追加到日志中，才会向外部通知“这个消息已经被提交”同步复制Producer 会先通过 Zookeeper识别到 Leader，然后向 Leader 发送消息，Leader收到消息后写入到本地 log文件。这个时候 Follower 再向 Leader Pull 消息，Pull回来的消息会写入的本地 log 中，写入完成后会向 Leader 发送 Ack 回执，等到 Leader 收到所有 Follower 的回执之后，才会向 Producer 回传 Ack。异步复制Kafka 中 Producer 异步发送消息是基于同步发送消息的接口来实现的，异步发送消息的实现很简单，客户端消息发送过来以后，会先放入一个 BlackingQueue队列中然后就返回了。Producer 再开启一个线程 ProducerSendTread 不断从队列中取出消息，然后调用同步发送消息的接口将消息发送给 Broker。Kafka 中什么情况下会出现消息丢失/不一致的问题消息发送时消息发送有两种方式：同步 - sync 和 异步 - async。默认是同步的方式，可以通过 prod
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ucer.type 属性进行配置，kafka 也可以通过配置 acks 属性来确认消息的生产0：表示不进行消息接收是否成功的确认1：表示当 leader 接收成功时的确认-1：表示 leader 和 follower 都接收成功的确认当 acks = 0 时，不和 Kafka 进行消息接收确认，可能会因为网络异常，缓冲区满的问题，导致消息丢失当 acks = 1 时，只有 leader 同步成功而 follower 尚未完成同步，如果 leader挂了，就会造成数据丢失消息消费时Kafka 有两个消息消费的 consumer 接口，分别是 low-level 和 hign-levellow-level：消费者自己维护 offset 等值，可以实现对 kafka 的完全控制high-level：封装了对 partition 和 offset，使用简单如果使用高级接口，可能存在一个消费者提取了一个消息后便提交了 offset，那么还没来得及消费就已经挂了，下次消费时的数据就是 offset + 1 的位置，那么原先 offset 的数据就丢失了Kafa 中如何保证顺序消费Kafka 的消费单元是 Partitio
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n，同一个 Partition 使用 offset 作为唯一标识保证顺序性，但这只是保证了在 Partition 内部的顺序性而不是 Topic 中的顺序，因此我们需要将所有消息发往统一 Partition 才能保证消息顺序消费，那么可以在发送的时候指定 MessageKey，同一个 key 的消息会发到同一个 Partition 中。Java介绍一下 javajava 是一门开源的跨平台的面向对象的计算机语言.跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而虚拟机是不同平台有不同版本,所以说 java 是跨平台的.面向对象有几个特点:1.封装两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者'封装'在一个不可分割的独立单元(即对象)中另一层含义指'信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能隐藏对象的功能实现细节。2.继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有父类相同的行为。3.多态多态是同一个行为具有多个不同表现形式或形态的能力。Java 语言中含有方法重载与对象多态两种形式的多态：1.方法重载：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。2.对象多态：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。Java有哪些数据类型？java 主要有两种数据类型1.基本数据类型基本数据有八个,byte,short,int,long 属于数值型中的整数型float,double属于数值型中的浮点型char属于字符型boolean属于布尔型2.引用数据类型引用数据类型有三个,分别是类,接口和数组接口和抽象类有什么区别？1.接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类是声明方法的存在而不去实现它的类。2.接口可以多继承，抽象类不行。3.接口定义方法，不能实现，默认是 public abstract，而抽象类可以实现部分方法。4.接口中基本数据类型为 public static final 并且需要给出初始值，而抽类象不是的。重载和重写什么区别？重写：1.参数列表必须完全与被重写的方法相同，
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否则不能称其为重写而是重载.2.返回的类型必须一直与被重写的方法的返回类型相同，否则不能称其为重写而是重载。3.访问修饰符的限制一定要大于被重写方法的访问修饰符4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。重载：1.必须具有不同的参数列表；2.可以有不同的返回类型，只要参数列表不同就可以了；3.可以有不同的访问修饰符；4.可以抛出不同的异常；常见的异常有哪些？NullPointerException 空指针异常ArrayIndexOutOfBoundsException 索引越界异常InputFormatException 输入类型不匹配SQLException SQL 异常IllegalArgumentException 非法参数NumberFormatException 类型转换异常 等等....异常要怎么解决？Java标准库内建了一些通用的异常，这些类以 Throwable 为顶层父类。Throwable又派生出 Error 类和 Exception类。错误：Error类以及他的子类的实例，代表了 JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，程序员应该关注 Exception 为父类的分支下的各种异常类。异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被 Java异常处理机制使用，是异常处理的核心。hashMap 线程不安全体现在哪里？在 hashMap1.7 中扩容的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法在任意版本的 hashMap 中，如果在插入数据时多个线程命中了同一个槽，可能会有数据覆盖的情况发生，导致线程不安全。说说进程和线程的区别？进程是系统资源分配和调度的基本单位，它能并发执行较高系统资源的利用率.线程是比进程更小的能独立运行的基本单位,创建、销毁、切换成本要小于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为 1，结果又是什么？Integer a = 1000，Integer b = 1000，a==b 结果为 falseInteger a = 1，Integer b = 1，a==b 结
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果为 true这道题主要考察 Integer 包装类缓存的范围,在-128~127 之间会缓存起来,比较的是直接缓存的数据,在此之外比较的是对象JMM 就是 Java 内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以 java 内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 java 程序在各种平台下都能达到一致的并发效果。Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。线程不能直接读写主内存中的变量。每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式cas 是什么？cas 叫做 CompareAndSwap，比较并交换，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是通过处理器的指令来保证操作的原子性，它主要包含三
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个变量：当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，如果相同就认为其他线程没有进行修改，而将新值写入内存聊聊 ReentrantLock 吧ReentrantLock 意为可重入锁，说起 ReentrantLock 就不得不说 AQS ，因为其底层就是使用 AQS 去实现的。ReentrantLock有两种模式，一种是公平锁，一种是非公平锁。公平模式下等待线程入队列后会严格按照队列顺序去执行非公平模式下等待线程入队列后有可能会出现插队情况公平锁第一步：获取状态的 state 的值如果 state=0 即代表锁没有被其它线程占用，执行第二步。如果 state!=0 则代表锁正在被其它线程占用，执行第三步。第二步：判断队列中是否有线程在排队等待如果不存在则直接将锁的所有者设置成当前线程，且更新状态 state 。如果存在就入队。第三步：判断锁的所有者是不是当前线程如果是则更新状态 state 的值。如果不是，线程进入队列排队等待。非公平锁获取状态的 state 的值如果 state=0 即代表锁
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 没有被其它线程占用，则设置当前锁的持有者为当前线程，该操作用 CAS 完成。如果不为 0或者设置失败，代表锁被占用进行下一步。此时获取 state 的值如果是，则给 state+1，获取锁如果不是，则进入队列等待如果是 0，代表刚好线程释放了锁，此时将锁的持有者设为自己如果不是 0，则查看线程持有者是不是自己多线程的创建方式有哪些？继承 Thread类，重写 run()方法public class Demo extends Thread{//重写父类 Thread的 run()public void run() {}public static void main(String[] args) {Demo d1 = new Demo();Demo d2 = new Demo();d1.start();d2.start();}}实现 Runnable接口，重写 run()public class Demo2 implements Runnable{//重写 Runnable接口的 run()public void run() {}public static void main(String[] args) {Th
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: read t1 = new Thread(new Demo2());Thread t2 = new Thread(new Demo2());t1.start();t2.start();}}实现 Callable 接口public class Demo implements Callable<String>{public String call() throws Exception {System.out.println("正在执行新建线程任务");Thread.sleep(2000);return "结果";}public static void main(String[] args) throws InterruptedException,ExecutionException {Demo d = new Demo();FutureTask<String> task = new FutureTask<>(d);Thread t = new Thread(task);t.start();//获取任务执行后返回的结果String result = task.get();}}使用线程池创建public class 
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Demo {public static void main(String[] args) {Executor threadPool = Executors.newFixedThreadPool(5);for(int i = 0 ;i < 10 ; i++) {threadPool.execute(new Runnable() {public void run() {//todo}});}}}线程池有哪些参数？corePoolSize：核心线程数，线程池中始终存活的线程数。2.maximumPoolSize: 最大线程数，线程池中允许的最大线程数。3.keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。4.unit: 单位，参数 keepAliveTime 的时间单位，7种可选。5.workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种可选。6.threadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。7.handler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicy。线程池的执行流程？判断线程池中的
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程数是否大于设置的核心线程数如果小于，就创建一个核心线程来执行任务如果大于，就会判断缓冲队列是否满了如果没有满，则放入队列，等待线程空闲时执行任务如果队列已经满了，则判断是否达到了线程池设置的最大线程数如果没有达到，就创建新线程来执行任务如果已经达到了最大线程数，则执行指定的拒绝策略深拷贝、浅拷贝是什么？浅拷贝并不是真的拷贝，只是复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象聊聊 ThreadLocal 吧ThreadLocal其实就是线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。一个对象的内存布局是怎么样的?1.对象头: 对象头又分为 MarkWord 和 Class Pointer 两部分。MarkWord:包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位,gc 记录信息等等。ClassPointer:用来指向对象对应的 Class 对象（其对应的元数据对象）的内存地址。在 32 位系统占 4 字节，在 64 位系统中占 8 字节
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 。2.Length:只在数组对象中存在，用来记录数组的长度，占用 4 字节3.Instance data: 对象实际数据，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括静态成员变量，因为其是在方法区维护的)4.Padding:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，padding 的作用就是补充字节，保证对象是 8 字节的整数倍。HashMapHashMap的底层数据结构是什么？JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。链表过长，会严重影响HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是糟糕的 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：当链
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 表超过 8 且数据总量超过 64 时会转红黑树。将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。解决 hash冲突的办法有哪些？HashMap用的哪种？解决 Hash 冲突方法有：开放定址法：也称为再散列法，基本思想就是，如果 p=H(key)出现冲突时，则以p为基础，再次 hash，p1=H(p),如果 p1再次出现冲突，则以 p1为基础，以此类推，直到找到一个不冲突的哈希地址 pi。因此开放定址法所需要的 hash表的长度要大于等于所需要存放的元素，而且因为存在再次 hash，所以只能在删除的节点上做标记，而不能真正删除节点。再哈希法：双重散列，多重散列，提供多个不同的 hash函数，当 R1=H1(key1)发生冲突时，再计算 R2=H2(key1)，直到没有冲突为止。这样做虽然不易产生堆集，但增加了计算的时间。链地址法：拉链法，将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第 i 个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。建立公共溢出区：将哈希表分为公共表和
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。HashMap中采用的是链地址法为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。为什么 hash 值要与 length-1 相与？把 hash 值对数组长度取模运算，模运算的消耗很大，没有位运算快。当 length 总是 2 的 n次方时，h& (length-1) 运算等价于对 length取模，也就是 h%length，但是 & 比 % 具有更高的效率HashMap数组的长度为什么是 2 的幂次方？2 的 N 次幂有助于减少碰撞的几率。如果 length 为 2的幂次方，则 length-1 转化为二进制必定是 11111……的形式，在与 h 的二进制与操作效率会非
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 常的快，而且空间不浪费。HashMap 的 put方法流程？首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；2、如果数组是空的，则调用 resize 进行初始化；3、如果没有哈希冲突直接放在对应的数组下标里；4、如果冲突了，且 key 已经存在，就覆盖掉 value；5、如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；6、如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。一般用什么作为 HashMap的 key?一般用 Integer、String 这种不可变类当作 HashMap 的 key，String 最为常见。因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的。Integer、String 这些类已经很规范的重写了hashCode() 以及 
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: equals() 方法。MySQL关系型和非关系型数据库的区别？关系型数据库的优点容易理解，因为它采用了关系模型来组织数据。可以保持数据的一致性。数据更新的开销比较小。支持复杂查询（带 where 子句的查询）非关系型数据库（NOSQL）的优点无需经过 SQL 层的解析，读写效率高。基于键值对，读写性能很高，易于扩展可以支持多种类型数据的存储，如图片，文档等等。扩展（可分为内存性数据库以及文档型数据库，比如 Redis，MongoDB，HBase 等，适合场景：数据量大高可用的日志系统/地理位置存储系统）。详细说一下一条 MySQL 语句执行的步骤Server 层按顺序执行 SQL 的步骤为：客户端请求 -> 连接器（验证用户身份，给予权限）查询缓存（存在缓存则直接返回，不存在则执行后续操作）分析器（对 SQL 进行词法分析和语法分析操作）优化器（主要对执行的 SQL 优化选择最优的执行方案方法）执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-> 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）MySQL 使用索引的原因？根本原因索引的出现，就是为了提高数据查询的效率，就像书的
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 目录一样。对于数据库的表而言，索引其实就是它的“目录”。扩展创建唯一性索引，可以保证数据库表中每一行数据的唯一性。帮助引擎层避免排序和临时表将随机 IO 变为顺序 IO，加速表和表之间的连接索引的三种常见底层数据结构以及优缺点三种常见的索引底层数据结构：分别是哈希表、有序数组和搜索树。哈希表这种适用于等值查询的场景，比如 memcached 以及其它一些 NoSQL 引擎，不适合范围查询，哈希表的数据是完全无序存储的。它只能回答“某个键值等于多少”的记录在哪，无法高效地查询“键值在某个范围之间”的所有记录（如WHERE id BETWEEN 10 AND 20）。需要扫描全表或遍历所有桶，效率极低 (O(n))。有序数组索引只适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高。N 叉树由于读写上的性能优点以及适配磁盘访问模式以及广泛应用在数据库引擎中。扩展（以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。）索引的常见类型以及它是如何发挥作用的？根据叶子节点的内容，索引类型分为主键索引和非主键索引。主键索引的叶子节点存的整行数据，在 InnoDB 里也被称为聚簇索引。非主键索引叶子节点存的主键的值，在 InnoDB 里也被称为二级索引MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件。MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data域存放的是数据记录的地址，索引文件和数据文件是分离的InnoDB 为什么设计 B+ 树索引？两个考虑因素：InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。CPU 将磁盘上的数据加载到内存中需要花费大量时间。为什么选择 B+ 树：哈希索引虽然能提供 O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机IO。而 B+ 树的所有叶节点可以通过指针来相互连接
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，减少顺序遍历带来的随机 IO。普通索引还是唯一索引？由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。什么是覆盖索引和索引下推？覆盖索引：在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。索引下推：MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。MySQL 的 change buffer 是什么？当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。注意唯一索引的更新就不
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 能使用 change buffer，实际上也只有普通索引可以使用。适用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 changebuffer 的维护代价。MySQL 是如何判断一行扫描数的？MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。redo log 和 binlog 的区别？为什么需要 redo log？redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash 后 内存
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据丢失的隐患，而 redo log 具备 crash safe 崩溃恢复 的能力。为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让innoDB 判断哪些数据已经刷盘，哪些数据还没有。但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交redo log 每次更新操作
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。redo log 内部结构是基于页的，记录了这个页的字段值变化，只要 crash 后读取redo log 进行重放，就可以恢复数据。这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备当数据库 crash 后，如何恢复未刷盘的数据到内存中？根据 redo log 和 binlog 的两阶段提交，未持久化的数据分为几种情况：change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从binlog 恢复 redo log，再从 redo log 恢复 change buffer。change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。redo log 写入方式？redo log 包括两部分内容，分别是内存中的日志缓冲(re
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: do log buffer)和磁盘上的日志文件(redo log file)。MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是 WAL。可以发现，redo log buffer 写入到 redo log file，是经过 OS buffer 中转的。其实可以通过参数 innodb_flush_log_at_trx_commit 进行配置，参数值含义如下：0：称为延迟写，事务提交时不会将 redo log buffer 中日志写入到 OS buffer，而是每秒写入 OS buffer 并调用写入到 redo log file 中。1：称为实时写，实时刷”，事务每次提交都会将 redo log buffer 中的日志写入 OS buffer 并保存到 redo log file 中。2： 称为实时写，延迟刷。每次事务提交写入到 OS buffer，然后是每秒将日志写入到 redo log file。
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: redo log 的执行流程?MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server层。MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。InnoDB 存储引擎层将 a修改为 1的这个操作记录到内存中。记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。binlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗?binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。但是
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。什么是两阶段提交？MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog，这就是"两阶段提交"。而两阶段提交就是让这两个状态保持逻辑上的一致。redolog 用于恢复主机故障时的未更新的物理数据，binlog 用于备份操作。两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。为什么需要两阶段提交呢?如果不用两阶段提交的话，可能会出现这样情况先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log备份恢复时，数据不一致。两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。在恢复数据时，redolog 状态为 com
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: mit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。MySQL 怎么知道 binlog 是完整的?一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event什么是 WAL 技术，有什么优点？WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过 redo log恢复，所以能够实现快速响应 SQL 语句redo log 日志格式redo log buffer (内存中)是由首尾相连的四个文件组成的，它们分别是：ib_logfile_1、ib_logfile_2、ib_logfile_3、ib_logfile_4。write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。chec
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: kpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。有了 redo log，当数据库发生宕机重启后，可通过 redo log 将未落盘的数据（check point 之后的数据）恢复，保证已经提交的事务记录不会丢失，这种能力称为 crash-safe。InnoDB 数据页结构一个数据页大致划分七个部分File Header：表示页的一些通用信息，占固定的 38 字节。page Header：表示数据页专有信息，占固定的 56 字节。inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的 26 字节。User Records：真正存储我们插入的数据，大小不固定。Free Space：页中尚未使用的部分，大小不固定。Page Directory：页中某些记录的相对位置，也就是各个槽对
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 应的记录在页面中的地址偏移量。File Trailer：用于检验页是否完整，占固定大小 8 字节。MySQL 是如何保证数据不丢失的？只要 redolog 和 binlog 保证持久化磁盘就能确保 MySQL 异常重启后回复数据在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。28、误删数据怎么办？DBA 的最核心的工作就是保证数据的完整性，先要做好预防，预防的话大概是通过这几个点：权限控制与分配(数据库和服务器权限)制作操作规范定期给开发进行培训搭建延迟备库做好 SQL 审计，只要是对线上数据有更改操作的语句(DML 和 DDL)都需要进行审核做好备份。备份的话又分为两个点 (1)如果数据量比较大，用物理备份xtrabackup。定期对数据库进行全量备份，也可以做增量备份。 (2)如果数据量较少，用 mysqldump 或者 mysqldumper。再利用 binlog 来恢复或者搭建主从的方式来恢复数据。 定期备份 binlog 文件也是很有必要的如果发
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 生了数据删除的操作，又可以从以下几个点来恢复:DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的myflash，也是一个不错的工具，本质都差不多，都是先解析 binlog event，然后在进行反转。把 delete 反转为 insert，insert 反转为 delete，update 前后 image 对调。所以必须设置 binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上。DDL 语句误操作(truncate 和 drop)，由于 DDL 语句不管 binlog_format 是 row还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长rm 删除：使用备份跨机房，或者最好是跨城市保存。DDL（数据定义语言） DML（数据操纵语言） DQL（数据查询语言）drop、truncate 和 delete 的区别DELETE 语句执行删除的
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。drop 语句将表所占用的空间全释放掉。在速度上，一般来说，drop> truncate > delete。如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大；如果想删除表，当然用 drop； 如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可；如果和事务有关，或者想触发 trigger，还是用 delete； 如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 W
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: eb、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。都说 InnoDB 好，那还要不要使用 MEMORY 引擎？内存表就是使用 memory 引擎创建的表为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉MySQL 是如何保证主备同步？主备关系的建立：一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从 binlog 文件 A的位置 P”开始同步，主库就从这个指定的位置开始往后发。而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。MySQL 主备切换流程：客户端读写都是直接访问 A，而节点 B是备库，只要将 A的更新都同步过来，到本地执行就可以保证数据是相同的。当需要切换的时候就
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 把节点换一下，A的节点 B的备库一个事务完整的同步过程：备库 B和主库 A建立来了长链接，主库 A内部专门线程用于维护了这个长链接。在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求 binlog 包括文件名和日志偏移量在备库 B上执行 start-slave 命令备库会启动两个线程：io_thread 和sql_thread 分别负责建立连接和读取中转日志进行解析执行备库读取主库传过来的 binlog 文件备库收到文件写到本地成为中转日志后来由于多线程复制方案的引入，sql_thread 演化成了多个线程什么是主备延迟主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有：有些部署条件下，备库所在机器的性能要比主库性能差。备库的压力较大。大事务，一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10分钟MySQL 的一主一备和一主多从有什么区别？在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上短时间提高 MySQL 性能的方法第一种方法：先处理掉那
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。 kill connection + id第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL选错了索引（force index）。InnoDB 为什么要用自增 ID 作为主键？自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。说一下 MySQL 的锁MySQL 在 server 层 和 存储引擎层 都运用了大量的锁MySQL server 层需要讲两种锁，第一种是 MDL(metadata lock) 元数据锁，第二种则 Table Lock 表锁。MDL 又名元数据锁，那么什么是元数据呢，任何描述数据库的
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内容就是元数据，比如我们的表结构、库结构等都是元数据。那为什么需要 MDL 呢？主要解决两个问题：事务隔离问题；数据复制问题InnoDB 有五种表级锁：IS（意向读锁）；IX（意向写锁）；S（读）；X（写）；AUTO-INC在对表进行 select/insert/delete/update 语句时候不会加表级锁IS 和 IX 的作用是为了判断表中是否有已经被加锁的记录自增主键的保障就是有 AUTO-INC 锁，是语句级别的：为表的某个列添加AUTO_INCREMENT 属性，之后在插⼊记录时，可以不指定该列的值，系统会⾃动为它赋上单调递增的值。InnoDB 4 种行级锁RecordLock：记录锁GapLock：间隙锁解决幻读；前一次查询不存在的东西在下一次查询出现了，其实就是事务 A中的两次查询之间事务 B执行插入操作被事务 A感知了Next-KeyLock：锁住某条记录又想阻止其它事务在改记录前面的间隙插入新纪录InsertIntentionLock：插入意向锁;如果插入到同一行间隙中的多个事务未插入到间隙内的同一位置则无须等待行锁和表锁的抉择全表扫描用行级锁索引是一种能提高数据库查询效率的数据结构。它可
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 以比作一本字典的目录，可以帮你快速找到对应的记录。索引一般存储在磁盘的文件中，它是占用物理空间的。正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。数据结构维度B+树索引：所有数据存储在叶子节点，复杂度为 O(logn)，适合范围查询。哈希索引: 适合等值查询，检索效率高，一次到位。全文索引：MyISAM 和 InnoDB 中都支持使用全文索引，一般在文本类型char,text,varchar 类型上创建。R-Tree 索引: 用来对 GIS 数据类型创建 SPATIAL 索引物理存储维度聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。（Innodb 存储引擎）非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。（Innodb 存储引擎）逻辑维度主键索引：一种特殊的唯一索引，不允许有空值。普通索引：MySQL 中基本索引类型，允许空值和重复值。联合索引：多个字段创建的索引，使用时遵循最左前缀原则。唯一索引：索引列中的值必须是唯一的，但是允许为空值。空间索引：MySQL5.7 之后支持空间索引，在空间索引这方面遵循 Op
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: enGIS 几何数据模型规则索引什么时候会失效？查询条件包含 or，可能导致索引失效如果字段类型是字符串，where 时一定用引号括起来，否则索引失效like 通配符可能导致索引失效。联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。在索引列上使用 mysql 的内置函数，索引失效。对索引列运算（如，+、-、*、/），索引失效。索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。索引字段上使用 is null， is not null，可能导致索引失效。左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。mysql 估计使用全表扫描要比使用索引快,则不使用索引哪些场景不适合建立索引？数据量少的表，不适合加索引更新比较频繁的也不适合加索引区分度低的字段不适合加索引（如性别）where、group by、order by 等后面没有使用到的字段，不需要建立索引已经有冗余的索引的情况（比如已经有 a,b 的联合索引，不需要再单独建立 a索引）为什么不是一般二叉树？如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找 树来说，查找效率更稳定，总体的查
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 找速度也更快。为什么不是平衡二叉树呢？我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作 为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说 的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果 是 B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数 就降下来啦，查询效率就快啦。那为什么不是 B 树而是 B+树呢？B+树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储 键值，也会存储数据。innodb 中页的默认大小是 16KB，如果不存储数据，那 么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就 会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数有会再次减少，数据查 询的效率也会更快。B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链 表连着的。那么 B+树使得范围查找，排序查找，分组查找以及去重查找变得 异常简单。一次 B+树索引树查找过程：select * from Temployee where age=32;这条 SQL 查询语句执行大概流程是这样的：搜索 idx_age 索引
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 树，将磁盘块 1加载到内存，由于 32<43,搜索左路分支，到磁盘寻址磁盘块 2。将磁盘块 2加载到内存中，由于 32<36,搜索左路分支，到磁盘寻址磁盘块 4。将磁盘块 4加载到内存中，在内存继续遍历，找到 age=32 的记录，取得 id = 400.拿到 id=400 后，回到 id 主键索引树。搜索 id 主键索引树，将磁盘块 1加载到内存，因为 300<400<500,所以在选择中间分支，到磁盘寻址磁盘块 3。虽然在磁盘块 3，找到了 id=400，但是它不是叶子节点，所以会继续往下找。到磁盘寻址磁盘块 8。将磁盘块 8加载内存，在内存遍历，找到 id=400 的记录，拿到 R4 这一行的数据，好的，大功告成。什么是回表？如何减少回表？当查询的数据在索引树中，找不到的时候，需要回到主键索引树中去获取，这个过程叫做回表。比如在第 6小节中，使用的查询 SQLselect * from Temployee where age=32;需要查询所有列的数据，idx_age 普通索引不能满足，需要拿到主键 id 的值后，再回到 id 主键索引查找获取，这个过程就是回表。什么是覆盖索引？如果我们查询 SQL 的
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  select * 修改为 select id, age 的话，其实是不需要回表的。因为 id 和 age 的值，都在 idx_age 索引树的叶子节点上，这就涉及到覆盖索引的知识点了。覆盖索引是 select 的数据列只用从索引中就能够取得，不必回表，换句话说，查询列要被所建的索引覆盖聊聊索引的最左前缀原则、索引的最左前缀原则，可以是联合索引的最左 N个字段。比如你建立一个组合索引（a,b,c），其实可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。索引下推了解过吗？什么是索引下推select * from employee where name like '小%' and age=28 and sex='0';其中，name 和 age 为联合索引（idx_name_age）。如果是Mysql5.6之前，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键 id，然后回表找出数据行，再去对比年龄和性别等其他字段。有些朋友可能觉得奇怪，idx_name_age（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄 age 
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。因此，MySQL5.6 版本之后，选出包含“小”字后，顺表过滤 age=28大表如何添加索引？如果一张表数据量级是千万级别以上的，那么，如何给这张表添加索引？我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：先创建一张跟原表 A数据结构相同的新表 B。在新表 B添加需要加上的新索引。把原表 A数据导到新表 Brename 新表 B为原表的表名 A，原表 A换别的表名Hash 索引和 B+树区别是什么？你在设计索引是怎么抉择的？B+树可以进行范围查询，Hash 索引不能。B+树支持联合索引的最左侧原则，Hash 索引不支持。B+树支持 order by 排序，Hash 索引不支持。Hash 索引在等值查询上比 B+树效率更高。（但是索引列的重复值很多的话，Hash冲突，效率降低）。B+树使用 like 进行模糊查询的时候，like 后面（比如%开头）的话可以起到优化的作用，Hash 索引根
2025-08-11 10:55:37.056 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 本无法进行模糊查询。索引有哪些优缺点？优点：索引可以加快数据查询速度，减少查询时间唯一索引可以保证数据库表中每一行的数据的唯一性缺点：创建索引和维护索引要耗费时间索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间以表中的数据进行增、删、改的时候，索引也要动态的维护。聚簇索引与非聚簇索引的区别聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它表示索引结构和数据一起存放的索引。非聚集索引是索引结构和数据分开存放的索引。接下来，我们分不同存存储引擎去聊哈~在 MySQL 的 InnoDB 存储引擎中， 聚簇索引与非聚簇索引最大的区别，在于叶节点是否存放一整行记录。聚簇索引叶子节点存储了一整行记录，而非聚簇索引叶子节点存储的是主键信息，因此，一般非聚簇索引还需要回表查询。一个表中只能拥有一个聚集索引（因为一般聚簇索引就是主键索引），而非聚集索引一个表则可以存在多个。一般来说，相对于非聚簇索引，聚簇索引查询效率更高，因为不用回表。而在 MyISM 存储引擎中，它的主键索引，普通索引都是非聚簇索引，因为数据和索引是分开的，叶子节点都使用一个地址指向真正的表数据。Nginx什么是 Ng
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: inx？Nginx 是一个 轻量级/高性能的反向代理 Web 服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。他实现非常高效的反向代理、负载平衡，他可以处理 2-3万并发连接数，官方监测能支持 5万并发，现在中国使用 nginx 网站用户有很多，例如：新浪、网易、 腾讯等。Nginx 怎么处理请求的？首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的 Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与 nginx 进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建nginx 对连接的封装，即 ngx
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: _connection_t 结构体。接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。Nginx 是如何实现高并发的？如果一个 server 采用一个进程(或者线程)负责一个 request 的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。而 Nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。每进来一个 request ，会有一个 worker 进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发 request ，并等待请求返回。那么，这个处理的 worker 不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有 request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker 才会来接手
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，这个 request 才会接着往下走。这就是为什么说，Nginx 基于事件模型。由于 web server 的工作性质决定了每个 request 的大部份生命都是在网络传输中，实际上花费在 server 机器上的时间片不多。这是几个进程就解决高并发的秘密所在。即：webserver 刚好属于网络 IO 密集型应用，不算是计算密集型。异步，非阻塞，使用 epoll ，和大量细节处的优化。也正是 Nginx 之所以然的技术基石。什么是正向代理？一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的 OpenVPN 等等。什么是反向代理？反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet 上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 反向代理总结就一句话：代理端代理的是服务端。反向代理服务器的优点是什么?反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。cookie 和 session 区别？共同：存放用户信息。存放的形式：key-value 格式 变量和变量内容键值对。区别：cookie存放在客户端浏览器每个域名对应一个 cookie，不能跨跃域名访问其他 cookie用户可以查看或修改 cookiehttp 响应报文里面给你浏览器设置钥匙（用于打开浏览器上锁头）session:存放在服务器（文件，数据库，redis）存放敏感信息锁头为什么 Nginx 不使用多线程？Apache: 创建多个进程或线程，而每个进程或线程都会为其分配 cpu 和内存（线程要比进程小的多，所以 worker 支持比 perfork 高的并发），并发过大会榨干服务器资源。Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置 Nginx 主进程的工作进程的数量）(epoll)，不会为每个请求分配 cpu 和内存资源，节省了大量资源，同时也减少了大量的 
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: CPU 的上下文切换。所以才使得 Nginx 支持更高的并发。nginx 和 apache 的区别轻量级，同样起 web 服务，比 apache 占用更少的内存和资源。抗并发，nginx 处理请求是异步非阻塞的，而 apache 则是阻塞性的，在高并发下 nginx 能保持低资源，低消耗高性能。高度模块化的设计，编写模块相对简单。最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，nginx是异步的，多个连接可以对应一个进程什么是动态资源、静态资源分离？动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。为什么要做动、静分离？在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。什么叫 CDN 服务？CDN ，即内容分发网络。其目的是，通过在现有的 Internet 中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务Nginx 怎么做的动静分离？只需要指定路径对应的目录。locatio
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n/可以使用正则表达式匹配。并指定对应的硬盘中的目录Nginx 负载均衡的算法怎么实现的?策略有哪些?为了避免服务器崩溃，大家会通过负载均衡的方式来分担服务器压力。将对台服务器组成一个集群，当用户访问时，先访问到一个转发服务器，再由转发服务器将访问分发到压力更小的服务器。Nginx 负载均衡实现的策略有以下种：1 .轮询(默认)每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。upstream backserver {server 192.168.0.12;server 192.168.0.13;}2. 权重 weightweight 的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。# 权重越高，在被访问的概率越大，如上例，分别是 20%，80%。upstream backserver {server 192.168.0.12 weight=2;server 192.168.0.13 weight=8;}3. ip_hash( IP 绑定)每个请求按访问 IP 的哈希结果分配，
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 使来自同一个 IP 的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的 session 共享问题upstream backserver {ip_hash;server 192.168.0.12:88;server 192.168.0.13:80;}Nginx 虚拟主机怎么配置?1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台3、基于 ip 的虚拟主机。location 的作用是什么？location 指令的作用是根据用户请求的 URI 来执行不同的应用，也就是根据用户请求的网站 URL 进行匹配，匹配成功即进行相关的操作限流怎么做的？Nginx 限流就是限制用户请求速度，防止服务器受不了限流有 3种正常限制访问频率（正常流量）突发限制访问频率（突发流量）限制并发连接数Nginx 的限流都是基于漏桶流算法漏桶流算法和令牌桶算法知道？漏桶算法漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 续进入的水直接溢出（拒绝请求），以此实现限流。令牌桶算法令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。Nginx 配置高可用性怎么配置？当上游服务器(真实访问服务器)，一旦出现故障或者是没有及时相应的话，应该直接轮训到下一台服务器，保证服务器的高可用生产中如何设置 worker 进程的数量呢？在有多个 cpu 的情况下，可以设置多个 worker，worker 进程的数量可以设置到和 cpu 的核心数一样多，如果在单个 cpu 上起多个 worker 进程，那么操作系统会在多个 worker 之间进行调度，这种情况会降低系统性能，如果只有一个 cpu，那么只启动一个 worker 进程就可以了。Java 基础八股文Java 语言具有哪些特点？Java 为纯面向对象的语言。它能够直接反应现实生活中的
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象。具有平台无关性。Java 利用 Java 虚拟机运行字节码，无论是在 Windows、Linux还是 MacOS 等其它平台对 Java 程序进行编译，编译后的程序可在其它平台运行。Java 为解释型语言，编译器把 Java 代码编译成平台无关的中间代码，然后在JVM 上解释运行，具有很好的可移植性。Java 提供了很多内置类库。如对多线程支持，对网络通信支持，最重要的一点是提供了垃圾回收器。Java 具有较好的安全性和健壮性。Java 提供了异常处理和垃圾回收机制，去除了 C++中难以理解的指针特性JDK 与 JRE 有什么区别？JDK：Java 开发工具包（Java Development Kit），提供了 Java 的开发环境和运行环境。JRE：Java 运行环境(Java Runtime Environment)，提供了 Java 运行所需的环境。JDK 包含了 JRE。如果只运行 Java 程序，安装 JRE 即可。要编写 Java 程序需安装 JDK简述 Java 基本数据类型byte: 占用 1 个字节，取值范围-128 ~ 127short: 占用 2 个字节，取值范围-215 ~ 21
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 5-1int：占用 4 个字节，取值范围-231 ~ 231-1long：占用 8 个字节float：占用 4 个字节double：占用 8 个字节char: 占用 2 个字节boolean：占用大小根据实现虚拟机不同有所差异简述自动装箱拆箱对于 Java 基本数据类型，均对应一个包装类。装箱就是自动将基本数据类型转换为包装器类型，如 int->Integer拆箱就是自动将包装器类型转换为基本数据类型，如 Integer->int简述 Java 访问修饰符default: 默认访问修饰符，在同一包内可见private: 在同一类内可见，不能修饰类protected : 对同一包内的类和所有子类可见，不能修饰类public: 对所有类可见构造方法、成员变量初始化以及静态成员变量三者的初始化顺序？先后顺序：静态成员变量、成员变量、构造方法。详细的先后顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。面向对象的三大特性？继承：对象的一个新类可以从现有的类中派生，派生类可以从它的基类那继承方法和实例变量，且
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 派生类可以修改或新增新的方法使之更适合特殊的需求。封装：将客观事物抽象成类，每个类可以把自身数据和方法只让可信的类或对象操作，对不可信的进行信息隐藏。多态：允许不同类的对象对同一消息作出响应。不同对象调用相同方法即使参数也相同，最终表现行为是不一样的为什么 Java 语言不支持多重继承？为了程序的结构能够更加清晰从而便于维护。假设 Java 语言支持多重继承，类C 继承自类 A 和类 B，如果类 A 和 B 都有自定义的成员方法 f()，那么当代码中调用类 C 的 f() 会产生二义性。Java 语言通过实现多个接口间接支持多重继承，接口由于只包含方法定义，不能有方法的实现，类 C 继承接口 A 与接口 B 时即使它们都有方法 f()，也不能直接调用方法，需实现具体的 f()方法才能调用，不会产生二义性。多重继承会使类型转换、构造方法的调用顺序变得复杂，会影响到性能Java 提供的多态机制？Java 提供了两种用于多态的机制，分别是重载与覆盖(重写)。重载：重载是指同一个类中有多个同名的方法，但这些方法有不同的参数，在编译期间就可以确定调用哪个方法。覆盖：覆盖是指派生类重写基类的方法，使用基类指向其子类的实例
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象，或接口的引用变量指向其实现类的实例对象，在程序调用的运行期根据引用变量所指的具体实例对象调用正在运行的那个对象的方法，即需要到运行期才能确定调用哪个方法重载与覆盖的区别？覆盖是父类与子类之间的关系，是垂直关系；重载是同一类中方法之间的关系，是水平关系。覆盖只能由一个方法或一对方法产生关系；重载是多个方法之间的关系。覆盖要求参数列表相同；重载要求参数列表不同。覆盖中，调用方法体是根据对象的类型来决定的，而重载是根据调用时实参表与形参表来对应选择方法体。重载方法可以改变返回值的类型，覆盖方法不能改变返回值的类型。接口和抽象类的相同点和不同点？相同点:都不能被实例化。接口的实现类或抽象类的子类需实现接口或抽象类中相应的方法才能被实例化。不同点：接口只能有方法定义，不能有方法的实现，而抽象类可以有方法的定义与实现。实现接口的关键字为 implements，继承抽象类的关键字为 extends。一个类可以实现多个接口，只能继承一个抽象类。当子类和父类之间存在逻辑上的层次结构，推荐使用抽象类，有利于功能的累积。当功能不需要，希望支持差别较大的两个或更多对象间的特定交互行为，推荐使用接口。使用接口能降低软件系统的耦合
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度，便于日后维护或添加删除方法Java 语言中关键字 static 的作用是什么？static 的主要作用有两个：为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。具体而言 static 又可分为 4 种使用方式：修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用。修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法。修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次。修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方法为什么要把 String 设计为不可变？节省空间：字符串常量存储在 JVM 的字符串池中可以被用户共享。提高效率：String 可以被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。安全：String 常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改简述 String/StringBuffer 与 StringBuilderString 类采用利用 final 修饰的字符数组进行字符串保存，因此不可变。如果对 String 类型对象修改，需要新建对象，将老字符和新增加的字符一并存进去。StringBuilder，采用无 final 修饰的字符数组进行保存，因此可变。但线程不安全。StringBuffer，采用无 final 修饰的字符数组进行保存，可理解为实现线程安全的 StringBuilder。判等运算符==与 equals 的区别？== 比较的是引用，equals 比较的是内容。如果变量是基础数据类型，== 用于比较其对应值是否相等。如果变量指向的是对象，== 用于比较两个对象是否指向同一块存储空间。equals 是 Object 类提供的方法之一，每个 J
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 类都继承自 Object 类，所以每个对象都具有 equals 这个方法。Object 类中定义的 equals 方法内部是直接调用 == 比较对象的。但通过覆盖的方法可以让它不是比较引用而是比较数据内容简述 Java 异常的分类Java 异常分为 Error（程序无法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。Error 常见的有 StackOverFlowError、OutOfMemoryError 等等。Exception 可分为运行时异常和非运行时异常。对于运行时异常，可以利用 trycatch 的方式进行处理，也可以不处理。对于非运行时异常，必须处理，不处理的话程序无法通过编译final、finally 和 finalize 的区别是什么？final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承。finally 作为异常处理的一部分，只能在 try/catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下。finalize 是 Object 类的一个方法
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存简述 Java 中 Class 对象java 中对象可以分为实例对象和 Class 对象，每一个类都有一个 Class 对象，其包含了与该类有关的信息。获取 Class 对象的方法：Class.forName(“类的全限定名”)实例对象.getClass()类名.classJava 反射机制是什么？Java 反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得 Java 具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射 API。简述 Java 序列化与反序列化的实现序列化：将 java 对象转化为字节序列，由此可以通过网络对象进行传输。反序列化：将字节序列转化为 java 对象。具体实现：实现 Serializable 接口，或实现 Externalizable 接口中的writeExte
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: rnal()与 readExternal()方法。简述 Java 的 ListList 是一个有序队列，在 Java 中有两种实现方式:ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。Java 中线程安全的基本数据结构有哪些HashTable: 哈希表的线程安全版，效率低ConcurrentHashMap：哈希表的线程安全版，效率高，用于替代 HashTableVector：线程安全版 ArraylistStack：线程安全版栈BlockingQueue 及其子类：线程安全版队列简述 Java 的 SetSet 即集合，该数据结构不允许元素重复且无序。Java 对 Set 有三种实现方式：HashSet 通过 HashMap 实现，HashMap 的 Key 即 HashSet 存储的元素，Value系统自定义一个名为 PRESENT 的 Object 类型常量。判断元素是否相同时，先比较 hashCode，相同后再利用 equ
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: als 比较，查询 O(1)LinkedHashSet 继承自 HashSet，通过 LinkedHashMap 实现，使用双向链表维护元素插入顺序。TreeSet 通过 TreeMap 实现的，底层数据结构是红黑树，添加元素到集合时按照比较规则将其插入合适的位置，保证插入后的集合仍然有序。查询 O(logn)简述 Java 的 HashMapJDK8 之前底层实现是数组 + 链表，JDK8 改为数组 + 链表/红黑树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因子 loadFactor。HashMap 中数据以键值对的形式存在，键对应的 hash 值用来计算数组下标，如果两个元素 key 的 hash 值一样，就会发生哈希冲突，被放到同一个链表上。table 数组记录 HashMap 的数据，每个下标对应一条链表，所有哈希冲突的数据都会被存放到同一条链表，Node/Entry 节点包含四个成员变量：key、value、next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红黑树为何 HashMap 线程不安全在 JDK1.7 中，HashMap 采用头插法插入元素
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，因此并发情况下会导致环形链表，产生死循环。虽然 JDK1.8 采用了尾插法解决了这个问题，但是并发下的 put 操作也会使前一个 key 被后一个 key 覆盖。由于 HashMap 有扩容机制存在，也存在 A 线程进行扩容后，B 线程执行 get 方法出现失误的情况。简述 Java 的 TreeMapTreeMap 是底层利用红黑树实现的 Map 结构，底层实现是一棵平衡的排序二叉树，由于红黑树的插入、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表无法提供键值对的有序输出，红黑树可以按照键的值的大小有序输出ArrayList、Vector 和 LinkedList 有什么共同点与区别？ArrayList、Vector 和 LinkedList 都是可伸缩的数组，即可以动态改变长度的数组。ArrayList 和 Vector 都是基于存储元素的 Object[] array 来实现的，它们会在内存中开辟一块连续的空间来存储，支持下标、索引访问。但在涉及插入元素时可能需要移动容器中的元素，插入效率较低。当存储元素超过容器的初始化容量大小，ArrayList 与 Vector 均会进
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行扩容。Vector 是线程安全的，其大部分方法是直接或间接同步的。ArrayList 不是线程安全的，其方法不具有同步性质。LinkedList 也不是线程安全的。LinkedList 采用双向列表实现，对数据索引需要从头开始遍历，因此随机访问效率较低，但在插入元素的时候不需要对数据进行移动，插入效率较高HashMap 和 Hashtable 有什么区别？HashMap 是 Hashtable 的轻量级实现，HashMap 允许 key 和 value 为 null，但最多允许一条记录的 key 为 null.而 HashTable 不允许。HashTable 中的方法是线程安全的，而 HashMap 不是。在多线程访问 HashMap需要提供额外的同步机制。Hashtable 使用 Enumeration 进行遍历，HashMap 使用 Iterator 进行遍历。如何决定使用 HashMap 还是 TreeMap?如果对 Map 进行插入、删除或定位一个元素的操作更频繁，HashMap 是更好的选择。如果需要对 key 集合进行有序的遍历，TreeMap 是更好的选择Collection 和 Colle
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ctions 有什么区别？Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。Collections 是一个包装类，包含了很多静态方法、不能被实例化，而是作为工具类使用，比如提供的排序方法：Collections.sort(list);提供的反转方法：Collections.reverse(list)。Java 并发编程1.并行跟并发有什么区别？并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。你是如何理解线程安全的？如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。可以从三个要素来确保线程安全：1 、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态2 、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行2.说说进程和线程的区别？进
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤：线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中3. 说说线程有几种创建方式？分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象。还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。调用 start 方法时会执行 run 方法，那怎么不直接调用 run 方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。说说 wait 方法和 notify 方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到：线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法、当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。如果调用的是 notifyAll 方法，会唤醒所
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有在这个共享变量上调用 wait 系列方法而被挂起的线程。说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。6.线程有几种状态？6 种。new 代表线程被创建但未启动；runnable 代表线程处于就绪或正在运行状态，由操作系统调度；blocked 代表线程被阻塞，等待获取锁；waiting 代表线程等待其他线程的通知或中断；timed_waiting 代表线程会等待一段时间，超时后自动恢复；terminated 代表线程执行完毕，生命周期结束。什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程使用完时间片后，就会让出 CPU 让其他线程占用。守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM内部，同时还启动了很多守护线程，比如垃圾回收线程。守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。sleep() 方法专属于 Thread 类。wait() 方法专属于 Object 类。waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。而当线程执行 wait 方法时，它会释放持有的对象锁，
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 因此其他线程也有机会获取该对象的锁。有个 int 的变量为 0，十个线程轮流对其进行++操作（循环 10000 次），结果大于 10 万还是小于等于 10 万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。读取变量的值。将读取到的值加 1。将结果写回变量。这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。详细解释下。多个线程在并发执行 ++ 操作时，可能出现以下竞态条件：线程 1 读取变量值为 0。线程 2 也读取变量值为 0。线程 1 进行加法运算并将结果 1 写回变量。线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。.ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程隔离。在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry数组，key 是 Thread
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Local 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value设置为 null，这样可以在很大程度上避免内存泄漏。ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。3、Map 的大小由 ThreadLocal 对象的多少决定。15.ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove()
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法释放内存空间。那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。ThreadLocalMap 的源码看过吗？有研究过。ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位：如果 i 位置被占用，尝试 i+1。如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。如果到达数组末尾，则回到数组头部，继续寻找空位。为什么要用线性探测法而不是 HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。拉链法还需要单独维护一个链表，甚至红黑树，
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不适合 ThreadLocal 这种场景ThreadLocalMap 扩容机制了解吗？了解。与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。父线程能用 ThreadLocal 给子线程传值吗？不能。因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。可以使用 InheritableThreadLocal 来解决这个问题。InheritableThreadLocal 的原理了解吗？了解。在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap：普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 newThread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有inheritableThreadLocals，
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 如果有，就会拷贝 InheritableThreadLocal 变量到子线程：说一下你对 Java 内存模型的理解？Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突volatile 了解吗？了解。第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为：StoreStore 屏障可以禁止普通写操作与 volatile 写操作的重排StoreLoad 屏障会禁止 volatile 写与 volatile 读重排LoadLoad 屏障会禁止 volatile 读与后续普通读操作重排LoadStore 屏障会禁止 volatile 读与后续普通写操作重排volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。锁synchronized 用过吗？用过，频率还
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 很高。synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象this。synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步：前者表示线程正在尝试获取 lock 对象的 Monitor；后者表示线程执行完了同步代码块，正在释放锁。你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——MarkWord，用于存储锁的状态，以及 Monito
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: r 对象的指针。synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。synchronized 怎么保证可见性？通过两步操作：加锁时，线程必须从主内存读取最新数据。释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 标记为未持有状态，并清除线程 ID 信息。synchronized 锁升级了解吗？JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。了解 synchronized 四种锁状态吗？了解。①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。未持有锁的线程通过 CAS 自旋等待锁释放④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，轻量级锁就会升级为重量级锁。此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。随着 JDK 版本的更新，synchronized 的性能得到了极大的优化：①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销详细解释一下：①、从无锁到偏向锁：当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。②、偏向锁的轻量级锁：进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。③、轻量级锁到重量级锁：轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。30.synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。synchronized 可以自动加锁和解锁，ReentrantLoc
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: k 需要手动 lock() 和 unlock()。并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为：ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制AQS 了解多少？AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为ReentrantLock 等类提供底层支持。AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。内部通过一个计数器 state 来跟踪锁的状态和
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。非公平锁和公平锁有什么不同？两句话回答：公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序CAS 了解多少？CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。在 CAS 中，有三个值：V：要更新的变量(var)E：预期值(expected)N：新值(new)先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 当前线程就放弃更新。这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe类实现的。怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS会误认为这个值没有发生变化。可以使用版本号/时间戳的方式来解决 ABA 问题。比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新Java 有哪些保证原子性的方法？比如说以 Atomic 开头的原子类，synchroni
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: zed 关键字，ReentrantLock 锁等原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有AtomicInteger、AtomicLong、AtomicReference 等。线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁R2；线程 2 持有锁 R2，等待锁 R1。第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请聊聊线程同步和互斥？
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: （补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程A 先执行，线程 B 再执行。互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费CPU 资源。聊聊悲观锁和乐观锁？（补充）悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。乐观锁认为冲突不
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。CountDownLatch 了解吗？CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。场景题：假如要查 10万多条数据，用线程池分成 20 个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。第三步，主线程调用 await() 方法，等待所有线程执行完毕。CyclicBarrier 和 CountDownLatch 有什么
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。说一下 JDK 8 中 ConcurrentHashMap 的实现原理？JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS的方式初始化，以确保只有一个线程在初始化数组。第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为synchronized 代码块来插入节点。插入的过程中会判断桶的哈希是否小于 0（f.hash >= 0），小于 0 说明是红黑树，大于等于 0 说明是链表。这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash值固定为 -2。第三步，如果链表长度超过 8，转换为红黑树。第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了ReentrantL
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ock，这样可以保证每个 Segment 都可以独立地加锁。而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized关键字。而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。比如说 put 的时候优先使用 CAS 尝试
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 插入，如果失败再使用 synchronized 代码块加锁。get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。说一下线程池的工作流程？可以简单总结为：任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。第一步，线程池通过 submit() 提交任务。第二步，线程池会先创建核心线程来执行任务。第三步，如果核心线程都在忙，任务会被放入任务队列中。第四步，如果任务
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。另外一版回答。第一步，创建线程池。第二步，调用线程池的 execute()方法，准备执行任务。如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务；如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列；如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务；如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。①、corePoolSize：核心线程数，长期存活，执行任务的主力。②、maxim
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: umPoolSize：线程池允许的最大线程数。③、workQueue：任务队列，存储等待执行的任务。④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。线程池的拒绝策略有哪些？AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute方法的线程。DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedB
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: lockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列SynchronousQueue。1 、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。2 、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是Integer.MAX_VALUE，几乎相当于一个无界队列。3 、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务4 、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。5 、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。submit 有返回值，适用于需要获取结果或
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 处理异常的场景。线程池怎么关闭知道吗？可以调用线程池的 shutdown 或 shutdownNow方法来关闭线程池。shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？1 、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。2 、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。有哪几种常见的线程池？主要有四种：固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等缓存线程池 Executors.newCach
2025-08-11 10:55:37.057 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: edThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。能说一下四种常见线程池的原理吗？说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 & 周期性任务执行，使用DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate()方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 延迟执行。缺点是，如果任务执行时间 > 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。状态由 RUNNING→ SHUTDOWN→ STOP → TIDYING → TERMINATED 依次流转。RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，比如说setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点：第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中JVMJVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件；程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高说说 JVM 的组织架构（补充）JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器能说一下 JVM 的内存区域吗？按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 C/C++编写。在本地方法栈中，主要存放了 native
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。详细说明一下：比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。介绍一下方法区？方法区并不真实存在，属于 J
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 虚拟机规范中的一个逻辑概念，用于存储已被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的GC 分代年龄等信息。最后，JVM 会执行构造方法 <init> 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。new 对象时，堆会发生抢占吗？new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存，两个线程就发生了抢占JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。能说一下对象的内存布局吗？对象在内存中包括三部分：对象头、实例数据和对齐填充说说对象头的作用？对象头是对象存储在内存中的元信息，包含了 Mark Word、类型指针等信息。对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。比如说，如果对象头 + 实例数据 = 14 字节，则需要填充 2 个字节，使总长度变为 16 字节为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 就会降低。new Object() 对象的内存大小是多少？一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16字节；如果开启了压缩指针，就是 12 字节。实例数据的大小取决于对象的成员变量和它们的类型。对于 new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。对象的引用大小了解吗？在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。、在直接指针访问中，引用直接存储对象的内存地址；对象的实
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例数据和类型信息都存储在堆中固定的内存区域。说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。对
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。STW 了解吗？了解。JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为 Stop TheWorld。简称 STW。如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：JVM 发出暂停信号；线程执行到安全点后，挂起自身并等待垃圾收集完成；垃圾回收器完成 GC 操作；线程恢复执行对象一定分配在堆中吗？不一定。默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程逃逸分析会带来什么好处？主要有三个。第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 无法被回收而导致的内存泄漏可能由哪些原因导致呢？静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。数据库、IO、Socket 等连接资源没有及时关闭；ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。有没有处理过内存泄漏问题？当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出StackOverflowError。讲讲 JVM 的垃圾回收机制（补充）垃圾回收就
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩/整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。这也是 G1、CMS 等主流垃圾收集器使用的主要算法。什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失Java 中可作为 GC Roots 的引用有哪几种？所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种虚拟机栈中的引用（方法的参数、局部变量等）本地方法栈中 JNI 的引用类静态变量运行时常量池中的常量（String 或 Class 类型）finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。筛选的条件是对象是否有必要执行 finalize()方法。如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法说说标记-清除算法？标记-清除算法分为两个阶段：标记：标记所有需要回收的对
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象清除：回收所有被标记的对象优点是实现简单，缺点是回收过程中会产生内存碎片说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。缺点是浪费了一半的内存空间。说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。缺点是移动对象的成本比较高。说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 集。年轻代包含 Eden 区以及两个 Survivor 区Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生FULL gc 怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 < 新生代历次Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一次Full GC。知道哪些垃圾收集器？JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。说说 CMS 收集器？CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC能详细说一下 CMS 的垃圾收集过程吗？CMS 使用标记-清除算法进行垃圾收集，分 4 大步：初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。并发清除：清除未被标记的对象，回收它们占用的内存空间。三色标记法的工作流程：①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。②、并发标记（Concurrent Marking）：在此
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。说说 G1 收集器？G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。G1 收集器的运行过程大致可划分为这几个步骤：①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区域和老年代区域。选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。3 、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1会尽可能地在这个时间内完成垃圾回收。CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 序使用的，并释放这些内存以便重新使用。这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。G1 非常适合大内存、多核处理器的环境。工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间JVM 调优37. 用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。你一般都怎么用 jmap？我一般会使用 jmap -heap <pid> 查看堆内存摘要，包括新生代、老年代、元空间等。
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 或者使用 jmap -histo <pid> 查看对象分布了解哪些可视化的性能监控工具？JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？-Xms：初始堆大小-Xmx：最大堆大小-XX:NewSize=n：设置年轻代大小-XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 1/4-XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n=3表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 1/5做过 JVM 调优吗？JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。在项目运行期间，我会使用 JVisualVM
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等CPU 占用过高怎么排查？首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。接着，使用 jstack 命令查看对应进程的线程堆栈信息。然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程ID接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收排查的方法主要分为以下几步：第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。或者使用 jmap 
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。第二步，通过 jmap 命令 dump 出堆内存信息第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过-XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。调整为 6 的话，会减少
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象该怎么排查 Full GC 频繁问题？通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过-XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。或者将大对象拆分成小对象，减少大对象的创建。比如说分页。假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说ThreadLocal、数据库连接、IO 资源等。了解类的加载机制吗？（补充）JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。类加载过程：包括加载、验证、准备、解析和初始化等步骤。双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。以下是整理自网络的一些 JVM 调优实例：网站流量浏览量暴增后，网站反应页面响很慢问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。定位：为了确认推测的正确性，在线上通过 jstat -gc 指令 看到 JVM 进行 GC 次数频率非常高，GC 所占用的时间非常长，所以基本推断就是因为 GC 频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。解决
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁 GC，所以这里问题在于新生代内存太小，所以这里可以增加 JVM 内存就行了，所以初步从原来的 2G 内存增加到 16G 内存。第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次 GC 的时间变长从而导致间接性的卡顿。定位：还是通过 jstat -gc 指令 查看到 的确 FGC 次数并不是很高，但是花费在 FGC 上的时间是非常高的,根据 GC 日志 查看到单次 FGC 的时间有达到几十秒的。解决方案： 因为 JVM 默认使用的是 PS+PO 的组合，PS+PO 垃圾标记和收集阶段都是 STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次 GC 时间过长，所以需要更换并发类的收集器，因为当前的 JDK 版本为 1.7，所以最后选择 CMS 垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。公司的后台系统
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，偶发性的引发 OOM 异常，堆内存溢出。因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从 4G 调整到 8G。但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError 参数 获得堆内存的 dump 文件。VisualVM 对 堆 dump 文件进行分析，通过 VisualVM 查看到占用内存最大的对象是 String 对象，本来想跟踪着 String 对象找到其引用的地方，但 dump 文件太大，跟踪进去的时候总是卡死，而 String 对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成 excel，这个过程会产生大量的 String 对象为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和 EXCEL 对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出jvm.gc.time：每分钟的 GC 耗时在 1s 以内，500ms 以内尤佳jvm.gc.meantime：每次 YGC 耗时在 100ms 以内，50ms 以内尤佳jvm.fullgc.count：FGC 最多几小时 1次，1天不到 1次尤佳jvm.fullgc.time：每次 FGC 耗时在 1s 以内，500ms 以内尤佳// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pidSpringSpring 是什么？Spring 是一个 Java 后端开发框架，其最核心的作用是帮我们管理 Java 对象其最重要的特性就是 IoC，也就是控制反转。以前我们要使用一个对象时，都要自己先 new 出来。但有了 Spring 之后，我们只需要告诉 Spr
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ing 我们需要什么对象，它就会自动帮我们创建好并注入到 Spring 容器当中。比如我在一个Service 类里需要用到 Dao 对象，只需要加个 @Autowired 注解，Spring 就会自动把 Dao 对象注入到 Spring 容器当中，这样就不需要我们手动去管理这些对象之间的依赖关系了。另外，Spring 还提供了 AOP，也就是面向切面编程，在我们需要做一些通用功能的时候特别有用，比如说日志记录、权限校验、事务管理这些，我们不用在每个方法里都写重复的代码，直接用 AOP 就能统一处理。Spring 的生态也特别丰富，像 Spring Boot 能让我们快速搭建项目，Spring MVC能帮我们处理 web 请求，Spring Data 能帮我们简化数据库操作，Spring Cloud能帮我们做微服务架构等等Spring 有哪些特性？首先最核心的就是 IoC 控制反转和 DI 依赖注入。这个我前面也提到了，就是Spring 能帮我们管理对象的创建和依赖关系。第二个就是 AOP 面向切面编程。这个在我们处理一些横切关注点的时候特别有用，比如说我们要给某些 Controller 方法都加上权限控制，如
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果没有 AOP 的话，每个方法都要写一遍加权代码，维护起来很麻烦。用了 AOP 之后，我们只需要写一个切面类，定义好切点和通知，就能统一处理了。事务管理也是同样的道理，加个 @Transactional 注解就搞定了。简单说一下什么是 AOP 和 IoC？AOP 面向切面编程，简单点说就是把一些通用的功能从业务代码里抽取出来，统一处理。比如说技术派中的 @MdcDot 注解的作用是配合 AOP 在日志中加入 MDC信息，方便进行日志追踪。IoC 控制反转是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。Spring 有哪些模块呢？首先是 Spring Core 模块，这是整个 Spring 框架的基础，包含了 IoC 容器和依赖注入等核心功能。还有 Spring Beans 模块，负责 Bean 的配置和管理。这两个模块基本上是其他所有模块的基础，不管用 Spring 的哪个功能都会用到。然后是 Spring Context 上下文模块，它在 Core 的基础上提供了更多企业级的功能，比如国际化、事件传播、资源加载这些。ApplicationContext 就是在这
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个模块里面的。Spring AOP 模块提供了面向切面编程的支持，我们用的@Transactional、自定义切面这些都是基于这个模块。Web 开发方面，Spring Web 模块提供了基础的 Web 功能，Spring WebMVC 就是我们常用的 MVC 框架，用来处理 HTTP 请求和响应。现在还有 Spring WebFlux，支持响应式编程。还有一些其他的模块，比如 Spring Security 负责安全认证，Spring Batch 处理批处理任务等等。现在我们基本都是用 Spring Boot 来开发，它把这些模块都整合好了，用起来更方便。Spring 有哪些常用注解呢？Spring 的注解挺多的，我按照不同的功能分类来说一下平时用得最多的那些。首先是 Bean 管理相关的注解。@Component 是最基础的，用来标识一个类是Spring 组件。像 @Service、@Repository、@Controller 这些都是 @Component的特化版本，分别用在服务层、数据访问层和控制器层。依赖注入方面，@Autowired 是用得最多的，可以标注在字段、setter 方法或者构造方法上。
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: @Qualifier 在有多个同类型 Bean 的时候用来指定具体注入哪一个。@Resource 和 @Autowired 功能差不多，不过它是按名称注入的。配置相关的注解也很常用。@Configuration 标识配置类，@Bean 用来定义 Bean，@Value 用来注入配置文件中的属性值。我们项目里的数据库连接信息、Redis 配置这些都是用 @Value 来注入的。@PropertySource 用来指定配置文件的位置。@RequestMapping 及其变体@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 用来映射 HTTP 请求。@PathVariable 获取路径参数，@RequestParam 获取请求参数，@RequestBody 接收 JSON 数据。、AOP 相关的注解，@Aspect 定义切面，@Pointcut 定义切点，@Before、@After、@Around 这些定义通知类型不过我们用得最多的还是@Transactional，基本上 Service 层需要保证事务原子性的方法都会加上这个注解。Spring 用了哪些设计模
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 式？Spring 框架里面确实用了很多设计模式，我从平时工作中能观察到的几个来说说。首先是工厂模式，这个在 Spring 里用得非常多。BeanFactory 就是一个典型的工厂，它负责创建和管理所有的 Bean 对象。我们平时用的 ApplicationContext其实也是 BeanFactory 的一个实现。当我们通过 @Autowired 获取一个 Bean的时候，底层就是通过工厂模式来创建和获取对象的。单例模式也是 Spring 的默认行为。默认情况下，Spring 容器中的 Bean 都是单例的，整个应用中只会有一个实例。这样可以节省内存，提高性能。当然我们也可以通过 @Scope 注解来改变 Bean 的作用域，比如设置为 prototype 就是每次获取都创建新实例。代理模式在 AOP 中用得特别多。Spring AOP 的底层实现就是基于动态代理的，对于实现了接口的类用 JDK 动态代理，没有实现接口的类用 CGLIB 代理。比如我们用 @Transactional 注解的时候，Spring 会为我们的类创建一个代理对象，在方法执行前后添加事务处理逻辑。Spring 如何实现单例模式？传统的
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 单例模式是在类的内部控制只能创建一个实例，比如用 private 构造方法加 static getInstance() 这种方式。但是 Spring 的单例是容器级别的，同一个 Bean 在整个 Spring 容器中只会有一个实例。具体的实现机制是这样的：Spring 在启动的时候会把所有的 Bean 定义信息加载进来，然后在 DefaultSingletonBeanRegistry 这个类里面维护了一个叫singletonObjects 的 ConcurrentHashMap，这个 Map 就是用来存储单例 Bean的。key 是 Bean 的名称，value 就是 Bean 的实例对象。当我们第一次获取某个 Bean 的时候，Spring 会先检查 singletonObjects 这个 Map 里面有没有这个 Bean，如果没有就会创建一个新的实例，然后放到 Map 里面。后面再获取同一个 Bean 的时候，直接从 Map 里面取就行了，这样就保证了单例。还有一个细节就是 Spring 为了解决循环依赖的问题，还用了三级缓存。除了singletonObjects 这个一级缓存，还有 earlySingl
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: etonObjects 二级缓存和singletonFactories 三级缓存。这样即使有循环依赖，Spring 也能正确处理Spring 容器和 Web 容器之间的区别知道吗？（补充）首先从概念上来说，Spring 容器是一个 IoC 容器，主要负责管理 Java 对象的生命周期和依赖关系。而 Web 容器，比如 Tomcat、Jetty 这些，是用来运行 Web应用的容器，负责处理 HTTP 请求和响应，管理 Servlet 的生命周期。从功能上看，Spring 容器专注于业务逻辑层面的对象管理，比如我们的 Service、Dao、Controller 这些 Bean 都是由 Spring 容器来创建和管理的。而 Web 容器主要处理网络通信，比如接收 HTTP 请求、解析请求参数、调用相应的 Servlet，然后把响应返回给客户端在实际项目中，这两个容器是相辅相成的。我们的 Web 项目部署在 Tomcat 上的时候，Tomcat 会负责接收 HTTP 请求，然后把请求交给 DispatcherServlet处理，而 DispatcherServlet 又会去 Spring 容器中查找相应的 Cont
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: roller来处理业务逻辑。现在我们都用 Spring Boot 了，Spring Boot 内置了 Tomcat，把 Web 容器和Spring 容器都整合在一起了，我们只需要运行一个 jar 包就可以了说一说什么是 IoC？IoC 的全称是 Inversion of Control，也就是控制反转。这里的“控制”指的是对象创建和依赖关系管理的控制权。以前我们写代码的时候，如果 A 类需要用到 B 类，我们就在 A 类里面直接 new一个 B 对象出来，这样 A 类就控制了 B 类对象的创建。有了 IoC 之后，这个控制权就“反转”了，不再由 A 类来控制 B 对象的创建，而是交给外部的容器来管理。DI 和 IoC 的区别了解吗？IoC 的思想是把对象创建和依赖关系的控制权由业务代码转移给 Spring 容器。这是一个比较抽象的概念，告诉我们应该怎么去设计系统架构。而 DI，也就是依赖注入，它是实现 IoC 这种思想的具体技术手段。在 Spring 里，我们用 @Autowired 注解就是在使用 DI 的字段注入方式。为什么要使用 IoC 呢？在日常开发中，如果我们需要实现某一个功能，可能至少需要两个以上
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系能说一下 IoC 的实现机制吗？好的，Spring IoC 的实现机制还是比较复杂的，我尽量用比较通俗的方式来解释一下整个流程。第一步是加载 Bean 的定义信息。Spring 会扫描我们配置的包路径，找到所有标注了 @Component、@Service、@Repository 这些注解的类，然后把这些类的元信息封装成 BeanDefinition 对象。第二步是 Bean 工厂的准备。Spring 会创建一个 DefaultListableBeanFactory作为 Bean 工厂来负责 Bean 的创建和管理。第三步是 Bean 的实例化和初始化。这个过程比较复杂，Spring 会根据BeanDefinition 来创建 Bean 实例。对于单例 Bean，Spring 会先检查缓存中是否已经存在，如果不存在就创建新实例。创建实例的时候会通过反射调用构造方法，然后进行属性注入，最后执行初始化回调方法。依赖注入的实现主
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 要是通过反射来完成的。比如我们用 @Autowired 标注了一个字段，Spring 在创建 Bean 的时候会扫描这个字段，然后从容器中找到对应类型的 Bean，通过反射的方式设置到这个字段上。你是怎么理解 Spring IoC 的？IoC 本质上一个超级工厂，这个工厂的产品就是各种 Bean 对象。我们通过 @Component、@Service 这些注解告诉工厂：“我要生产什么样的产品，这个产品有什么特性，需要什么原材料”。然后工厂里各种生产线，在 Spring 中就是各种 BeanPostProcessor。比如AutowiredAnnotationBeanPostProcessor 专门负责处理 @Autowired 注解。工厂里还有各种缓存机制用来存放产品，比如说 singletonObjects 是成品仓库，存放完工的单例 Bean；earlySingletonObjects 是半成品仓库，用来解决循环依赖问题。说说 BeanFactory 和 ApplicantContext 的区别?BeanFactory 算是 Spring 的“心脏”，而 ApplicantContext 可以说是Spri
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ng 的完整“身躯”。BeanFactory 提供了最基本的 IoC 能力。它就像是一个 Bean 工厂，负责 Bean的创建和管理。他采用的是懒加载的方式，也就是说只有当我们真正去获取某个Bean 的时候，它才会去创建这个 Bean。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上扩展了很多企业级的功能。它不仅包含了 BeanFactory 的所有功能，还提供了国际化支持、事件发布机制、AOP、JDBC、ORM 框架集成等等。ApplicationContext 采用的是饿加载的方式，容器启动的时候就会把所有的单例 Bean 都创建好，虽然这样会导致启动时间长一点，但运行时性能更好。另外一个重要的区别是生命周期管理。ApplicationContext 会自动调用 Bean的初始化和销毁方法，而 BeanFactory 需要我们手动管理。在 Spring Boot 项目中，我们可以通过 @Autowired 注入 ApplicationContext，或者通过实现 ApplicationContextAware 接口来获取 Applicatio
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: nContext。项目启动时 Spring 的 IoC 会做什么？第一件事是扫描和注册 Bean。IoC 容器会根据我们的配置，比如@ComponentScan 指定的包路径，去扫描所有标注了 @Component、@Service、@Controller 这些注解的类。然后把这些类的元信息包装成 BeanDefinition 对象，注册到容器的 BeanDefinitionRegistry 中。这个阶段只是收集信息，还没有真正创建对象。第二件事是 Bean 的实例化和注入。这是最核心的过程，IoC 容器会按照依赖关系的顺序开始创建 Bean 实例。对于单例 Bean，容器会通过反射调用构造方法创建实例，然后进行属性注入，最后执行初始化回调方法在依赖注入时，容器会根据 @Autowired、@Resource 这些注解，把相应的依赖对象注入到目标 Bean 中。比如 UserService 需要 UserDao，容器就会把UserDao 的实例注入到 UserService 中。说说 Spring 的 Bean 实例化方式？Spring 提供了 4 种方式来实例化 Bean，以满足不同场景下的需求第一种是通过
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法实例化，这是最常用的方式。当我们用 @Component、@Service 这些注解标注类的时候，Spring 默认通过无参构造器来创建实例的。如果类只有一个有参构造方法，Spring 会自动进行构造方法注入。第二种是通过静态工厂方法实例化。有时候对象的创建比较复杂，我们会写一个静态工厂方法来创建，然后用 @Bean 注解来标注这个方法。Spring 会调用这个静态方法来获取 Bean 实例。第三种是通过实例工厂方法实例化。这种方式是先创建工厂对象，然后通过工厂对象的方法来创建 Bean：第四种是通过 FactoryBean 接口实例化。这是 Spring 提供的一个特殊接口，当我们需要创建复杂对象的时候特别有用：你是怎么理解 Bean 的？在我看来，Bean 本质上就是由 Spring 容器管理的 Java 对象，但它和普通的Java 对象有很大区别。普通的 Java 对象我们是通过 new 关键字创建的。而Bean 是交给 Spring 容器来管理的，从创建到销毁都由容器负责。从实际使用的角度来说，我们项目里的 Service、Dao、Controller 这些都是Bean。比如 UserServ
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ice 被标注了 @Service 注解，它就成了一个 Bean，Spring会自动创建它的实例，管理它的依赖关系，当其他地方需要用到 UserService 的时候，Spring 就会把这个实例注入进去。这种依赖注入的方式让对象之间的关系变得松耦合。Spring 提供了多种 Bean 的配置方式，基于注解的方式是最常用的。@Component 和 @Bean 有什么区别？首先从使用上来说，@Component 是标注在类上的，而 @Bean 是标注在方法上的。@Component 告诉 Spring 这个类是一个组件，请把它注册为 Bean，而 @Bean 则告诉 Spring 请将这个方法返回的对象注册为 Bean。从控制权的角度来说，@Component 是由 Spring 自动创建和管理的。而 @Bean 则是由我们手动创建的，然后再交给 Spring 管理，我们对对象的创建过程有完全的控制权。能说一下 Bean 的生命周期吗？Bean 的生命周期可以分为 5 个主要阶段，我按照实际的执行顺序来说一下。第一个阶段是实例化。Spring 容器会根据 BeanDefinition，通过反射调用 Bean的
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法创建对象实例。如果有多个构造方法，Spring 会根据依赖注入的规则选择合适的构造方法。第二阶段是属性赋值。这个阶段 Spring 会给 Bean 的属性赋值，包括通过@Autowired、@Resource 这些注解注入的依赖对象，以及通过 @Value 注入的配置值第三阶段是初始化。这个阶段会依次执行：@PostConstruct 标注的方法InitializingBean 接口的 afterPropertiesSet 方法通过 @Bean 的 initMethod 指定的初始化方法初始化后，Spring 还会调用所有注册的 BeanPostProcessor 后置处理方法。这个阶段经常用来创建代理对象，比如 AOP 代理。第五阶段是使用 Bean。比如我们的 Controller 调用 Service，Service 调用DAO。最后是销毁阶段。当容器关闭或者 Bean 被移除的时候，会依次执行：@PreDestroy 标注的方法DisposableBean 接口的 destroy 方法通过 @Bean 的 destroyMethod 指定的销毁方法Aware 类型的接口有什么作用？Aware 
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 接口在 Spring 中是一个很有意思的设计，它们的作用是让 Bean 能够感知到 Spring 容器的一些内部组件。从设计理念来说，Aware 接口实现了一种“回调”机制。正常情况下，Bean 不应该直接依赖 Spring 容器，这样可以保持代码的独立性。但有些时候，Bean 确实需要获取容器的一些信息或者组件，Aware 接口就提供了这样一个能力。什么是自动装配？自动装配的本质就是让 Spring 容器自动帮我们完成 Bean 之间的依赖关系注入，而不需要我们手动去指定每个依赖。简单来说，就是“我们不用告诉 Spring具体怎么注入，Spring 自己会想办法找到合适的 Bean 注入进来”。自动装配的工作原理简单来说就是，Spring 容器在启动时自动扫描@ComponentScan 指定包路径下的所有类，然后根据类上的注解，比如@Autowired、@Resource 等，来判断哪些 Bean 需要被自动装配。之后分析每个 Bean 的依赖关系，在创建 Bean 的时候，根据装配规则自动找到合适的依赖 Bean，最后根据反射将这些依赖注入到目标 Bean 中Bean 的作用域有哪些Bean 的作用域决
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 定了 Bean 实例的生命周期和创建策略，singleton 是默认的作用域。整个 Spring 容器中只会有一个 Bean 实例。不管在多少个地方注入这个 Bean，拿到的都是同一个对象。生命周期和 Spring 容器相同，容器启动时创建，容器销毁时销毁。实际开发中，像 Service、Dao 这些业务组件基本都是单例的，因为单例既能节省内存，又能提高性能。当把 scope 设置为 prototype 时，每次从容器中获取 Bean 的时候都会创建一个新的实例。当需要处理一些有状态的 Bean 时会用到 prototype，比如每个订单处理器需要维护不同的状态信息如果作用于是 request，表示在 Web 应用中，每个 HTTP 请求都会创建一个新的 Bean 实例，请求结束后 Bean 就被销毁。如果作用于是 session，表示在 Web 应用中，每个 HTTP 会话都会创建一个新的 Bean 实例，会话结束后 Bean 被销毁。application 作用域表示在整个应用中只有一个 Bean 实例，类似于 singleton，但它的生命周期与 ServletContext 绑定。Spring 中的单
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例 Bean 会存在线程安全问题吗？首先要明确一点。Spring 容器本身保证了 Bean 创建过程的线程安全，也就是说不会出现多个线程同时创建同一个单例 Bean 的情况。但是 Bean 创建完成后的使用过程，Spring 就不管了换句话说，单例 Bean 在被创建后，如果它的内部状态是可变的，那么在多线程环境下就可能会出现线程安全问题单例 Bean 的线程安全问题怎么解决呢？第一种，使用局部变量，也就是使用无状态的单例 Bean，把所有状态都通过方法参数传递：第二种，当确实需要维护线程相关的状态时，可以使用 ThreadLocal 来保存状态。ThreadLocal 可以保证每个线程都有自己的变量副本，互不干扰。第三种，如果需要缓存数据或者计数，使用 JUC 包下的线程安全类，比如说AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。第四种，对于复杂的状态操作，可以使用 synchronized 或 Lock：第五种，如果 Bean 确实需要维护状态，可以考虑将其改为 prototype 作用域，这样每次注入都会创建一个新的实例，避免了多线程共享同
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个实例的问题。说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖：一级缓存：存放完全初始化好的单例 Bean。二级缓存：存放正在创建但未完全初始化的 Bean 实例。三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。三级缓存解决循环依赖的过程是什么样的？实例化 Bean 时，将其早期引用放入三级缓存。其他依赖该 Bean 的对象，可以从缓存中获取其引用。初始化完成后，将 Bean 移入一级缓存。假如 A、B 两个类发生循环依赖：A 实例的初始化过程：1 、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道2 、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。3 、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。4 、发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 已经实例化并且初始化完成了，把 B 放入⼀级缓存5 、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存6 、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor在 Bean 的初始化阶段，会通过 
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Bean 后置处理器来进行一些前置和后置的处理。实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是AutowiredAnnotationBeanPostProcessor。Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。AOP 有哪些核心概念？切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法切点（Pointcut）：对连接点进行拦截的定位通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强目标对象 （Target）：代理的目标对象引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。在 Spring 容器初始化 Bean的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 理对象中织入切面逻辑。这一过程发生在Spring 容器的后处理器（BeanPostProcessor）阶段。简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像@Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现AOP。③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。AOP 的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。第一步，自定义注解作为切点第二
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间第三步，在使用的地方加上自定义注解第四步，当接口被调用时，就可以看到对应的执行日志。说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。说说 Spring AOP 和 AspectJ AOP 区别?说
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。反射：运行时操作类的元信息的底层能力动态代理：基于反射实现方法拦截的设计模式Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚.我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部实现事务的控制。介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。说说两者的区别？编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务说说 Spring 的事务隔离级别？好，事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: TransactionDefinition 接口中。①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 ），MySQL默认的是可重复读，Oracle 默认的读已提交。②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 下文如何在方法调用链中传播。Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。事务传播机制是使用ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。第一步，在 Bean 初始化阶段创建代理对象：Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性切面，也就是@Transactional 注解及其属性值。然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。第二步，在执行目标方法时进行事务增强操作：当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为 MethodInterceptor，事务增强对该接口的实现为 TransactionInterceptor，@Transactional 应用在非 public 修饰的方法上如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。Spring MVC 的核心组件？DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping将不同
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的请求映射到不同的 Handler。HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。Spring MVC 的
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。然后 DispatcherServlet 通过 ViewResolver 解析视图。最后，DispatcherServlet 渲染视图并将响应返回给客户端①、发起请求：客户端通过 HTTP 协议向服务器发起请求。②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个Controller 应该处理这个请求。通常会根据请求的 URL 来确定。④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用HandlerAdapter 来调用 Controller 的处理方法。⑤、执行处理器：Controller 处理请求，处理完后
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用ViewResolver 来解析视图名称，找到具体的视图页面。⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。⑧、响结果：DispatcherServlet 将视图结果返回给客户端。Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller 注解的类，还可以是实现了特定接口的其他类（如
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  HttpRequestHandler 或SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。SpringMVC Restful 风格的接口的流程是什么样的呢？我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同：客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServletDispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个 ControllerController 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理HandlerAdapter 执行完对 Controller 的请求，会调用HandlerMethodReturnValueHandler 去处理返回值，主要的过程：5.1. 调用 RequestResponseBodyMethodProcessor，创建ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入ServletServerHttpResponse 的 OutputStream 输出流中5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化执行完请求后，返回的 ModealAndView 为 null，ServletServerHtt
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: pResponse里也已经写入了响应，所以不用关心 View 的处理介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架Spring Boot 的优点非常多，比如说：Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和Spring MVC。Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控Spring Boot 常用注解有哪些？@SpringBootApplication：Spring Boot 应用的入口，用在启动类上。还有一些 Spring 框架本身的注解，比如 @Component、@RestController、@Service、@ConfigurationProperties、@Transact
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ional 等。SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。SpringBoot 的自动装配机制主要通过 @EnableAutoConfiguration 注解实现，这个注解是 @SpringBootApplication 注解的一部分，后者是一个组合注解，包括 @SpringBootConfiguration、@ComponentScan 和@EnableAutoConfiguration。@EnableAutoConfiguration 注解通过AutoConfigurationImportSelector 类来加载自动装配类，这个类实现了ImportSelector 接口的 selectImports 方法，该方法负责获取所有符
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat和 Jackson 等依赖。自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。为什么使用 Spring Boot？Spring Boot 解决了传统 Spring 开发的三大痛点：简化配置：自动装配 + 起步依赖，告别 XML 配置地狱快速启动：内嵌 Tomcat/Jetty，一键启动独立运行应用生产就绪：Actua
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: tor 提供健康检查、监控等运维能力@Import 的作用实现模块化配置导入，三种用法导入普通配置类：@Import(MyConfig.class)导入 ImportSelector 实现（自动装配核心）：@Import(AutoConfigurationImportSelector.class)导入 ImportBeanDefinitionRegistrar 实现（动态注册 Bean）Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责：第一步，创建 SpringApplication 实例，负责应用的启动和初始化；第二步，从 application.yml 中加载配置文件和环境变量；第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入；第四步，启动内嵌的 Web 容器。第五步，发布启动完成事件 ApplicationReadyEvent，并调用ApplicationRunner 的 run 方法完成启动后的逻辑。了解@SpringBootApplication 注解吗？@SpringBootApplic
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ation 是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解：@SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。@EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。@ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication 注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部的一系列处理流程Spring Boot 利用 Java 反射机制来读取传递给 run 方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication@SpringBootApplication 是一个组合注解，它里面的@ComponentScan 注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。如果需要自定义包扫描路径，可以在@SpringBootApplication 注解上添加@ComponentScan 注解，指定要扫描的包路径。这种方式会覆盖默认的包扫描路径，只扫描 com.github.paicoding.forum 包及其子包下的所有组件。SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种Model-View-Controller（模型-视图-控制器）
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的开发模式。Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个SpringMVC 的 Web 项目。Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。微服务架构主要要解决哪些问题？服务很多，客户端怎么访问，如何提供对外网关?这么多服务，服务之间如何通信? HTTP 还是 RPC?这
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 么多服务，如何治理? 服务的注册和发现。服务挂了怎么办？熔断机制SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml：用 SpringTask资源占用太高，有什么其他的方式解决？（补充）第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。第二，使用数据库调度器（如 Quartz）Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。Spring Cache 和 Redis 有什么区别？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。Redis 是一个分布式的缓存中间件，支持多种数
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。说说什么是 MyBatis?Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL语句的功底有一定要求SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中JDBC 编程有哪些不足之处，MyBatis 是如何解决的？1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xm
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: l文件中与 java 代码分离。3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。Hibernate 和 MyBatis 有什么区别？不同点1）映射关系MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂2）SQL 优化和移植性Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。MyBatis 需要
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步：1）创建 SqlSessionFactory2）通过 SqlSessionFactory 创建 SqlSessionSqlSession（会话）可以理解为程序和数据库之间的桥梁3）通过 sqlsession 执行数据库操作，可以通过 SqlSession 实例来直接执行已映射的 SQL 语句：4）调用 session.commit()提交事务5）调用 session.close()关闭会话说说 MyBatis 生命周期？SqlSessionFactoryBuilder一旦创建了 SqlSessionFactory，就不再需要它了。 因此SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。SqlSessionSqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。Mapper映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。在 mapper 中如何传递多个参数？#{}和${}的区别?①、当使用 #{} 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。由于 #{} 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。<select id="selectUser" resultType="User">SELECT * FROM users WHERE id = #{id}</select>②、当使用 ${} 时，参数的值会
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直接替换到 SQL 语句中去，而不会经过预处理。这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or1=1，这样就会导致查询出所有用户的结果。${} 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。<select id="selectUsersByOrder" resultType="User">SELECT * FROM users ORDER BY ${columnName} ASC</select>模糊查询 like 语句该怎么写?CONCAT('%',#{question},'%') 使用 CONCAT()函数，（推荐 ✨）说说 Mybatis 的一级、二级缓存？一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或close 之后，该 SqlSession 中的所有 Ca
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: che 就将清空，MyBatis 默认打开一级缓存。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。能说说 MyBatis 的工作原理吗？按工作原理，可以分为两大步：生成会话工厂、会话运行构造会话工厂也可以分为两步：获取配置获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括：读取配置文件，包括基础配置文件和映射文件初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等提供一个单例，作为会话工厂构建的重要参数它的构建过程也会初始化一些环境变量，比如数据源构建 SqlSessionFactorySqlSessionFactory 只是一
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合：Executor（执行器）Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法StatementHandler（数据库会话器）StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个handler 去执行 query。ParameterHandler （参数处理器）PreparedStatementHandler 里对 sql 进行了预编译处理ResultSetHandler（结果处理器）我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂SqlSessionFactory。创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。参数处理：对输入参数的类型进行处理，并预编译。结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。MyBatis 的功能架构是什么样的？我们一般把 Mybatis 的功能架构分为三层：API 接口层：提供给外部使用的接口 API，开发人员通
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑Mybatis 都有哪些 Executor 执行器？Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String, Statement>内，供下一次使用。简言之，就是重复使
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 用Statement 对象。BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。说说 JDBC 的执行步骤？Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤：在与数据库建立连接之前，首先需要通过 Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了 DriverManager 类中。Class.forName("com.mysql.cj.jdbc.Driver");第二步，建立数据库连接使用 DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数C
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: onnection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/databaseName", "username","password");第三步，创建 Statement 对象通过建立的数据库连接对象 Connection 创建 Statement、PreparedStatement或 CallableStatement 对象，用于执行 SQL 语句Statement stmt = conn.createStatement();第四步，执行 SQL 语句使用 Statement 或 PreparedStatement 对象执行 SQL 语句。执行查询（SELECT）语句时，使用 executeQuery()方法，它返回 ResultSet 对象；执行更新（INSERT、UPDATE、DELETE）语句时，使用 executeUpdate()方法，它返回一个整数表示受影响的行数。ResultSet rs = stmt.executeQuery("SELECT * FROM tableName");int affectedRow
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: s = stmt.executeUpdate("UPDATE tableName SET column ='value' WHERE condition");第五步，处理结果集如果执行的是查询操作，需要处理 ResultSet 对象来获取数据第六步，关闭资源最后，需要依次关闭 ResultSet、Statement 和 Connection 等资源，释放数据库连接等资源创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是 java.sql.Connection 对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。Connection 对象代表了应用程序和数据库的一个连接会话。通过调用 DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。一旦获得 Connection 对象，就可以使用它来创建执行 SQL 语句的 Statement、PreparedStatement 和 CallableStatement 对象，以及管理事务等。什么是 SQL 注入？如
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。为了防止 SQL 注入，可以采取以下措施：①、使用参数化查询使用参数化查询，即使用 PreparedStatement 对象，通过 setXxx 方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。②、限制用户输入对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或SQL 关键字。③、使用 ORM 框架比如，在 MyBatis 中，使用#{}占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。<select id="selectUser" resultType="User">SELECT * FROM users WHERE username = #{userName}</select>分布式说说 CAP 原则？、CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition toleran
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ce（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。假如现在有这样的场景：用户访问了 N1，修改了 D1 的数据。用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。接下来：保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。ASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致
2025-08-11 10:55:37.058 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性（Eventual consistency）的效果。BASE 的主要含义：Basically Available（基本可用）什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。Soft State（软状态）什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。Eventually Consistent（最终一致性）上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL 分布式锁、ZooKepper 分布式锁、Redis分布式锁。MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。加锁的时候，在锁表中增加一条记录
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 即可；释放锁的时候删除记录就行。如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。基于 Redis 的分布式锁核心思想： 利用 Redis 单线程执行命令的特性以及其丰富的数据结构和命令（尤其是 SETNX, SET with NX/PX/EX, Lua 脚本）来实现高性能锁。当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。什么是分布式事务
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性分布式事务有哪些常见的实现方案？二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。本地消息表：在数据库中存储事务事件，通过定时任务处理消息。基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。7.1 说说 2PC 两阶段提交？两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。优点：尽量保证了数据的强一致，实现成本
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 较低，在各大主流数据库都有自己实现，缺点:单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务：如果所有参与者都返回正确的 ACK 响应，则提交事务如果参与者有一个或多个参与者收到错误的 ACK 响应或者超时，则中断事务可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。Confirm：确
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。执行流程：订单服务，添加一条订单和一条消息，在一个事务里提交订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送库存服务，接收 MQ 消息，修改库存表，需要保证幂等
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 操作如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息如果修改失败，可以不做处理，等待重试MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。分布式算法 paxos 了解么 ？Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一在 Paxos 中有这么几个角色：Proposer（提议者） : 提议者提出提案，用于投票表决。Accecptor（接受者） : 对提案进行投票，并接受达成
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 共识的提案。Learner（学习者） : 被告知投票的结果，接受达成共识的提案Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受)Prepare(准备)阶段提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答：两个承诺：承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求承诺不会再接受提案号小于 Mn 的 Accept 请求一个应答：不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。Accept(接受)阶段如果提议者收到来自
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题笔试题1 为什么使用消息队列？消息队列（Message Queue，简称 MQ）是一种跨进程的通信机制，用于上下游传递消息。它在现代分布式系统中扮演着重要的角色，主要用于系统间的解耦、异步消息处理以及流量削峰。消息队列的使用场景解耦在没有消息队列的系统中，如果
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个系统需要与多个系统交互，它们之间的耦合度会非常高。例如，系统 A直接调用系统 B 和 C的接口，如果未来需要接入系统 D或取消 B 系统，系统 A 需要修改代码，这增加了系统的风险。使用消息队列后，系统 A 只需将消息推送到队列，其他系统根据需要从队列中订阅消息。这样，系统 A 不需要做任何修改，也不需要考虑下游消费失败的情况，从而实现了系统间的解耦。异步处理在同步操作中，一些非关键的业务逻辑可能会消耗大量时间，导致用户体验不佳。例如，系统 A 在处理一个请求时，需要在多个系统中进行操作，这可能导致总延迟增加。通过使用消息队列，系统 A 可以将消息写入队列，而其他业务逻辑可以异步执行，从而显著减少总耗时。流量削峰对于面临突发流量的系统，如果直接将所有请求发送到数据库，可能会导致数据库连接异常或系统崩溃。消息队列可以帮助系统按照下游系统的处理能力从队列中慢慢拉取消息，从而避免因突发流量导致的系统崩溃。消息队列的优缺点优点解耦：使得系统间的依赖关系最小化，降低系统间的耦合度。异步处理：提高系统的响应速度和吞吐量。流量削峰：使系统能够应对高流量压力，避免系统因突发流量而崩溃2.简述数据库的事务，说出事务的特点？
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 在数据库管理系统中，事务是一个非常重要的概念，它指的是一系列的数据库操作，这些操作要么全部成功，要么全部失败，确保数据的完整性和一致性。事务的四大特性通常被称为 ACID属性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。原子性（Atomicity）原子性确保事务中的所有操作要么全部成功，要么全部失败回滚，不会出现只执行了部分操作的情况。这意味着事务是一个不可分割的工作单位，例如，在银行转账的场景中，转账操作需要同时更新两个账户的余额，这两个操作必须要么都执行，要么都不执行一致性（Consistency）一致性意味着数据库在事务开始之前和结束之后，都必须保持一致状态。事务不会破坏数据的完整性和业务规则。例如，如果一个转账事务在执行过程中系统崩溃，事务没有提交，那么事务中所做的修改也不会保存到数据库中，保证了数据的一致性隔离性（Isolation）隔离性保证了当多个用户并发访问数据库时，数据库系统能够为每个用户的事务提供一个独立的运行环境，事务之间不会互相干扰。例如，当一个事务正在处理数据时，其他事务必须等待，直到该事务完成，
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 才能访问同样的数据持久性（Durability）持久性确保一旦事务提交，它对数据库的改变就是永久性的。即使发生系统故障，事务的结果也不会丢失。例如，一旦银行转账事务提交，转账的金额就会永久地反映在各个账户的余额中事务的四大特性是数据库管理系统设计的基础，它们确保了数据库操作的安全性和可靠性，使得用户可以信赖数据库处理复杂的业务逻辑。3. SOA 和微服务之间的区别？SOA（面向服务的架构）[&和微服务架构&]是两种常见的软件架构设计方法，它们在服务划分、通信方式和应用场景等方面存在显著差异。SOA 的特点 SOA 是一种高层次的架构设计理念，旨在通过服务接口实现系统间的松耦合和功能复用。服务通过企业服务总线（ESB）进行通信，ESB负责消息路由、协议转换和服务集成。SOA 的服务粒度较粗，适用于复杂的企业级系统，尤其是需要集成异构系统的场景。微服务的特点 微服务架构是对 SOA 的进一步演进，强调将单一业务系统拆分为多个独立的小型服务。每个服务独立开发、部署和运行，通常通过轻量级协议（如 HTTP/REST）进行通信。微服务更注重快速交付和自动化运维，适合快速变化的互联网系统。主要区别服务粒度：SOA 的服务
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 粒度较粗，通常是一个完整的业务模块；微服务的服务粒度较细，专注于单一功能。通信方式：SOA 依赖 ESB 进行服务间通信，支持多种协议；微服务使用轻量级协议（如 HTTP/REST），去掉了 ESB。部署方式：SOA 通常整体部署，微服务则支持独立部署，便于快速迭代。应用场景：SOA 适用于复杂的企业级系统，微服务更适合轻量级、基于 Web 的系统。总结 SOA 和微服务各有优劣，选择哪种架构取决于具体的业务需求和系统复杂性。SOA 更适合需要集成异构系统的大型企业应用，而微服务则适合快速变化的互联网应用。4.深拷贝和浅拷贝的区别？在编程中，深复制和浅复制是两种不同的对象复制方式。它们主要用于处理对象和数组等引用数据类型。浅复制浅复制只复制对象的引用，而不复制对象本身。也就是说，新旧对象共享同一块内存空间，对其中一个对象的修改会影响到另一个对象。浅复制适用于对象的属性是基本数据类型的情况，但如果属性是引用类型，则会出现共享内存的问题。深复制深复制会创建一个新的对象，并递归复制所有层级的属性和数组元素。新对象与原对象不共享内存，修改新对象不会影响到原对象。深复制适用于需要完全独立的对象副本的情况。5.有了关系型
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据库，为什么还需要 NOSQL 数据库？关系型数据库（RDBMS）在数据存储和管理方面表现出色，但在某些场景下存在局限性。随着互联网和大数据时代的到来，传统的关系型数据库在处理大量非结构化和半结构化数据时显得力不从心。为了解决这些问题，非关系型数据库（NoSQL）应运而生。关系型数据库的局限性扩展性：关系型数据库通常采用垂直扩展（Scale-Up）的方式，通过增加硬件资源来提升性能。然而，这种方式在高并发、大数据量的情况下成本高昂且效果有限。灵活性：关系型数据库要求预先定义数据模式（Schema），在需求频繁变化的应用场景中显得不够灵活。每次修改数据模式都需要停机或复杂的迁移操作。性能：在高并发读写、大规模数据处理的情况下，关系型数据库的性能可能无法满足需求，特别是在分布式环境下。成本：关系型数据库通常需要昂贵的硬件和专业的维护团队，对于中小型企业和初创公司来说，成本压力较大。非关系型数据库的优势水平扩展（Scale-Out）：NoSQL 数据库通常设计为支持水平扩展，通过增加更多的服务器节点来提升性能。这种方式在大规模数据和高并发场景下非常有效。灵活的数据模型：NoSQL数据库通常采用无模式（Schema
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -less）或弱模式（Schema-flexible）的设计，允许数据以更灵活的方式存储。高性能：NoSQL 数据库针对特定的应用场景进行了优化，通常具有更高的读写性能。低成本：NoSQL数据库通常采用开源软件，硬件要求较低，适合在云环境中部署，降低了总体拥有成本（TCO）。分布式架构：NoSQL 数据库通常采用分布式架构，能够更好地处理大规模数据和高并发请求。关系型数据库与非关系型数据库的对比数据模型：关系型数据库以表格形式存储数据，而 NoSQL数据库以键值对、文档、列族或图的形式存储数据。扩展性：关系型数据库采用垂直扩展，而 NoSQL数据库采用水平扩展。数据一致性：关系型数据库强调强一致性（ACID），而 NoSQL 数据库通常采用最终一致性（BASE）。数据模式：关系型数据库需要固定模式，而 NoSQL 数据库通常无模式或弱模式。性能：关系型数据库适合事务处理和小规模数据，而 NoSQL数据库适合大规模数据和高并发。成本：关系型数据库成本较高，而 NoSQL数据库成本较低。结论NoSQL数据库的出现并不是为了取代关系型数据库，而是为了填补关系型数据库在某些场景下的不足。NoSQL 数据库提供了更高的
2025-08-11 10:55:37.059 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 扩展性、灵活性和性能，适合处理大规模数据和高并发请求。然而，关系型数据库在事务处理和企业应用中仍然具有不可替代的优势。
2025-08-11 10:55:37.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:37.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:37.491 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.yizhaoqi.smartpai.service.VectorizationService - 开始向量化文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:37.503 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.client.EmbeddingClient - 开始生成向量，文本数量: 238
2025-08-11 10:55:37.525 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:37.751 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:37.950 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] [10d4b7b0-1] Response 400 BAD_REQUEST
2025-08-11 10:55:37.960 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] [10d4b7b0-1] Read 312 bytes
2025-08-11 10:55:38.979 [parallel-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:38.980 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:39.218 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] [10d4b7b0-2] Response 400 BAD_REQUEST
2025-08-11 10:55:39.218 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] [10d4b7b0-2] Read 312 bytes
2025-08-11 10:55:40.220 [parallel-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:40.221 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:40.473 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] [10d4b7b0-3] Response 400 BAD_REQUEST
2025-08-11 10:55:40.474 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] [10d4b7b0-3] Read 312 bytes
2025-08-11 10:55:41.480 [parallel-3] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:41.480 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:41.730 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] [10d4b7b0-4] Response 400 BAD_REQUEST
2025-08-11 10:55:41.730 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2a093f92] [10d4b7b0-4] Read 312 bytes
2025-08-11 10:55:41.731 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2a093f92] Cancel signal (to close connection)
2025-08-11 10:55:41.731 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR com.yizhaoqi.smartpai.client.EmbeddingClient - 调用向量化 API 失败: Retries exhausted: 3/3
reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:41.735 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.yizhaoqi.smartpai.service.VectorizationService - 向量化失败，fileMd5: c8f8cebf90c764b93d862694096a2af9
java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:41.735 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.y.smartpai.consumer.FileProcessingConsumer - Error processing task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:44.757 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR o.s.kafka.listener.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(com.yizhaoqi.smartpai.model.FileProcessingTask)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: java.lang.RuntimeException: Error processing task
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:67)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	... 25 common frames omitted
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:44.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Received task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
2025-08-11 10:55:44.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件权限信息: userId=1, orgTag=PRIVATE_sy, isPublic=true
2025-08-11 10:55:44.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Downloading file from storage: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:44.762 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Detected remote URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:44.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Successfully connected to URL, starting download...
2025-08-11 10:55:44.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 开始解析文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文件元数据:
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:unmappedUnicodeCharsPerPage: 0
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:PDFVersion: 1.7
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmp:CreatorTool: WPS 文字
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXFA: false
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:modify_annotations: true
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print_degraded: true
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By-Full-Set: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:creator: SongYu
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:num3DAnnotations: 0
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:45.105 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:format: application/pdf; version=1.7
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator_tool: WPS 文字
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:overallPercentageUnmappedUnicodeChars: 0.0
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:fill_in_form: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasCollection: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:encrypted: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsNonEmbeddedFont: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:custom:SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasMarkedContent: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - Content-Type: application/pdf
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator: SongYu
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:totalUnmappedUnicodeChars: 0
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_for_accessibility: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:assemble_document: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmpTPg:NPages: 133
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXMP: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:charsPerPage: 1441
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_content: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:trapped: False
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_modify: true
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsDamagedFont: false
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 提取的文本内容长度: 121403
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆状态的检查、为游客和已登录用户展示不同界面与功能。实现不同用户的权限控制和网站数据统计(UV、DAU)，管理员可以查看网站数据统计和网站监控信息。支持用户上传头像，实现发布帖子、评论帖子、热帖排行、发送私信与敏感词过滤等功能。实现了点赞关注与系统通知功能。支持全局搜索帖子信息的功能。核心功能具体实现1. 通过对登录用户颁发登录凭证，将登陆凭证存进 Redis 中来记录登录用户登录状态，使用拦截器进行登录状态检查，使用 Spring Security 实现权限控制，解决了 http 无状态带来的缺陷，保护需登录或权限才能使用的特定资源。（登入时将生成的 Ticket存入 Redies, 然后在登入请求成功时，将 Redies中的Ticket存入新建的 Cookie 中，然后反馈给浏览器，随后在该浏览器访问其他请求时，会先经过 LoginTicketInterceptor，判断请求中是否有 Ticket，是否和 Redies中的 T
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: icket一致，如果一致会将用户信息存入到 hostHolder（属于线程局部缓存）中，以便后续在请求处理过程中可以方便地获取到当前登录用户的信息，在请求处理完成后，清除当前线程中存储的用户信息。通过这种方式，确保每个请求都是独立处理的，不会因为线程复用而导致用户信息泄露或混淆。 注意可以将用户信息存入 Redis缓存中来减少 DB 的访问量，但是当用户数据更新时，必须即使删除 Redis中的用户数据，以保证数据的一致性和准确性。Spring Security 的用户认证是在自定义的过滤器中，也是获取请求 Cookie 中的 Ticket 和 Redis中的Ticket是否一致，然后将认证用户存到安全上下文中，在 Security 配置类中根据安全上下文获取用户信息，判断用户对各个资源的访问权限。Security 认证应当放到过滤器中而不是拦截器，因为过滤器比拦截器先执行，在拦截器中配置安全上下文会导致 Security 配置类获取不到用户信息，因为此时还没执行拦截器。）2. 使用 ThreadLocal 在当前线程中存储用户数据，代替 session 的功能便于分布式部署。在拦截器的 preHandle 中
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存储用户数据，在 postHandle 中将用户数据存入 Model，在 afterCompletion 中清理用户数据。（ThreadLocal 为每个线程提供独立的变量副本。每个线程操作自己的副本，互不干扰。在 Web 应用中，一个请求从开始到结束通常由同一个线程处理。因此，在拦截器的 preHandle 中存储的数据，可在整个请求链路（Controller、Service、Dao）中通过 ThreadLocal 获取。 在分布式部署中，由于 Session需要共享，使用 ThreadLocal存储用户数据，我们并不需要在多个服务器之间共享这些数据。因为每个请求都是独立的，处理完一个请求后，数据就被清除了。所以，在分布式环境下，我们只需要确保每个服务器能够独立处理请求即可，不需要考虑多个服务器之间的 Session同步问题。）3. 使用 Redis 的集合数据类型来解决踩赞、相互关注功能，采用事务管理，保证数据的正确，采用“先更新数据库，再删除缓存”策略保证数据库与缓存数据的一致性。采用 Redis 存储验证码，解决性能问题和分布式部署时的验证码需求。采用 Redis 的 HyperLogLog 存储每日
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  UV、Bitmap 存储 DAU，实现网站数据统计的需求。（使用 RedisTemplate执行一个 Redis 事务。SessionCallback 接口的 execute方法会在一个事务中执行所有操作，确保操作的原子性。通过调用 multi()方法，开启一个 Redis事务。在这个事务中执行的命令会被缓存，直到 exec()方法被调用时才会一次性提交。数据结构：HyperLogLog，12KB 内存可计算 2^64 个不重复元素 误差率仅 0.81% 数据结构：Bitmap 一连串二进制数组，可以进行二值状态统计）4. 使用 Kafka 作为消息队列，在用户被点赞、评论、关注后以系统通知的方式推送给用户，用户发布或删除帖子后向 elasticsearch 同步，对系统进行解耦、削峰。（在这个系统中 kafka 就办了三件事，一是用户在被点赞、评论、关注后会借助kafka 消费者来异步的生成系统通知，二三是在用户发布帖子和删除帖子时，将内容添加到 elasticsearch或者从 elasticsearch 中删除）5. 使用 elasticsearch + ik 分词插件实现全局搜索功能，当用户发布、修
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 改或删除帖子时，使用 Kafka 消息队列去异步将帖子信息给 elasticsearch 同步。（Elasticsearch（简称 ES）是一个开源的分布式搜索和分析引擎，它专为处理海量数据设计，提供近实时的全文搜索能力。IK Analyzer 是专为中文设计的开源分词插件，解决中文文本分析的核心难题。ik_smart：粗粒度切分 ik_max_word 细粒度切分）6. 使用分布式定时任务 Quartz 定时计算帖子分数，来实现热帖排行的业务功能。对频繁需要访问的数据，如用户信息、帖子总数、热帖的单页帖子列表，使用Caffeine 本地缓存 + Redis 分布式缓存的多级缓存，提高服务器性能，实现系统的高可用。（上面三个部分就是 Quartz的基本组成部分：调度器：Scheduler任务：JobDetail触发器：Trigger，包括 SimpleTrigger 和 CronTrigger定义一个 Quartz定时任务及其触发器。具体来说，它配置了一个名为postScoreRefreshJob的任务，该任务属于 communityJobGroup 组，并且被设置为持久化和请求恢复。同时，它还配置了一个名为
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  postScoreRefreshTrigger 的触发器，该触发器也属于 communityTriggerGroup组，并且每 5 分钟触发一次postScoreRefreshJob任务。因此，这段代码的主要目的是确保 PostScoreRefreshJob类中定义的任务每 5分钟执行一次）核心技术Spring Boot、SSMRedis、Kafka、ElasticsearchSpring Security、Quartz、Caffeine项目亮点项⽬构建在 Spring Boot+SSM 框架之上，并统⼀的进⾏了状态管理、事务管理、异常处理；利⽤ Redis 实现了点赞和关注功能，单机可达 5000TPS；利⽤ Kafka 实现了异步的站内通知，单机可达 7000TPS；利⽤ Elasticsearch 实现了全⽂搜索功能，可准确匹配搜索结果，并⾼亮显示关键词；利⽤ Caffeine+Redis 实现了两级缓存，并优化了热⻔帖⼦的访问，单机可达8000QPS。利⽤ Spring Security 实现了权限控制，实现了多重⻆⾊、URL 级别的权限管理；利⽤ HyperLogLog、Bitmap 分别实现了 
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: UV、DAU 的统计功能，100 万⽤户数据只需*M 内存空间；利⽤ Quartz 实现了任务调度功能，并实现了定时计算帖⼦分数、定时清理垃圾⽂件等功能；利⽤ Actuator 对应⽤的 Bean、缓存、⽇志、路径等多个维度进⾏了监控，并通过⾃定义的端点对数据库连接进⾏了监控。面试题：1.你提到使用 Spring Security 实现权限控制。能具体说明如何整合登录凭证（Redis存储）与 Spring Security？如何实现 URL 级别的动态权限管理？答：用户登入成功时系统会生成一个 Ticket并存入 Redis，新建一个 cookie 存入Ticket；在自定义的过滤器中验证请求携带的 Ticket与 Redis内的 Ticket是否一致，构建 Authentication对象存入 SecurityContextHolder；在 Security 的配置类中对固定 URL 如/admin/**）使用 antMatchers().hasRole("ADMIN")，即根据用户权限赋予访问资源的能力。2. 你提到点赞功能采用‘先更新 DB 再删缓存’策略。如果删除缓存失败导致不一致，如何解决？为何不用
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ‘更新缓存’方案？答：“删除失败兜底方案：设置缓存短过期时间（如 30s），容忍短期不一致异步重试：将失败操作推入 Kafka，消费者重试删除监听 MySQL Binlog（如 Canal）触发缓存删除不更新缓存的原因：写冲突： 并发更新可能导致缓存脏数据（如线程 A更新 DB 后未更新缓存时，线程 B又更新）浪费资源： 频繁更新但低读取的数据会占用带宽复杂度： 需维护缓存与 DB的强一致性逻辑（如分布式锁），而删除策略更简单可靠。”3. 系统通知使用 Kafka异步推送。如果通知发送失败（如网络抖动），如何保证用户最终能收到通知？答：“我们通过三级保障实现可靠性：生产者确认： 设置 Kafka acks=all，确保消息写入所有副本；消费者容错：开启手动提交 Offset，业务处理成功后才提交捕获异常后重试（如 3次），仍失败则存入死信队列补偿机制：定时任务扫描未通知记录（DB状态标记）重新投递死信队列消息人工介入处理此外，消息体包含唯一 ID 防重复消费。”4. 热帖列表用了 Caffeine+Redis两级缓存。如何解决缓存穿透？如何同步本地缓存（Caffeine）的数据？答：“缓存穿透防护：布隆过滤器
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ：将所有的 key 提前存入布隆过滤器，在访问缓存层之前，查询前校验 Key 是否存在，不存在返回空值。缓存空值：对不存在的帖子 ID 缓存 NULL（短过期时间）本地缓存同步过期同步： Caffeine设置 refreshAfterWrite=30s自动刷新主动推送： 当帖子更新时，通过 Redis Pub/Sub 广播失效事件，节点监听后删除本地缓存兜底策略： 本地缓存过期时间短于 Redis（如本地 60s vs Redis 300s），确保最终一致。”5. 定时计算帖子分数时，如何避免分布式环境下的重复执行？如果计算耗时过长导致阻塞，如何优化？答：“防重复执行：使用 Quartz集群模式：数据库锁（QRTZ_LOCKS 表）保证同一任务仅一个节点执行性能优化：分片处理： 按帖子 ID 范围分片（如 0-10000, 10001-20000），多线程并行计算增量计算： 仅扫描最近 X 小时变化的帖子（如 last_modified_time > now()-6h）异步化： 将计算任务拆解为多个子任务投递到 Kafka，消费者并发处理降级策略： 超时后记录断点，下次任务从断点继续。”6. 你使用 Elas
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ticsearch 实现全文搜索并高亮关键词。请说明：如何设计索引映射（Mapping）以优化搜索效率和准确性？如何实现搜索结果的高亮显示？遇到 HTML 标签转义问题如何处理？搜索性能瓶颈可能在哪里？如何优化？答: 1. 索引映射设计：分词策略： 对帖子标题和内容字段使用 ik_max_word 分词器进行细粒度分词（索引时），搜索时结合 ik_smart 提高相关性。字段类型： 标题用 text（分词） + keyword（不分词，用于精确匹配/聚合），ID 用 keyword，发布时间用 date。副本分片： 设置合理副本数（如 1-2）提高查询吞吐量和容错性。关闭不必要特性： 对不需聚合/排序的字段关闭 doc_values 节省存储。2. 高亮实现与转义：高亮请求： 在搜索请求中添加 highlight 部分，指定字段、pre_tags（如 <em>）、post_tags（如 </em>）。HTML 转义： ES 默认会转义高亮片段中的 HTML。我们确保存入 ES 的内容是纯文本（不含用户输入的原始 HTML），避免 XSS 同时解决转义混乱。前端渲染高亮片段时使用 textContent 而非 
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: innerHTML。3. 性能优化点：瓶颈： 复杂查询（多条件+聚合）、深度分页（from + size 过大）、索引设计不佳。优化措施：避免深度分页：使用 search_after + 唯一排序值（如 ID+时间戳）替代 from/size。限制查询范围： 使用 filter 缓存（如时间范围、状态）减少 query 计算量。冷热数据分离： 历史数据迁移到低性能节点或归档索引。合理硬件： SSD、充足内存（ES 堆内存 ≈ 50% 物理内存，不超过 31GB）。7. 项目用 HyperLogLog (HLL) 统计 UV，Bitmap 统计 DAU。请解释：HLL 如何用极小空间估算大基数？它的误差范围是多少？Bitmap 如何统计 DAU？如何解决用户 ID 非连续导致的空间浪费？如果某天 UV 突增，HLL 合并结果会怎样？如何验证其准确性？答：“1. HLL 原理与误差：原理： 对每个用户 ID 做哈希，计算哈希值二进制表示中 ‘1’ 的最高位位置（如 0001... 最高位=4），维护一个 ‘寄存器数组’ 记录每个桶的最大位置。最终通过调和平均数估算基数。核心是利用概率分布。误差： Redis 的 
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: PF 实现标准误差约 0.81%（使用 16384 个寄存器时）。空间仅需 12KB（固定大小）。2. Bitmap 与 DAU：实现： 每天一个 Bitmap Key（如 dau:20230702），用户 ID 作为 Offset，访问则设位为 1。BITCOUNT 获取当日活跃用户数。稀疏优化： 使用 RLE (Run-Length Encoding) 压缩的 Bitmap 库（如 RoaringBitmap）或 Redis 的 BITFIELD 命令动态管理非连续 ID，避免传统 Bitmap 的空间浪费。3. HLL 突增与验证：突增影响：HLL 是基数估计，突增时估算值会上升，误差仍在理论范围内（0.81%）。合并多个 HLL（如按小时合并成天）误差会累积但可控。验证： 定期抽样对比：对某小段时间用 SET 精确计算 UV，与 HLL 结果对比，监控误差是否符合预期。业务上接受近似值是其使用前提。8. 敏感词过滤是社区必备功能。你如何实现它？如何平衡过滤效率和敏感词库的更新？“技术选型：Trie 树 (前缀树)实现：初始化： 服务启动时将敏感词库（DB 或文件）加载到内存中的 Trie 树。节点标记
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是否为词尾。过滤过程： 对用户输入（帖子/评论）进行滑动窗口扫描。匹配到 Trie 树路径且到达词尾时，替换为 *** 或阻断提交。优化： 结合 DFA (确定有限状态自动机) 减少回溯，支持跳过无关字符（如 敏*感*词）。词库更新：热更新： 后台管理添加敏感词后，通过 ZooKeeper 配置中心 或 Redis Pub/Sub广播到所有服务节点，节点异步重建 Trie 树。降级： 更新期间短暂使用旧词库，避免服务中断。词库版本号控制。效率：Trie 树查询时间复杂度 O(n) (n=文本长度)，内存占用可控（可压缩节点）。避免正则表达式（性能差）。”9. 你提到用 Spring Boot Actuator 进行监控并自定义了数据库监控端点。请说明：暴露了哪些关键内置端点？（至少 3个）如何自定义一个端点监控数据库连接池状态（如活跃连接数、等待连接数）？如何保证这些监控接口的安全？答：“1. 关键内置端点：/health：应用健康状态（DB, Redis, Disk 等）/metrics：JVM 内存、线程、HTTP 请求指标等/loggers：动态查看/调整日志级别/threaddump：获取线程快照（排
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 查死锁）创建一个类实现 Endpoint 接口或使用 @Endpoint(id = "dbpool") 注解。注入连接池对象（如 HikariDataSource）。在 @ReadOperation 方法中返回关键指标：安全保障：访问控制：通过 management.endpoints.web.exposure.include/exclude 精确控制暴露的端点。安全加固：集成 Spring Security：只允许管理员角色访问 /actuator/** 路径。修改默认端口：management.server.port 使用与管理网络隔离的端口。HTTPS： 强制要求监控端点使用 HTTPS。10. 在“点赞后发通知”这个场景，涉及更新数据库点赞数 (DB) 和发送 Kafka 消息 (通知) 。如何保证这两个操作的原子性？如果 Kafka 发送失败，如何处理？答：“核心思路：最终一致性 + 本地事务 + 可靠消息原子性保障： 将 ‘更新点赞状态/计数’ 和 ‘写入待通知消息’ 放在同一个数据库事务中。使用 ‘本地消息表’ 方案：在业务数据库创建 message_event 表 (含业务 ID、消息体、状态
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: [PENDING, SENT])。事务内操作：更新点赞相关 DB 数据。向 message_event 插入一条 PENDING 状态的通知记录。事务提交。Kafka 发送与补偿：后台定时任务扫描 PENDING 的消息。发送消息到 Kafka，成功后将状态改为 SENT。发送失败处理：记录重试次数和错误信息，下次任务重试（指数退避）。超过最大重试则标记为失败，告警人工介入。消费者幂等： 通知消费者根据业务 ID 去重，避免重复处理。为什么不强一致？ 跨系统（DB 与 MQ）的强一致（如 2PC）成本高且降低可用性。本方案在 CAP 中优先保证 AP，通过可靠消息实现最终一致，满足业务需求。”11.你提到使用 ThreadLocal 存储用户数据以替代 Session。这在单机中可行，但分布式部署时（如多台 Tomcat 节点）会失效。如何解决分布式场景下的用户状态共享问题？业界主流方案是什么？答：“ThreadLocal 的局限： 它绑定于单个 JVM 线程，无法跨节点共享。在负载均衡（如 Nginx 轮询）下，用户请求落到不同节点会导致状态丢失。方案演进：Session 复制： 利用 Tomcat Red
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: is Session Manager 等工具将 Session 存入Redis。所有节点从 Redis 读写 Session，实现共享。无状态 Token (JWT)： 当前项目采用的核心方案。用户登录后生成包含用户 ID和权限的 JWT Token 返回客户端（通常存于 Cookie 或 Header）。后续请求携带 Token，服务端无需存储 Session，仅需验证 Token 签名和有效期并从 Token中解析用户信息（如注入到 SecurityContext）。这天然支持分布式。项目整合： 我们实际采用了 JWT + Redis 黑名单 的增强方案：JWT 本身无状态，解析快速。主动登出/失效： 将需提前失效的 Token ID 存入 Redis 并设置 TTL（作为黑名单）。校验 Token 时额外检查黑名单。安全性： Token 使用强密钥签名（如 HMAC-SHA256），防止篡改。优点： 彻底解决分布式状态问题，减轻服务端存储压力，更适合 RESTful API。12. 热帖列表使用了 Caffeine 本地缓存。请说明：你选择了哪种缓存淘汰策略（如 LRU、LFU）？依据是什么？如何配置缓
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存大小和过期时间？如何监控缓存命中率？本地缓存导致不同节点数据不一致的风险如何缓解？（例如帖子被删除）答：“1. 淘汰策略与依据：策略： 使用 Window TinyLFU (W-TinyLFU)，Caffeine 的默认算法。它结合了 LRU（近期使用）和 LFU（频率统计）的优点，对突发流量和长期热点都有良好表现。依据：论坛热帖访问模式既有突发（新热帖），也有长尾（持续热帖）。W-TinyLFU在有限空间内能最大化命中率，优于纯 LRU/LFU。2. 配置与监控：配置：javaCaffeine.newBuilder().maximumSize(10_000) // 最大条目数.expireAfterWrite(5, TimeUnit.MINUTES) // 写入后 5 分钟过期.recordStats() // 开启统计.build();监控： 通过 Cache.stats() 获取 CacheStats 对象，关键指标：hitRate()：命中率evictionCount()：淘汰数量averageLoadPenalty()：平均加载耗时可定期输出到日志或监控系统（如 Prometheus）。3. 数据
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不一致风险缓解：主动失效： 核心策略！当帖子被删除或更新时：更新数据库。删除 Redis 中的缓存 Key。通过 Redis Pub/Sub 或 专门的广播方案（如 RabbitMQ Fanout Exchange） 发布“帖子失效”事件。所有服务节点监听到事件后，删除本地 Caffeine 缓存中对应的条目。兜底： 设置较短的本地缓存过期时间（如 5分钟），确保最终一致。”13.你提到点赞功能单机 TPS 达 5000，通知单机 TPS 7000，热帖访问 QPS 8000。请说明：这些数据是如何测试得到的？（工具、场景、环境）TPS 和 QPS 的区别是什么？测试中发现了哪些性能瓶颈？如何定位和优化的？（如 GC、慢 SQL）1. 测试方法：工具： JMeter（模拟并发用户）。场景：点赞 TPS： 持续模拟用户对随机帖子点赞（高并发写）。通知 TPS： 模拟触发通知事件（评论/关注），测量 Kafka 生产者吞吐量。热帖 QPS： 持续请求热帖列表接口（高并发读）。环境： 明确标注是 单机测试（如 4C8G Linux, JDK 17, Tomcat, Redis/Kafka 同机或独立）。2. TPS
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  vs QPS：QPS (Queries Per Second)： 服务器每秒处理的查询请求数（如 HTTP 请求）。适用于读场景。TPS (Transactions Per Second)： 服务器每秒处理的事务数（一个事务可能包含多个操作/请求）。点赞（写 DB + Redis + 发 Kafka）是一个事务。此处 5000 TPS 指每秒完成 5000 次点赞事务。3. 瓶颈发现与优化：发现工具： Arthas (监控方法耗时)、JVisualVM/PerfMa (GC 分析)、Redis Slowlog、MySQL Slow Query Log。典型瓶颈 & 优化：GC 频繁 (Young GC >1s)： 优化 JVM 参数（如 -XX:+UseG1GC, 调整MaxGCPauseMillis），减少大对象分配（如缓存 DTO 复用）。慢 SQL (全表扫描)： 添加索引（如 post_id 在点赞表），优化查询（避免 SELECT*）。Redis 单线程阻塞： 避免长命令（如 KEYS *），分片（Cluster），热点 Key 本地缓存（Caffeine）。Kafka 生产瓶颈： 调优 batc
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: h.size 和 linger.ms，增加分区数，提高 acks 级别换取可靠性（需权衡）。”14.如果这个论坛用户量增长 100 倍（如日活千万级），当前架构在哪些地方可能最先遇到瓶颈？你会如何改造？（请结合你已用技术栈思考）“潜在瓶颈与改造方向：数据库 (MySQL)：瓶颈： 写压力（点赞、发帖）、复杂查询（搜索、统计）、单表数据量过大。改造：读写分离： 主库写，多个从库读（评论列表、用户信息查询）。分库分表： 按 user_id 或 post_id 分片（如 ShardingSphere）。将点赞/关注等高频写操作分离到独立库。冷热数据分离： 归档旧帖到分析型数据库（如 HBase）。Redis：瓶颈： 单机内存容量、带宽、单线程处理。改造：集群化： Redis Cluster 自动分片。区分数据类型： 热点数据（用户信息）用集群；超大 Value（如长帖缓存）考虑其他存储或压缩；统计类（UV/DAU）可保留。Elasticsearch：瓶颈： 索引过大导致查询慢、写入堆积。改造：分片策略优化： 增加主分片数（提前规划）。按时间分索引： 如 posts-202307，便于管理/查询/删除旧数据。Kafk
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: a：瓶颈： 单个 Topic 分区数限制吞吐量。改造： 增加分区数，生产者根据 Key（如 user_id）分区保证顺序性。应用层：瓶颈： 单节点处理能力。改造： 水平扩展无状态节点（更多 Tomcat 实例），通过 Nginx 负载均衡。微服务化拆分（如独立用户服务、帖子服务、消息服务），便于独立伸缩。监控与治理：加强：引入 APM（如 SkyWalking）、集中日志（ELK）、更强健的配置中心（Nacos）和熔断限流（Sentinel）。RedisIO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？”同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。举例子说一下阻塞 IO 和 IO
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  多路复用的差别？假设我是一名老师，让学生解答一道题目。我的第一种选择：按顺序逐个检查，先检查 A 同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种就是阻塞 IO，不具有并发能力我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 ARedis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。等子进
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。AOF 重写期间命令会同时写入现有 AOF 文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。缺点是可能丢失两次快照期间的所有数据变更AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。缺点是文件体积较大，恢复速度慢。在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。如果是缓存场景，可以接受一定程度的数据丢失，
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布/订阅机制通知客户端主节点已经发生变化。主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。当
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。常见的数据分区有三种：节点取余、一致性哈希和哈希槽。节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过CRC16 算法计算后对 16384 取模，确定它属于哪个槽。布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来解决 Redis 缓存穿透的问题。布隆过滤器并不支持删除操作，这是它的一个重要限制。如何保证缓存和数据库数据的一致性？具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题：线程 A 要更新用户信息，先删除了缓存线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值线程 B 将查到的旧值重新放入缓存线程 A 完成数据库更新结果就是数据库是新的值，但缓存中还是旧值当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。即使缓存删除失败，消息队列的重试机制也能保证最终一致性。第二种
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。如何保证本地缓存和分布式缓存的一致性？为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pub/sub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内存压力。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis虽然不是专业的消息队列，但可以很好地实现延时队列功能。核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不存在时才能创建成功，EX 设置过期时间用以防止死锁。Kafka，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景Kafka 的设计Kafka 将消息以 topic 为单位进行归纳，发布消息的程序称为 Producer，消费消息的程序称为 Consumer。它是以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 Broker，Producer 通过网络将消息发送到 kafka 集群，集群向消费者提供消息，broker 在中间起到一个代理保存消息的中转站。Kafka 性能高原因利用了 PageCache 缓存磁盘顺序写零拷贝技术pull 拉模式优点高性能、高吞吐量、低延迟：Kafka 生产和消费消息的速度都达到每秒 10 万级高可用：所有消息持久化存储到磁盘，并支持数据备份防止数据丢失高并发：支持数千个客户端同时读写容错性：允许集群中节点失败（若副本数量为 n，则允许 n-1 个节点失败）高扩展性：Kafka 集群支持热伸缩，无须停机缺点没有完整的监控工具集不支持通配符主
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 题选择Kafka 的应用场景日志聚合：可收集各种服务的日志写入 kafka 的消息队列进行存储消息系统：广泛用于消息中间件系统解耦：在重要操作完成后，发送消息，由别的服务系统来完成其他操作流量削峰：一般用于秒杀或抢购活动中，来缓冲网站短时间内高流量带来的压力异步处理：通过异步处理机制，可以把一个消息放入队列中，但不立即处理它，在需要的时候再进行处理Kafka 为什么要把消息分区方便扩展：因为一个 topic 可以有多个 partition，每个 Partition 可用通过调整以适应它所在的机器，而一个 Topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率Kafka 中生产者运行流程一条消息发过来首先会被封装成一个 ProducerRecord 对象对该对象进行序列化处理（可以使用默认，也可以自定义序列化）对消息进行分区处理，分区的时候需要获取集群的元数据，决定这个消息会被发送到哪个主题的哪个分区分好区的消息不会直接发送到服务端，而是放入生产者的缓存区，多条消息会被封装成一个批次（
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Batch），默认一个批次的大小是 16KBSender 线程启动以后会从缓存里面去获取可以发送的批次Sender 线程把一个一个批次发送到服务端Kafka采用大部分消息系统遵循的传统模式：Producer 将消息推送到 Broker，Consumer 从 Broker 获取消息。负载均衡是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度保证系统整体运行效率与稳定性负载均衡Kakfa 的负载均衡就是每个 Broker 都有均等的机会为 Kafka 的客户端（生产者与消费者）提供服务，可以负载分散到所有集群中的机器上。Kafka 通过智能化的分区领导者选举来实现负载均衡，提供智能化的 Leader 选举算法，可在集群的所有机器上均匀分散各个 Partition的 Leader，从而整体上实现负载均衡。故障转移Kafka 的故障转移是通过使用会话机制实现的，每台 Kafka 服务器启动后会以会话的形式把自己注册到 Zookeeper 服务器上。一旦服务器运转出现问题，就会导致与 Zookeeper 的会话不能维持从而超时断连，此时 Kafka 集群会选举出另一台服务器来完全替代这台服务
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 器继续提供服务。Kafka 中 Zookeeper 的作用Kafka 是一个使用 Zookeeper 构建的分布式系统。Kafka 的各 Broker 在启动时都要在 Zookeeper上注册，由 Zookeeper统一协调管理。如果任何节点失败，可通过Zookeeper从先前提交的偏移量中恢复，因为它会做周期性提交偏移量工作。同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也是 Zookeeper在维护Kafka 中消费者与消费者组的关系与负载均衡实现Consumer Group 是 Kafka 独有的可扩展且具有容错性的消费者机制。一个组内可以有多个 Consumer，它们共享一个全局唯一的 Group ID。组内的所有 Consumer协调在一起来消费订阅主题（Topic）内的所有分区（Partition）。当然，每个 Partition只能由同一个 Consumer Group内的一个 Consumer 来消费。消费组内的消费者可以使用多线程的方式实现，消费者的数量通常不超过分区的数量，且二者最好保持整数倍的关系，这样不会造成有空
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲的消费者。Consumer 订阅的是 Topic 的 Partition，而不是 Message。所以在同一时间点上，订阅到同一个分区的 Consumer 必然属于不同的 Consumer GroupConsumer Group与 Consumer的关系是动态维护的，当一个 Consumer 进程挂掉或者是卡住时，该 Consumer 所订阅的 Partition会被重新分配到改组内的其他Consumer 上，当一个 Consumer加入到一个 Consumer Group中时，同样会从其他的 Consumer 中分配出一个或者多个 Partition到这个新加入的 Consumer。当生产者试图发送消息的速度快于 Broker 可以处理的速度时，通常会发生QueueFullException首先先进行判断生产者是否能够降低生产速率，如果生产者不能阻止这种情况，为了处理增加的负载，用户需要添加足够的 Broker。或者选择生产阻塞，设置Queue.enQueueTimeout.ms 为 -1，通过这样处理，如果队列已满的情况，生产者将组织而不是删除消息。或者容忍这种异常，进行消息丢弃。Consumer 如何
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 消费指定分区消息Cosumer 消费消息时，想 Broker 发出 fetch 请求去消费特定分区的消息，Consumer 可以通过指定消息在日志中的偏移量 offset，就可以从这个位置开始消息消息，Consumer 拥有了 offset 的控制权，也可以向后回滚去重新消费之前的消息。也可以使用 seek(Long topicPartition) 来指定消费的位置。Replica、Leader 和 Follower 三者的概念:Kafka 中的 Partition 是有序消息日志，为了实现高可用性，需要采用备份机制，将相同的数据复制到多个 Broker 上，而这些备份日志就是 Replica，目的是为了防止数据丢失。所有 Partition 的副本默认情况下都会均匀地分布到所有 Broker 上,一旦领导者副本所在的 Broker 宕机，Kafka 会从追随者副本中选举出新的领导者继续提供服务。Leader： 副本中的领导者。负责对外提供服务，与客户端进行交互。生产者总是向 Leader 副本些消息，消费者总是从 Leader 读消息Follower： 副本中的追随者。被动地追随 Leader，不能与外界进
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行交付。只是向 Leader 发送消息，请求 Leader把最新生产的消息发给它，进而保持同步。Kafka 中 AR、ISR、OSR 三者的概念AR：分区中所有副本称为 ARISR：所有与主副本保持一定程度同步的副本（包括主副本）称为 ISROSR：与主副本滞后过多的副本组成 OSR分区副本什么情况下会从 ISR 中剔出Leader 会维护一个与自己基本保持同步的 Replica列表，该列表称为 ISR，每个Partition都会有一个 ISR，而且是由 Leader 动态维护。所谓动态维护，就是说如果一个 Follower比一个 Leader 落后太多，或者超过一定时间未发起数据复制请求，则 Leader 将其从 ISR 中移除。当 ISR 中所有 Replica 都向 Leader 发送 ACK（Acknowledgement确认）时，Leader 才 commit分区副本中的 Leader 如果宕机但 ISR 却为空该如何处理可以通过配置 unclean.leader.election ：true：允许 OSR 成为 Leader，但是 OSR 的消息较为滞后，可能会出现消息不一致的问题false：会一
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直等待旧 leader 恢复正常，降低了可用性Kafka的 Producer 有三种 ack机制，参数值有 0、1 和 -10： 相当于异步操作，Producer 不需要 Leader 给予回复，发送完就认为成功，继续发送下一条（批）Message。此机制具有最低延迟，但是持久性可靠性也最差，当服务器发生故障时，很可能发生数据丢失。1： Kafka 默认的设置。表示 Producer 要 Leader 确认已成功接收数据才发送下一条（批）Message。不过 Leader 宕机，Follower 尚未复制的情况下，数据就会丢失。此机制提供了较好的持久性和较低的延迟性。-1： Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些Follower都确认消息已同步，Producer 才发送下一条（批）Message。此机制持久性可靠性最好，但延时性最差Kafka 的 consumer 如何消费数据在 Kafka中，Producers 将消息推送给 Broker 端，在 Consumer 和 Broker 建立连接之后，会主动去 Pull（或者说 Fetch）消息。这种模式有些优点
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，首先 Consumer端可以根据自己的消费能力适时的去 fetch消息并处理，且可以控制消息消费的进度（offset）；此外，消费者可以控制每次消费的数，实现批量消费。Kafka 的 Topic 中 Partition 数据是怎么存储到磁盘的用磁盘顺序写+内存页缓存+稀疏索引Topic 中的多个 Partition 以文件夹的形式保存到 Broker，每个分区序号从 0递增，且消息有序。Partition 文件下有多个 Segment（xxx.index，xxx.log），Segment文件里的大小和配置文件大小一致。默认为 1GB，但可以根据实际需要修改。如果大小大于 1GB时，会滚动一个新的 Segment并且以上一个 Segment 最后一条消息的偏移量命名。生产者发送消息│▼Leader Partition│▼ (追加写入)当前活跃 Segment: [00000000000000.log]│▼ (每隔 4KB 数据)更新 .index/.timeindex│▼ (Segment 满 1GB)创建新 Segment: [00000000000015.log]消费者请求 offset=520│▼查 .
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: index 文件 → 找到 offset=500 → position=10240│▼从 .log 文件 10240 位置顺序扫描│▼找到 offset=520 的消息返回Kafka 创建 Topic 后如何将分区放置到不同的 Broker 中Kafka创建 Topic 将分区放置到不同的 Broker 时遵循以下规则：副本因子不能大于 Broker 的个数。第一个分区（编号为 0）的第一个副本放置位置是随机从 Broker List 中选择的。其他分区的第一个副本放置位置相对于第 0个分区依次往后移。也就是如果有 3个 Broker，3 个分区，假设第一个分区放在第二个 Broker 上，那么第二个分区将会放在第三个 Broker 上；第三个分区将会放在第一个 Broker 上，更多 Broker 与更多分区依此类推。剩余的副本相对于第一个副本放置位置其实是由nextReplicaShift决定的，而这个数也是随机产生的。Kafka 中如何进行主从同步Kafka动态维护了一个同步状态的副本的集合（a set of In-SyncReplicas），简称 ISR，在这个集合中的结点都是和 Leader 保持高
2025-08-11 10:55:45.106 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度一致的，任何一条消息只有被这个集合中的每个结点读取并追加到日志中，才会向外部通知“这个消息已经被提交”同步复制Producer 会先通过 Zookeeper识别到 Leader，然后向 Leader 发送消息，Leader收到消息后写入到本地 log文件。这个时候 Follower 再向 Leader Pull 消息，Pull回来的消息会写入的本地 log 中，写入完成后会向 Leader 发送 Ack 回执，等到 Leader 收到所有 Follower 的回执之后，才会向 Producer 回传 Ack。异步复制Kafka 中 Producer 异步发送消息是基于同步发送消息的接口来实现的，异步发送消息的实现很简单，客户端消息发送过来以后，会先放入一个 BlackingQueue队列中然后就返回了。Producer 再开启一个线程 ProducerSendTread 不断从队列中取出消息，然后调用同步发送消息的接口将消息发送给 Broker。Kafka 中什么情况下会出现消息丢失/不一致的问题消息发送时消息发送有两种方式：同步 - sync 和 异步 - async。默认是同步的方式，可以通过 prod
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ucer.type 属性进行配置，kafka 也可以通过配置 acks 属性来确认消息的生产0：表示不进行消息接收是否成功的确认1：表示当 leader 接收成功时的确认-1：表示 leader 和 follower 都接收成功的确认当 acks = 0 时，不和 Kafka 进行消息接收确认，可能会因为网络异常，缓冲区满的问题，导致消息丢失当 acks = 1 时，只有 leader 同步成功而 follower 尚未完成同步，如果 leader挂了，就会造成数据丢失消息消费时Kafka 有两个消息消费的 consumer 接口，分别是 low-level 和 hign-levellow-level：消费者自己维护 offset 等值，可以实现对 kafka 的完全控制high-level：封装了对 partition 和 offset，使用简单如果使用高级接口，可能存在一个消费者提取了一个消息后便提交了 offset，那么还没来得及消费就已经挂了，下次消费时的数据就是 offset + 1 的位置，那么原先 offset 的数据就丢失了Kafa 中如何保证顺序消费Kafka 的消费单元是 Partitio
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n，同一个 Partition 使用 offset 作为唯一标识保证顺序性，但这只是保证了在 Partition 内部的顺序性而不是 Topic 中的顺序，因此我们需要将所有消息发往统一 Partition 才能保证消息顺序消费，那么可以在发送的时候指定 MessageKey，同一个 key 的消息会发到同一个 Partition 中。Java介绍一下 javajava 是一门开源的跨平台的面向对象的计算机语言.跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而虚拟机是不同平台有不同版本,所以说 java 是跨平台的.面向对象有几个特点:1.封装两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者'封装'在一个不可分割的独立单元(即对象)中另一层含义指'信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能隐藏对象的功能实现细节。2.继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有父类相同的行为。3.多态多态是同一个行为具有多个不同表现形式或形态的能力。Java 语言中含有方法重载与对象多态两种形式的多态：1.方法重载：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。2.对象多态：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。Java有哪些数据类型？java 主要有两种数据类型1.基本数据类型基本数据有八个,byte,short,int,long 属于数值型中的整数型float,double属于数值型中的浮点型char属于字符型boolean属于布尔型2.引用数据类型引用数据类型有三个,分别是类,接口和数组接口和抽象类有什么区别？1.接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类是声明方法的存在而不去实现它的类。2.接口可以多继承，抽象类不行。3.接口定义方法，不能实现，默认是 public abstract，而抽象类可以实现部分方法。4.接口中基本数据类型为 public static final 并且需要给出初始值，而抽类象不是的。重载和重写什么区别？重写：1.参数列表必须完全与被重写的方法相同，
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否则不能称其为重写而是重载.2.返回的类型必须一直与被重写的方法的返回类型相同，否则不能称其为重写而是重载。3.访问修饰符的限制一定要大于被重写方法的访问修饰符4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。重载：1.必须具有不同的参数列表；2.可以有不同的返回类型，只要参数列表不同就可以了；3.可以有不同的访问修饰符；4.可以抛出不同的异常；常见的异常有哪些？NullPointerException 空指针异常ArrayIndexOutOfBoundsException 索引越界异常InputFormatException 输入类型不匹配SQLException SQL 异常IllegalArgumentException 非法参数NumberFormatException 类型转换异常 等等....异常要怎么解决？Java标准库内建了一些通用的异常，这些类以 Throwable 为顶层父类。Throwable又派生出 Error 类和 Exception类。错误：Error类以及他的子类的实例，代表了 JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，程序员应该关注 Exception 为父类的分支下的各种异常类。异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被 Java异常处理机制使用，是异常处理的核心。hashMap 线程不安全体现在哪里？在 hashMap1.7 中扩容的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法在任意版本的 hashMap 中，如果在插入数据时多个线程命中了同一个槽，可能会有数据覆盖的情况发生，导致线程不安全。说说进程和线程的区别？进程是系统资源分配和调度的基本单位，它能并发执行较高系统资源的利用率.线程是比进程更小的能独立运行的基本单位,创建、销毁、切换成本要小于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为 1，结果又是什么？Integer a = 1000，Integer b = 1000，a==b 结果为 falseInteger a = 1，Integer b = 1，a==b 结
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果为 true这道题主要考察 Integer 包装类缓存的范围,在-128~127 之间会缓存起来,比较的是直接缓存的数据,在此之外比较的是对象JMM 就是 Java 内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以 java 内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 java 程序在各种平台下都能达到一致的并发效果。Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。线程不能直接读写主内存中的变量。每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式cas 是什么？cas 叫做 CompareAndSwap，比较并交换，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是通过处理器的指令来保证操作的原子性，它主要包含三
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个变量：当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，如果相同就认为其他线程没有进行修改，而将新值写入内存聊聊 ReentrantLock 吧ReentrantLock 意为可重入锁，说起 ReentrantLock 就不得不说 AQS ，因为其底层就是使用 AQS 去实现的。ReentrantLock有两种模式，一种是公平锁，一种是非公平锁。公平模式下等待线程入队列后会严格按照队列顺序去执行非公平模式下等待线程入队列后有可能会出现插队情况公平锁第一步：获取状态的 state 的值如果 state=0 即代表锁没有被其它线程占用，执行第二步。如果 state!=0 则代表锁正在被其它线程占用，执行第三步。第二步：判断队列中是否有线程在排队等待如果不存在则直接将锁的所有者设置成当前线程，且更新状态 state 。如果存在就入队。第三步：判断锁的所有者是不是当前线程如果是则更新状态 state 的值。如果不是，线程进入队列排队等待。非公平锁获取状态的 state 的值如果 state=0 即代表锁
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 没有被其它线程占用，则设置当前锁的持有者为当前线程，该操作用 CAS 完成。如果不为 0或者设置失败，代表锁被占用进行下一步。此时获取 state 的值如果是，则给 state+1，获取锁如果不是，则进入队列等待如果是 0，代表刚好线程释放了锁，此时将锁的持有者设为自己如果不是 0，则查看线程持有者是不是自己多线程的创建方式有哪些？继承 Thread类，重写 run()方法public class Demo extends Thread{//重写父类 Thread的 run()public void run() {}public static void main(String[] args) {Demo d1 = new Demo();Demo d2 = new Demo();d1.start();d2.start();}}实现 Runnable接口，重写 run()public class Demo2 implements Runnable{//重写 Runnable接口的 run()public void run() {}public static void main(String[] args) {Th
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: read t1 = new Thread(new Demo2());Thread t2 = new Thread(new Demo2());t1.start();t2.start();}}实现 Callable 接口public class Demo implements Callable<String>{public String call() throws Exception {System.out.println("正在执行新建线程任务");Thread.sleep(2000);return "结果";}public static void main(String[] args) throws InterruptedException,ExecutionException {Demo d = new Demo();FutureTask<String> task = new FutureTask<>(d);Thread t = new Thread(task);t.start();//获取任务执行后返回的结果String result = task.get();}}使用线程池创建public class 
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Demo {public static void main(String[] args) {Executor threadPool = Executors.newFixedThreadPool(5);for(int i = 0 ;i < 10 ; i++) {threadPool.execute(new Runnable() {public void run() {//todo}});}}}线程池有哪些参数？corePoolSize：核心线程数，线程池中始终存活的线程数。2.maximumPoolSize: 最大线程数，线程池中允许的最大线程数。3.keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。4.unit: 单位，参数 keepAliveTime 的时间单位，7种可选。5.workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种可选。6.threadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。7.handler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicy。线程池的执行流程？判断线程池中的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程数是否大于设置的核心线程数如果小于，就创建一个核心线程来执行任务如果大于，就会判断缓冲队列是否满了如果没有满，则放入队列，等待线程空闲时执行任务如果队列已经满了，则判断是否达到了线程池设置的最大线程数如果没有达到，就创建新线程来执行任务如果已经达到了最大线程数，则执行指定的拒绝策略深拷贝、浅拷贝是什么？浅拷贝并不是真的拷贝，只是复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象聊聊 ThreadLocal 吧ThreadLocal其实就是线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。一个对象的内存布局是怎么样的?1.对象头: 对象头又分为 MarkWord 和 Class Pointer 两部分。MarkWord:包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位,gc 记录信息等等。ClassPointer:用来指向对象对应的 Class 对象（其对应的元数据对象）的内存地址。在 32 位系统占 4 字节，在 64 位系统中占 8 字节
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 。2.Length:只在数组对象中存在，用来记录数组的长度，占用 4 字节3.Instance data: 对象实际数据，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括静态成员变量，因为其是在方法区维护的)4.Padding:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，padding 的作用就是补充字节，保证对象是 8 字节的整数倍。HashMapHashMap的底层数据结构是什么？JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。链表过长，会严重影响HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是糟糕的 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：当链
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 表超过 8 且数据总量超过 64 时会转红黑树。将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。解决 hash冲突的办法有哪些？HashMap用的哪种？解决 Hash 冲突方法有：开放定址法：也称为再散列法，基本思想就是，如果 p=H(key)出现冲突时，则以p为基础，再次 hash，p1=H(p),如果 p1再次出现冲突，则以 p1为基础，以此类推，直到找到一个不冲突的哈希地址 pi。因此开放定址法所需要的 hash表的长度要大于等于所需要存放的元素，而且因为存在再次 hash，所以只能在删除的节点上做标记，而不能真正删除节点。再哈希法：双重散列，多重散列，提供多个不同的 hash函数，当 R1=H1(key1)发生冲突时，再计算 R2=H2(key1)，直到没有冲突为止。这样做虽然不易产生堆集，但增加了计算的时间。链地址法：拉链法，将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第 i 个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。建立公共溢出区：将哈希表分为公共表和
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。HashMap中采用的是链地址法为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。为什么 hash 值要与 length-1 相与？把 hash 值对数组长度取模运算，模运算的消耗很大，没有位运算快。当 length 总是 2 的 n次方时，h& (length-1) 运算等价于对 length取模，也就是 h%length，但是 & 比 % 具有更高的效率HashMap数组的长度为什么是 2 的幂次方？2 的 N 次幂有助于减少碰撞的几率。如果 length 为 2的幂次方，则 length-1 转化为二进制必定是 11111……的形式，在与 h 的二进制与操作效率会非
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 常的快，而且空间不浪费。HashMap 的 put方法流程？首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；2、如果数组是空的，则调用 resize 进行初始化；3、如果没有哈希冲突直接放在对应的数组下标里；4、如果冲突了，且 key 已经存在，就覆盖掉 value；5、如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；6、如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。一般用什么作为 HashMap的 key?一般用 Integer、String 这种不可变类当作 HashMap 的 key，String 最为常见。因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的。Integer、String 这些类已经很规范的重写了hashCode() 以及 
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: equals() 方法。MySQL关系型和非关系型数据库的区别？关系型数据库的优点容易理解，因为它采用了关系模型来组织数据。可以保持数据的一致性。数据更新的开销比较小。支持复杂查询（带 where 子句的查询）非关系型数据库（NOSQL）的优点无需经过 SQL 层的解析，读写效率高。基于键值对，读写性能很高，易于扩展可以支持多种类型数据的存储，如图片，文档等等。扩展（可分为内存性数据库以及文档型数据库，比如 Redis，MongoDB，HBase 等，适合场景：数据量大高可用的日志系统/地理位置存储系统）。详细说一下一条 MySQL 语句执行的步骤Server 层按顺序执行 SQL 的步骤为：客户端请求 -> 连接器（验证用户身份，给予权限）查询缓存（存在缓存则直接返回，不存在则执行后续操作）分析器（对 SQL 进行词法分析和语法分析操作）优化器（主要对执行的 SQL 优化选择最优的执行方案方法）执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-> 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）MySQL 使用索引的原因？根本原因索引的出现，就是为了提高数据查询的效率，就像书的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 目录一样。对于数据库的表而言，索引其实就是它的“目录”。扩展创建唯一性索引，可以保证数据库表中每一行数据的唯一性。帮助引擎层避免排序和临时表将随机 IO 变为顺序 IO，加速表和表之间的连接索引的三种常见底层数据结构以及优缺点三种常见的索引底层数据结构：分别是哈希表、有序数组和搜索树。哈希表这种适用于等值查询的场景，比如 memcached 以及其它一些 NoSQL 引擎，不适合范围查询，哈希表的数据是完全无序存储的。它只能回答“某个键值等于多少”的记录在哪，无法高效地查询“键值在某个范围之间”的所有记录（如WHERE id BETWEEN 10 AND 20）。需要扫描全表或遍历所有桶，效率极低 (O(n))。有序数组索引只适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高。N 叉树由于读写上的性能优点以及适配磁盘访问模式以及广泛应用在数据库引擎中。扩展（以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。）索引的常见类型以及它是如何发挥作用的？根据叶子节点的内容，索引类型分为主键索引和非主键索引。主键索引的叶子节点存的整行数据，在 InnoDB 里也被称为聚簇索引。非主键索引叶子节点存的主键的值，在 InnoDB 里也被称为二级索引MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件。MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data域存放的是数据记录的地址，索引文件和数据文件是分离的InnoDB 为什么设计 B+ 树索引？两个考虑因素：InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。CPU 将磁盘上的数据加载到内存中需要花费大量时间。为什么选择 B+ 树：哈希索引虽然能提供 O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机IO。而 B+ 树的所有叶节点可以通过指针来相互连接
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，减少顺序遍历带来的随机 IO。普通索引还是唯一索引？由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。什么是覆盖索引和索引下推？覆盖索引：在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。索引下推：MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。MySQL 的 change buffer 是什么？当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。注意唯一索引的更新就不
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 能使用 change buffer，实际上也只有普通索引可以使用。适用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 changebuffer 的维护代价。MySQL 是如何判断一行扫描数的？MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。redo log 和 binlog 的区别？为什么需要 redo log？redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash 后 内存
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据丢失的隐患，而 redo log 具备 crash safe 崩溃恢复 的能力。为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让innoDB 判断哪些数据已经刷盘，哪些数据还没有。但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交redo log 每次更新操作
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。redo log 内部结构是基于页的，记录了这个页的字段值变化，只要 crash 后读取redo log 进行重放，就可以恢复数据。这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备当数据库 crash 后，如何恢复未刷盘的数据到内存中？根据 redo log 和 binlog 的两阶段提交，未持久化的数据分为几种情况：change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从binlog 恢复 redo log，再从 redo log 恢复 change buffer。change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。redo log 写入方式？redo log 包括两部分内容，分别是内存中的日志缓冲(re
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: do log buffer)和磁盘上的日志文件(redo log file)。MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是 WAL。可以发现，redo log buffer 写入到 redo log file，是经过 OS buffer 中转的。其实可以通过参数 innodb_flush_log_at_trx_commit 进行配置，参数值含义如下：0：称为延迟写，事务提交时不会将 redo log buffer 中日志写入到 OS buffer，而是每秒写入 OS buffer 并调用写入到 redo log file 中。1：称为实时写，实时刷”，事务每次提交都会将 redo log buffer 中的日志写入 OS buffer 并保存到 redo log file 中。2： 称为实时写，延迟刷。每次事务提交写入到 OS buffer，然后是每秒将日志写入到 redo log file。
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: redo log 的执行流程?MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server层。MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。InnoDB 存储引擎层将 a修改为 1的这个操作记录到内存中。记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。binlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗?binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。但是
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。什么是两阶段提交？MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog，这就是"两阶段提交"。而两阶段提交就是让这两个状态保持逻辑上的一致。redolog 用于恢复主机故障时的未更新的物理数据，binlog 用于备份操作。两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。为什么需要两阶段提交呢?如果不用两阶段提交的话，可能会出现这样情况先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log备份恢复时，数据不一致。两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。在恢复数据时，redolog 状态为 com
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: mit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。MySQL 怎么知道 binlog 是完整的?一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event什么是 WAL 技术，有什么优点？WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过 redo log恢复，所以能够实现快速响应 SQL 语句redo log 日志格式redo log buffer (内存中)是由首尾相连的四个文件组成的，它们分别是：ib_logfile_1、ib_logfile_2、ib_logfile_3、ib_logfile_4。write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。chec
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: kpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。有了 redo log，当数据库发生宕机重启后，可通过 redo log 将未落盘的数据（check point 之后的数据）恢复，保证已经提交的事务记录不会丢失，这种能力称为 crash-safe。InnoDB 数据页结构一个数据页大致划分七个部分File Header：表示页的一些通用信息，占固定的 38 字节。page Header：表示数据页专有信息，占固定的 56 字节。inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的 26 字节。User Records：真正存储我们插入的数据，大小不固定。Free Space：页中尚未使用的部分，大小不固定。Page Directory：页中某些记录的相对位置，也就是各个槽对
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 应的记录在页面中的地址偏移量。File Trailer：用于检验页是否完整，占固定大小 8 字节。MySQL 是如何保证数据不丢失的？只要 redolog 和 binlog 保证持久化磁盘就能确保 MySQL 异常重启后回复数据在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。28、误删数据怎么办？DBA 的最核心的工作就是保证数据的完整性，先要做好预防，预防的话大概是通过这几个点：权限控制与分配(数据库和服务器权限)制作操作规范定期给开发进行培训搭建延迟备库做好 SQL 审计，只要是对线上数据有更改操作的语句(DML 和 DDL)都需要进行审核做好备份。备份的话又分为两个点 (1)如果数据量比较大，用物理备份xtrabackup。定期对数据库进行全量备份，也可以做增量备份。 (2)如果数据量较少，用 mysqldump 或者 mysqldumper。再利用 binlog 来恢复或者搭建主从的方式来恢复数据。 定期备份 binlog 文件也是很有必要的如果发
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 生了数据删除的操作，又可以从以下几个点来恢复:DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的myflash，也是一个不错的工具，本质都差不多，都是先解析 binlog event，然后在进行反转。把 delete 反转为 insert，insert 反转为 delete，update 前后 image 对调。所以必须设置 binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上。DDL 语句误操作(truncate 和 drop)，由于 DDL 语句不管 binlog_format 是 row还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长rm 删除：使用备份跨机房，或者最好是跨城市保存。DDL（数据定义语言） DML（数据操纵语言） DQL（数据查询语言）drop、truncate 和 delete 的区别DELETE 语句执行删除的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。drop 语句将表所占用的空间全释放掉。在速度上，一般来说，drop> truncate > delete。如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大；如果想删除表，当然用 drop； 如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可；如果和事务有关，或者想触发 trigger，还是用 delete； 如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 W
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: eb、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。都说 InnoDB 好，那还要不要使用 MEMORY 引擎？内存表就是使用 memory 引擎创建的表为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉MySQL 是如何保证主备同步？主备关系的建立：一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从 binlog 文件 A的位置 P”开始同步，主库就从这个指定的位置开始往后发。而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。MySQL 主备切换流程：客户端读写都是直接访问 A，而节点 B是备库，只要将 A的更新都同步过来，到本地执行就可以保证数据是相同的。当需要切换的时候就
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 把节点换一下，A的节点 B的备库一个事务完整的同步过程：备库 B和主库 A建立来了长链接，主库 A内部专门线程用于维护了这个长链接。在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求 binlog 包括文件名和日志偏移量在备库 B上执行 start-slave 命令备库会启动两个线程：io_thread 和sql_thread 分别负责建立连接和读取中转日志进行解析执行备库读取主库传过来的 binlog 文件备库收到文件写到本地成为中转日志后来由于多线程复制方案的引入，sql_thread 演化成了多个线程什么是主备延迟主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有：有些部署条件下，备库所在机器的性能要比主库性能差。备库的压力较大。大事务，一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10分钟MySQL 的一主一备和一主多从有什么区别？在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上短时间提高 MySQL 性能的方法第一种方法：先处理掉那
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。 kill connection + id第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL选错了索引（force index）。InnoDB 为什么要用自增 ID 作为主键？自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。说一下 MySQL 的锁MySQL 在 server 层 和 存储引擎层 都运用了大量的锁MySQL server 层需要讲两种锁，第一种是 MDL(metadata lock) 元数据锁，第二种则 Table Lock 表锁。MDL 又名元数据锁，那么什么是元数据呢，任何描述数据库的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内容就是元数据，比如我们的表结构、库结构等都是元数据。那为什么需要 MDL 呢？主要解决两个问题：事务隔离问题；数据复制问题InnoDB 有五种表级锁：IS（意向读锁）；IX（意向写锁）；S（读）；X（写）；AUTO-INC在对表进行 select/insert/delete/update 语句时候不会加表级锁IS 和 IX 的作用是为了判断表中是否有已经被加锁的记录自增主键的保障就是有 AUTO-INC 锁，是语句级别的：为表的某个列添加AUTO_INCREMENT 属性，之后在插⼊记录时，可以不指定该列的值，系统会⾃动为它赋上单调递增的值。InnoDB 4 种行级锁RecordLock：记录锁GapLock：间隙锁解决幻读；前一次查询不存在的东西在下一次查询出现了，其实就是事务 A中的两次查询之间事务 B执行插入操作被事务 A感知了Next-KeyLock：锁住某条记录又想阻止其它事务在改记录前面的间隙插入新纪录InsertIntentionLock：插入意向锁;如果插入到同一行间隙中的多个事务未插入到间隙内的同一位置则无须等待行锁和表锁的抉择全表扫描用行级锁索引是一种能提高数据库查询效率的数据结构。它可
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 以比作一本字典的目录，可以帮你快速找到对应的记录。索引一般存储在磁盘的文件中，它是占用物理空间的。正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。数据结构维度B+树索引：所有数据存储在叶子节点，复杂度为 O(logn)，适合范围查询。哈希索引: 适合等值查询，检索效率高，一次到位。全文索引：MyISAM 和 InnoDB 中都支持使用全文索引，一般在文本类型char,text,varchar 类型上创建。R-Tree 索引: 用来对 GIS 数据类型创建 SPATIAL 索引物理存储维度聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。（Innodb 存储引擎）非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。（Innodb 存储引擎）逻辑维度主键索引：一种特殊的唯一索引，不允许有空值。普通索引：MySQL 中基本索引类型，允许空值和重复值。联合索引：多个字段创建的索引，使用时遵循最左前缀原则。唯一索引：索引列中的值必须是唯一的，但是允许为空值。空间索引：MySQL5.7 之后支持空间索引，在空间索引这方面遵循 Op
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: enGIS 几何数据模型规则索引什么时候会失效？查询条件包含 or，可能导致索引失效如果字段类型是字符串，where 时一定用引号括起来，否则索引失效like 通配符可能导致索引失效。联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。在索引列上使用 mysql 的内置函数，索引失效。对索引列运算（如，+、-、*、/），索引失效。索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。索引字段上使用 is null， is not null，可能导致索引失效。左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。mysql 估计使用全表扫描要比使用索引快,则不使用索引哪些场景不适合建立索引？数据量少的表，不适合加索引更新比较频繁的也不适合加索引区分度低的字段不适合加索引（如性别）where、group by、order by 等后面没有使用到的字段，不需要建立索引已经有冗余的索引的情况（比如已经有 a,b 的联合索引，不需要再单独建立 a索引）为什么不是一般二叉树？如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找 树来说，查找效率更稳定，总体的查
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 找速度也更快。为什么不是平衡二叉树呢？我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作 为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说 的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果 是 B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数 就降下来啦，查询效率就快啦。那为什么不是 B 树而是 B+树呢？B+树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储 键值，也会存储数据。innodb 中页的默认大小是 16KB，如果不存储数据，那 么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就 会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数有会再次减少，数据查 询的效率也会更快。B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链 表连着的。那么 B+树使得范围查找，排序查找，分组查找以及去重查找变得 异常简单。一次 B+树索引树查找过程：select * from Temployee where age=32;这条 SQL 查询语句执行大概流程是这样的：搜索 idx_age 索引
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 树，将磁盘块 1加载到内存，由于 32<43,搜索左路分支，到磁盘寻址磁盘块 2。将磁盘块 2加载到内存中，由于 32<36,搜索左路分支，到磁盘寻址磁盘块 4。将磁盘块 4加载到内存中，在内存继续遍历，找到 age=32 的记录，取得 id = 400.拿到 id=400 后，回到 id 主键索引树。搜索 id 主键索引树，将磁盘块 1加载到内存，因为 300<400<500,所以在选择中间分支，到磁盘寻址磁盘块 3。虽然在磁盘块 3，找到了 id=400，但是它不是叶子节点，所以会继续往下找。到磁盘寻址磁盘块 8。将磁盘块 8加载内存，在内存遍历，找到 id=400 的记录，拿到 R4 这一行的数据，好的，大功告成。什么是回表？如何减少回表？当查询的数据在索引树中，找不到的时候，需要回到主键索引树中去获取，这个过程叫做回表。比如在第 6小节中，使用的查询 SQLselect * from Temployee where age=32;需要查询所有列的数据，idx_age 普通索引不能满足，需要拿到主键 id 的值后，再回到 id 主键索引查找获取，这个过程就是回表。什么是覆盖索引？如果我们查询 SQL 的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  select * 修改为 select id, age 的话，其实是不需要回表的。因为 id 和 age 的值，都在 idx_age 索引树的叶子节点上，这就涉及到覆盖索引的知识点了。覆盖索引是 select 的数据列只用从索引中就能够取得，不必回表，换句话说，查询列要被所建的索引覆盖聊聊索引的最左前缀原则、索引的最左前缀原则，可以是联合索引的最左 N个字段。比如你建立一个组合索引（a,b,c），其实可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。索引下推了解过吗？什么是索引下推select * from employee where name like '小%' and age=28 and sex='0';其中，name 和 age 为联合索引（idx_name_age）。如果是Mysql5.6之前，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键 id，然后回表找出数据行，再去对比年龄和性别等其他字段。有些朋友可能觉得奇怪，idx_name_age（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄 age 
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。因此，MySQL5.6 版本之后，选出包含“小”字后，顺表过滤 age=28大表如何添加索引？如果一张表数据量级是千万级别以上的，那么，如何给这张表添加索引？我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：先创建一张跟原表 A数据结构相同的新表 B。在新表 B添加需要加上的新索引。把原表 A数据导到新表 Brename 新表 B为原表的表名 A，原表 A换别的表名Hash 索引和 B+树区别是什么？你在设计索引是怎么抉择的？B+树可以进行范围查询，Hash 索引不能。B+树支持联合索引的最左侧原则，Hash 索引不支持。B+树支持 order by 排序，Hash 索引不支持。Hash 索引在等值查询上比 B+树效率更高。（但是索引列的重复值很多的话，Hash冲突，效率降低）。B+树使用 like 进行模糊查询的时候，like 后面（比如%开头）的话可以起到优化的作用，Hash 索引根
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 本无法进行模糊查询。索引有哪些优缺点？优点：索引可以加快数据查询速度，减少查询时间唯一索引可以保证数据库表中每一行的数据的唯一性缺点：创建索引和维护索引要耗费时间索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间以表中的数据进行增、删、改的时候，索引也要动态的维护。聚簇索引与非聚簇索引的区别聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它表示索引结构和数据一起存放的索引。非聚集索引是索引结构和数据分开存放的索引。接下来，我们分不同存存储引擎去聊哈~在 MySQL 的 InnoDB 存储引擎中， 聚簇索引与非聚簇索引最大的区别，在于叶节点是否存放一整行记录。聚簇索引叶子节点存储了一整行记录，而非聚簇索引叶子节点存储的是主键信息，因此，一般非聚簇索引还需要回表查询。一个表中只能拥有一个聚集索引（因为一般聚簇索引就是主键索引），而非聚集索引一个表则可以存在多个。一般来说，相对于非聚簇索引，聚簇索引查询效率更高，因为不用回表。而在 MyISM 存储引擎中，它的主键索引，普通索引都是非聚簇索引，因为数据和索引是分开的，叶子节点都使用一个地址指向真正的表数据。Nginx什么是 Ng
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: inx？Nginx 是一个 轻量级/高性能的反向代理 Web 服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。他实现非常高效的反向代理、负载平衡，他可以处理 2-3万并发连接数，官方监测能支持 5万并发，现在中国使用 nginx 网站用户有很多，例如：新浪、网易、 腾讯等。Nginx 怎么处理请求的？首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的 Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与 nginx 进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建nginx 对连接的封装，即 ngx
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: _connection_t 结构体。接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。Nginx 是如何实现高并发的？如果一个 server 采用一个进程(或者线程)负责一个 request 的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。而 Nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。每进来一个 request ，会有一个 worker 进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发 request ，并等待请求返回。那么，这个处理的 worker 不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有 request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker 才会来接手
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，这个 request 才会接着往下走。这就是为什么说，Nginx 基于事件模型。由于 web server 的工作性质决定了每个 request 的大部份生命都是在网络传输中，实际上花费在 server 机器上的时间片不多。这是几个进程就解决高并发的秘密所在。即：webserver 刚好属于网络 IO 密集型应用，不算是计算密集型。异步，非阻塞，使用 epoll ，和大量细节处的优化。也正是 Nginx 之所以然的技术基石。什么是正向代理？一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的 OpenVPN 等等。什么是反向代理？反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet 上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 反向代理总结就一句话：代理端代理的是服务端。反向代理服务器的优点是什么?反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。cookie 和 session 区别？共同：存放用户信息。存放的形式：key-value 格式 变量和变量内容键值对。区别：cookie存放在客户端浏览器每个域名对应一个 cookie，不能跨跃域名访问其他 cookie用户可以查看或修改 cookiehttp 响应报文里面给你浏览器设置钥匙（用于打开浏览器上锁头）session:存放在服务器（文件，数据库，redis）存放敏感信息锁头为什么 Nginx 不使用多线程？Apache: 创建多个进程或线程，而每个进程或线程都会为其分配 cpu 和内存（线程要比进程小的多，所以 worker 支持比 perfork 高的并发），并发过大会榨干服务器资源。Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置 Nginx 主进程的工作进程的数量）(epoll)，不会为每个请求分配 cpu 和内存资源，节省了大量资源，同时也减少了大量的 
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: CPU 的上下文切换。所以才使得 Nginx 支持更高的并发。nginx 和 apache 的区别轻量级，同样起 web 服务，比 apache 占用更少的内存和资源。抗并发，nginx 处理请求是异步非阻塞的，而 apache 则是阻塞性的，在高并发下 nginx 能保持低资源，低消耗高性能。高度模块化的设计，编写模块相对简单。最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，nginx是异步的，多个连接可以对应一个进程什么是动态资源、静态资源分离？动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。为什么要做动、静分离？在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。什么叫 CDN 服务？CDN ，即内容分发网络。其目的是，通过在现有的 Internet 中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务Nginx 怎么做的动静分离？只需要指定路径对应的目录。locatio
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n/可以使用正则表达式匹配。并指定对应的硬盘中的目录Nginx 负载均衡的算法怎么实现的?策略有哪些?为了避免服务器崩溃，大家会通过负载均衡的方式来分担服务器压力。将对台服务器组成一个集群，当用户访问时，先访问到一个转发服务器，再由转发服务器将访问分发到压力更小的服务器。Nginx 负载均衡实现的策略有以下种：1 .轮询(默认)每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。upstream backserver {server 192.168.0.12;server 192.168.0.13;}2. 权重 weightweight 的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。# 权重越高，在被访问的概率越大，如上例，分别是 20%，80%。upstream backserver {server 192.168.0.12 weight=2;server 192.168.0.13 weight=8;}3. ip_hash( IP 绑定)每个请求按访问 IP 的哈希结果分配，
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 使来自同一个 IP 的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的 session 共享问题upstream backserver {ip_hash;server 192.168.0.12:88;server 192.168.0.13:80;}Nginx 虚拟主机怎么配置?1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台3、基于 ip 的虚拟主机。location 的作用是什么？location 指令的作用是根据用户请求的 URI 来执行不同的应用，也就是根据用户请求的网站 URL 进行匹配，匹配成功即进行相关的操作限流怎么做的？Nginx 限流就是限制用户请求速度，防止服务器受不了限流有 3种正常限制访问频率（正常流量）突发限制访问频率（突发流量）限制并发连接数Nginx 的限流都是基于漏桶流算法漏桶流算法和令牌桶算法知道？漏桶算法漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 续进入的水直接溢出（拒绝请求），以此实现限流。令牌桶算法令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。Nginx 配置高可用性怎么配置？当上游服务器(真实访问服务器)，一旦出现故障或者是没有及时相应的话，应该直接轮训到下一台服务器，保证服务器的高可用生产中如何设置 worker 进程的数量呢？在有多个 cpu 的情况下，可以设置多个 worker，worker 进程的数量可以设置到和 cpu 的核心数一样多，如果在单个 cpu 上起多个 worker 进程，那么操作系统会在多个 worker 之间进行调度，这种情况会降低系统性能，如果只有一个 cpu，那么只启动一个 worker 进程就可以了。Java 基础八股文Java 语言具有哪些特点？Java 为纯面向对象的语言。它能够直接反应现实生活中的
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象。具有平台无关性。Java 利用 Java 虚拟机运行字节码，无论是在 Windows、Linux还是 MacOS 等其它平台对 Java 程序进行编译，编译后的程序可在其它平台运行。Java 为解释型语言，编译器把 Java 代码编译成平台无关的中间代码，然后在JVM 上解释运行，具有很好的可移植性。Java 提供了很多内置类库。如对多线程支持，对网络通信支持，最重要的一点是提供了垃圾回收器。Java 具有较好的安全性和健壮性。Java 提供了异常处理和垃圾回收机制，去除了 C++中难以理解的指针特性JDK 与 JRE 有什么区别？JDK：Java 开发工具包（Java Development Kit），提供了 Java 的开发环境和运行环境。JRE：Java 运行环境(Java Runtime Environment)，提供了 Java 运行所需的环境。JDK 包含了 JRE。如果只运行 Java 程序，安装 JRE 即可。要编写 Java 程序需安装 JDK简述 Java 基本数据类型byte: 占用 1 个字节，取值范围-128 ~ 127short: 占用 2 个字节，取值范围-215 ~ 21
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 5-1int：占用 4 个字节，取值范围-231 ~ 231-1long：占用 8 个字节float：占用 4 个字节double：占用 8 个字节char: 占用 2 个字节boolean：占用大小根据实现虚拟机不同有所差异简述自动装箱拆箱对于 Java 基本数据类型，均对应一个包装类。装箱就是自动将基本数据类型转换为包装器类型，如 int->Integer拆箱就是自动将包装器类型转换为基本数据类型，如 Integer->int简述 Java 访问修饰符default: 默认访问修饰符，在同一包内可见private: 在同一类内可见，不能修饰类protected : 对同一包内的类和所有子类可见，不能修饰类public: 对所有类可见构造方法、成员变量初始化以及静态成员变量三者的初始化顺序？先后顺序：静态成员变量、成员变量、构造方法。详细的先后顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。面向对象的三大特性？继承：对象的一个新类可以从现有的类中派生，派生类可以从它的基类那继承方法和实例变量，且
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 派生类可以修改或新增新的方法使之更适合特殊的需求。封装：将客观事物抽象成类，每个类可以把自身数据和方法只让可信的类或对象操作，对不可信的进行信息隐藏。多态：允许不同类的对象对同一消息作出响应。不同对象调用相同方法即使参数也相同，最终表现行为是不一样的为什么 Java 语言不支持多重继承？为了程序的结构能够更加清晰从而便于维护。假设 Java 语言支持多重继承，类C 继承自类 A 和类 B，如果类 A 和 B 都有自定义的成员方法 f()，那么当代码中调用类 C 的 f() 会产生二义性。Java 语言通过实现多个接口间接支持多重继承，接口由于只包含方法定义，不能有方法的实现，类 C 继承接口 A 与接口 B 时即使它们都有方法 f()，也不能直接调用方法，需实现具体的 f()方法才能调用，不会产生二义性。多重继承会使类型转换、构造方法的调用顺序变得复杂，会影响到性能Java 提供的多态机制？Java 提供了两种用于多态的机制，分别是重载与覆盖(重写)。重载：重载是指同一个类中有多个同名的方法，但这些方法有不同的参数，在编译期间就可以确定调用哪个方法。覆盖：覆盖是指派生类重写基类的方法，使用基类指向其子类的实例
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象，或接口的引用变量指向其实现类的实例对象，在程序调用的运行期根据引用变量所指的具体实例对象调用正在运行的那个对象的方法，即需要到运行期才能确定调用哪个方法重载与覆盖的区别？覆盖是父类与子类之间的关系，是垂直关系；重载是同一类中方法之间的关系，是水平关系。覆盖只能由一个方法或一对方法产生关系；重载是多个方法之间的关系。覆盖要求参数列表相同；重载要求参数列表不同。覆盖中，调用方法体是根据对象的类型来决定的，而重载是根据调用时实参表与形参表来对应选择方法体。重载方法可以改变返回值的类型，覆盖方法不能改变返回值的类型。接口和抽象类的相同点和不同点？相同点:都不能被实例化。接口的实现类或抽象类的子类需实现接口或抽象类中相应的方法才能被实例化。不同点：接口只能有方法定义，不能有方法的实现，而抽象类可以有方法的定义与实现。实现接口的关键字为 implements，继承抽象类的关键字为 extends。一个类可以实现多个接口，只能继承一个抽象类。当子类和父类之间存在逻辑上的层次结构，推荐使用抽象类，有利于功能的累积。当功能不需要，希望支持差别较大的两个或更多对象间的特定交互行为，推荐使用接口。使用接口能降低软件系统的耦合
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度，便于日后维护或添加删除方法Java 语言中关键字 static 的作用是什么？static 的主要作用有两个：为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。具体而言 static 又可分为 4 种使用方式：修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用。修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法。修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次。修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方法为什么要把 String 设计为不可变？节省空间：字符串常量存储在 JVM 的字符串池中可以被用户共享。提高效率：String 可以被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。安全：String 常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改简述 String/StringBuffer 与 StringBuilderString 类采用利用 final 修饰的字符数组进行字符串保存，因此不可变。如果对 String 类型对象修改，需要新建对象，将老字符和新增加的字符一并存进去。StringBuilder，采用无 final 修饰的字符数组进行保存，因此可变。但线程不安全。StringBuffer，采用无 final 修饰的字符数组进行保存，可理解为实现线程安全的 StringBuilder。判等运算符==与 equals 的区别？== 比较的是引用，equals 比较的是内容。如果变量是基础数据类型，== 用于比较其对应值是否相等。如果变量指向的是对象，== 用于比较两个对象是否指向同一块存储空间。equals 是 Object 类提供的方法之一，每个 J
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 类都继承自 Object 类，所以每个对象都具有 equals 这个方法。Object 类中定义的 equals 方法内部是直接调用 == 比较对象的。但通过覆盖的方法可以让它不是比较引用而是比较数据内容简述 Java 异常的分类Java 异常分为 Error（程序无法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。Error 常见的有 StackOverFlowError、OutOfMemoryError 等等。Exception 可分为运行时异常和非运行时异常。对于运行时异常，可以利用 trycatch 的方式进行处理，也可以不处理。对于非运行时异常，必须处理，不处理的话程序无法通过编译final、finally 和 finalize 的区别是什么？final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承。finally 作为异常处理的一部分，只能在 try/catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下。finalize 是 Object 类的一个方法
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存简述 Java 中 Class 对象java 中对象可以分为实例对象和 Class 对象，每一个类都有一个 Class 对象，其包含了与该类有关的信息。获取 Class 对象的方法：Class.forName(“类的全限定名”)实例对象.getClass()类名.classJava 反射机制是什么？Java 反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得 Java 具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射 API。简述 Java 序列化与反序列化的实现序列化：将 java 对象转化为字节序列，由此可以通过网络对象进行传输。反序列化：将字节序列转化为 java 对象。具体实现：实现 Serializable 接口，或实现 Externalizable 接口中的writeExte
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: rnal()与 readExternal()方法。简述 Java 的 ListList 是一个有序队列，在 Java 中有两种实现方式:ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。Java 中线程安全的基本数据结构有哪些HashTable: 哈希表的线程安全版，效率低ConcurrentHashMap：哈希表的线程安全版，效率高，用于替代 HashTableVector：线程安全版 ArraylistStack：线程安全版栈BlockingQueue 及其子类：线程安全版队列简述 Java 的 SetSet 即集合，该数据结构不允许元素重复且无序。Java 对 Set 有三种实现方式：HashSet 通过 HashMap 实现，HashMap 的 Key 即 HashSet 存储的元素，Value系统自定义一个名为 PRESENT 的 Object 类型常量。判断元素是否相同时，先比较 hashCode，相同后再利用 equ
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: als 比较，查询 O(1)LinkedHashSet 继承自 HashSet，通过 LinkedHashMap 实现，使用双向链表维护元素插入顺序。TreeSet 通过 TreeMap 实现的，底层数据结构是红黑树，添加元素到集合时按照比较规则将其插入合适的位置，保证插入后的集合仍然有序。查询 O(logn)简述 Java 的 HashMapJDK8 之前底层实现是数组 + 链表，JDK8 改为数组 + 链表/红黑树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因子 loadFactor。HashMap 中数据以键值对的形式存在，键对应的 hash 值用来计算数组下标，如果两个元素 key 的 hash 值一样，就会发生哈希冲突，被放到同一个链表上。table 数组记录 HashMap 的数据，每个下标对应一条链表，所有哈希冲突的数据都会被存放到同一条链表，Node/Entry 节点包含四个成员变量：key、value、next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红黑树为何 HashMap 线程不安全在 JDK1.7 中，HashMap 采用头插法插入元素
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，因此并发情况下会导致环形链表，产生死循环。虽然 JDK1.8 采用了尾插法解决了这个问题，但是并发下的 put 操作也会使前一个 key 被后一个 key 覆盖。由于 HashMap 有扩容机制存在，也存在 A 线程进行扩容后，B 线程执行 get 方法出现失误的情况。简述 Java 的 TreeMapTreeMap 是底层利用红黑树实现的 Map 结构，底层实现是一棵平衡的排序二叉树，由于红黑树的插入、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表无法提供键值对的有序输出，红黑树可以按照键的值的大小有序输出ArrayList、Vector 和 LinkedList 有什么共同点与区别？ArrayList、Vector 和 LinkedList 都是可伸缩的数组，即可以动态改变长度的数组。ArrayList 和 Vector 都是基于存储元素的 Object[] array 来实现的，它们会在内存中开辟一块连续的空间来存储，支持下标、索引访问。但在涉及插入元素时可能需要移动容器中的元素，插入效率较低。当存储元素超过容器的初始化容量大小，ArrayList 与 Vector 均会进
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行扩容。Vector 是线程安全的，其大部分方法是直接或间接同步的。ArrayList 不是线程安全的，其方法不具有同步性质。LinkedList 也不是线程安全的。LinkedList 采用双向列表实现，对数据索引需要从头开始遍历，因此随机访问效率较低，但在插入元素的时候不需要对数据进行移动，插入效率较高HashMap 和 Hashtable 有什么区别？HashMap 是 Hashtable 的轻量级实现，HashMap 允许 key 和 value 为 null，但最多允许一条记录的 key 为 null.而 HashTable 不允许。HashTable 中的方法是线程安全的，而 HashMap 不是。在多线程访问 HashMap需要提供额外的同步机制。Hashtable 使用 Enumeration 进行遍历，HashMap 使用 Iterator 进行遍历。如何决定使用 HashMap 还是 TreeMap?如果对 Map 进行插入、删除或定位一个元素的操作更频繁，HashMap 是更好的选择。如果需要对 key 集合进行有序的遍历，TreeMap 是更好的选择Collection 和 Colle
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ctions 有什么区别？Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。Collections 是一个包装类，包含了很多静态方法、不能被实例化，而是作为工具类使用，比如提供的排序方法：Collections.sort(list);提供的反转方法：Collections.reverse(list)。Java 并发编程1.并行跟并发有什么区别？并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。你是如何理解线程安全的？如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。可以从三个要素来确保线程安全：1 、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态2 、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行2.说说进程和线程的区别？进
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤：线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中3. 说说线程有几种创建方式？分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象。还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。调用 start 方法时会执行 run 方法，那怎么不直接调用 run 方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。说说 wait 方法和 notify 方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到：线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法、当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。如果调用的是 notifyAll 方法，会唤醒所
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有在这个共享变量上调用 wait 系列方法而被挂起的线程。说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。6.线程有几种状态？6 种。new 代表线程被创建但未启动；runnable 代表线程处于就绪或正在运行状态，由操作系统调度；blocked 代表线程被阻塞，等待获取锁；waiting 代表线程等待其他线程的通知或中断；timed_waiting 代表线程会等待一段时间，超时后自动恢复；terminated 代表线程执行完毕，生命周期结束。什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当
2025-08-11 10:55:45.107 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程使用完时间片后，就会让出 CPU 让其他线程占用。守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM内部，同时还启动了很多守护线程，比如垃圾回收线程。守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。sleep() 方法专属于 Thread 类。wait() 方法专属于 Object 类。waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。而当线程执行 wait 方法时，它会释放持有的对象锁，
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 因此其他线程也有机会获取该对象的锁。有个 int 的变量为 0，十个线程轮流对其进行++操作（循环 10000 次），结果大于 10 万还是小于等于 10 万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。读取变量的值。将读取到的值加 1。将结果写回变量。这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。详细解释下。多个线程在并发执行 ++ 操作时，可能出现以下竞态条件：线程 1 读取变量值为 0。线程 2 也读取变量值为 0。线程 1 进行加法运算并将结果 1 写回变量。线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。.ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程隔离。在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry数组，key 是 Thread
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Local 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value设置为 null，这样可以在很大程度上避免内存泄漏。ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。3、Map 的大小由 ThreadLocal 对象的多少决定。15.ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove()
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法释放内存空间。那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。ThreadLocalMap 的源码看过吗？有研究过。ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位：如果 i 位置被占用，尝试 i+1。如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。如果到达数组末尾，则回到数组头部，继续寻找空位。为什么要用线性探测法而不是 HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。拉链法还需要单独维护一个链表，甚至红黑树，
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不适合 ThreadLocal 这种场景ThreadLocalMap 扩容机制了解吗？了解。与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。父线程能用 ThreadLocal 给子线程传值吗？不能。因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。可以使用 InheritableThreadLocal 来解决这个问题。InheritableThreadLocal 的原理了解吗？了解。在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap：普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 newThread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有inheritableThreadLocals，
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 如果有，就会拷贝 InheritableThreadLocal 变量到子线程：说一下你对 Java 内存模型的理解？Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突volatile 了解吗？了解。第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为：StoreStore 屏障可以禁止普通写操作与 volatile 写操作的重排StoreLoad 屏障会禁止 volatile 写与 volatile 读重排LoadLoad 屏障会禁止 volatile 读与后续普通读操作重排LoadStore 屏障会禁止 volatile 读与后续普通写操作重排volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。锁synchronized 用过吗？用过，频率还
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 很高。synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象this。synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步：前者表示线程正在尝试获取 lock 对象的 Monitor；后者表示线程执行完了同步代码块，正在释放锁。你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——MarkWord，用于存储锁的状态，以及 Monito
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: r 对象的指针。synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。synchronized 怎么保证可见性？通过两步操作：加锁时，线程必须从主内存读取最新数据。释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 标记为未持有状态，并清除线程 ID 信息。synchronized 锁升级了解吗？JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。了解 synchronized 四种锁状态吗？了解。①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。未持有锁的线程通过 CAS 自旋等待锁释放④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，轻量级锁就会升级为重量级锁。此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。随着 JDK 版本的更新，synchronized 的性能得到了极大的优化：①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销详细解释一下：①、从无锁到偏向锁：当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。②、偏向锁的轻量级锁：进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。③、轻量级锁到重量级锁：轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。30.synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。synchronized 可以自动加锁和解锁，ReentrantLoc
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: k 需要手动 lock() 和 unlock()。并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为：ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制AQS 了解多少？AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为ReentrantLock 等类提供底层支持。AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。内部通过一个计数器 state 来跟踪锁的状态和
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。非公平锁和公平锁有什么不同？两句话回答：公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序CAS 了解多少？CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。在 CAS 中，有三个值：V：要更新的变量(var)E：预期值(expected)N：新值(new)先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 当前线程就放弃更新。这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe类实现的。怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS会误认为这个值没有发生变化。可以使用版本号/时间戳的方式来解决 ABA 问题。比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新Java 有哪些保证原子性的方法？比如说以 Atomic 开头的原子类，synchroni
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: zed 关键字，ReentrantLock 锁等原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有AtomicInteger、AtomicLong、AtomicReference 等。线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁R2；线程 2 持有锁 R2，等待锁 R1。第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请聊聊线程同步和互斥？
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: （补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程A 先执行，线程 B 再执行。互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费CPU 资源。聊聊悲观锁和乐观锁？（补充）悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。乐观锁认为冲突不
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。CountDownLatch 了解吗？CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。场景题：假如要查 10万多条数据，用线程池分成 20 个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。第三步，主线程调用 await() 方法，等待所有线程执行完毕。CyclicBarrier 和 CountDownLatch 有什么
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。说一下 JDK 8 中 ConcurrentHashMap 的实现原理？JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS的方式初始化，以确保只有一个线程在初始化数组。第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为synchronized 代码块来插入节点。插入的过程中会判断桶的哈希是否小于 0（f.hash >= 0），小于 0 说明是红黑树，大于等于 0 说明是链表。这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash值固定为 -2。第三步，如果链表长度超过 8，转换为红黑树。第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了ReentrantL
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ock，这样可以保证每个 Segment 都可以独立地加锁。而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized关键字。而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。比如说 put 的时候优先使用 CAS 尝试
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 插入，如果失败再使用 synchronized 代码块加锁。get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。说一下线程池的工作流程？可以简单总结为：任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。第一步，线程池通过 submit() 提交任务。第二步，线程池会先创建核心线程来执行任务。第三步，如果核心线程都在忙，任务会被放入任务队列中。第四步，如果任务
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。另外一版回答。第一步，创建线程池。第二步，调用线程池的 execute()方法，准备执行任务。如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务；如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列；如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务；如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。①、corePoolSize：核心线程数，长期存活，执行任务的主力。②、maxim
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: umPoolSize：线程池允许的最大线程数。③、workQueue：任务队列，存储等待执行的任务。④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。线程池的拒绝策略有哪些？AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute方法的线程。DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedB
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: lockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列SynchronousQueue。1 、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。2 、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是Integer.MAX_VALUE，几乎相当于一个无界队列。3 、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务4 、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。5 、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。submit 有返回值，适用于需要获取结果或
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 处理异常的场景。线程池怎么关闭知道吗？可以调用线程池的 shutdown 或 shutdownNow方法来关闭线程池。shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？1 、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。2 、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。有哪几种常见的线程池？主要有四种：固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等缓存线程池 Executors.newCach
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: edThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。能说一下四种常见线程池的原理吗？说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 & 周期性任务执行，使用DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate()方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 延迟执行。缺点是，如果任务执行时间 > 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。状态由 RUNNING→ SHUTDOWN→ STOP → TIDYING → TERMINATED 依次流转。RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，比如说setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点：第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中JVMJVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件；程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高说说 JVM 的组织架构（补充）JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器能说一下 JVM 的内存区域吗？按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 C/C++编写。在本地方法栈中，主要存放了 native
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。详细说明一下：比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。介绍一下方法区？方法区并不真实存在，属于 J
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 虚拟机规范中的一个逻辑概念，用于存储已被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的GC 分代年龄等信息。最后，JVM 会执行构造方法 <init> 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。new 对象时，堆会发生抢占吗？new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存，两个线程就发生了抢占JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。能说一下对象的内存布局吗？对象在内存中包括三部分：对象头、实例数据和对齐填充说说对象头的作用？对象头是对象存储在内存中的元信息，包含了 Mark Word、类型指针等信息。对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。比如说，如果对象头 + 实例数据 = 14 字节，则需要填充 2 个字节，使总长度变为 16 字节为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 就会降低。new Object() 对象的内存大小是多少？一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16字节；如果开启了压缩指针，就是 12 字节。实例数据的大小取决于对象的成员变量和它们的类型。对于 new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。对象的引用大小了解吗？在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。、在直接指针访问中，引用直接存储对象的内存地址；对象的实
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例数据和类型信息都存储在堆中固定的内存区域。说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。对
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。STW 了解吗？了解。JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为 Stop TheWorld。简称 STW。如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：JVM 发出暂停信号；线程执行到安全点后，挂起自身并等待垃圾收集完成；垃圾回收器完成 GC 操作；线程恢复执行对象一定分配在堆中吗？不一定。默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程逃逸分析会带来什么好处？主要有三个。第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 无法被回收而导致的内存泄漏可能由哪些原因导致呢？静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。数据库、IO、Socket 等连接资源没有及时关闭；ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。有没有处理过内存泄漏问题？当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出StackOverflowError。讲讲 JVM 的垃圾回收机制（补充）垃圾回收就
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩/整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。这也是 G1、CMS 等主流垃圾收集器使用的主要算法。什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失Java 中可作为 GC Roots 的引用有哪几种？所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种虚拟机栈中的引用（方法的参数、局部变量等）本地方法栈中 JNI 的引用类静态变量运行时常量池中的常量（String 或 Class 类型）finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。筛选的条件是对象是否有必要执行 finalize()方法。如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法说说标记-清除算法？标记-清除算法分为两个阶段：标记：标记所有需要回收的对
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象清除：回收所有被标记的对象优点是实现简单，缺点是回收过程中会产生内存碎片说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。缺点是浪费了一半的内存空间。说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。缺点是移动对象的成本比较高。说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 集。年轻代包含 Eden 区以及两个 Survivor 区Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生FULL gc 怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 < 新生代历次Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一次Full GC。知道哪些垃圾收集器？JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。说说 CMS 收集器？CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC能详细说一下 CMS 的垃圾收集过程吗？CMS 使用标记-清除算法进行垃圾收集，分 4 大步：初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。并发清除：清除未被标记的对象，回收它们占用的内存空间。三色标记法的工作流程：①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。②、并发标记（Concurrent Marking）：在此
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。说说 G1 收集器？G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。G1 收集器的运行过程大致可划分为这几个步骤：①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区域和老年代区域。选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。3 、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1会尽可能地在这个时间内完成垃圾回收。CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 序使用的，并释放这些内存以便重新使用。这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。G1 非常适合大内存、多核处理器的环境。工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间JVM 调优37. 用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。你一般都怎么用 jmap？我一般会使用 jmap -heap <pid> 查看堆内存摘要，包括新生代、老年代、元空间等。
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 或者使用 jmap -histo <pid> 查看对象分布了解哪些可视化的性能监控工具？JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？-Xms：初始堆大小-Xmx：最大堆大小-XX:NewSize=n：设置年轻代大小-XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 1/4-XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n=3表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 1/5做过 JVM 调优吗？JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。在项目运行期间，我会使用 JVisualVM
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等CPU 占用过高怎么排查？首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。接着，使用 jstack 命令查看对应进程的线程堆栈信息。然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程ID接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收排查的方法主要分为以下几步：第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。或者使用 jmap 
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。第二步，通过 jmap 命令 dump 出堆内存信息第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过-XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。调整为 6 的话，会减少
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象该怎么排查 Full GC 频繁问题？通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过-XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。或者将大对象拆分成小对象，减少大对象的创建。比如说分页。假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说ThreadLocal、数据库连接、IO 资源等。了解类的加载机制吗？（补充）JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。类加载过程：包括加载、验证、准备、解析和初始化等步骤。双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。以下是整理自网络的一些 JVM 调优实例：网站流量浏览量暴增后，网站反应页面响很慢问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。定位：为了确认推测的正确性，在线上通过 jstat -gc 指令 看到 JVM 进行 GC 次数频率非常高，GC 所占用的时间非常长，所以基本推断就是因为 GC 频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。解决
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁 GC，所以这里问题在于新生代内存太小，所以这里可以增加 JVM 内存就行了，所以初步从原来的 2G 内存增加到 16G 内存。第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次 GC 的时间变长从而导致间接性的卡顿。定位：还是通过 jstat -gc 指令 查看到 的确 FGC 次数并不是很高，但是花费在 FGC 上的时间是非常高的,根据 GC 日志 查看到单次 FGC 的时间有达到几十秒的。解决方案： 因为 JVM 默认使用的是 PS+PO 的组合，PS+PO 垃圾标记和收集阶段都是 STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次 GC 时间过长，所以需要更换并发类的收集器，因为当前的 JDK 版本为 1.7，所以最后选择 CMS 垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。公司的后台系统
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，偶发性的引发 OOM 异常，堆内存溢出。因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从 4G 调整到 8G。但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError 参数 获得堆内存的 dump 文件。VisualVM 对 堆 dump 文件进行分析，通过 VisualVM 查看到占用内存最大的对象是 String 对象，本来想跟踪着 String 对象找到其引用的地方，但 dump 文件太大，跟踪进去的时候总是卡死，而 String 对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成 excel，这个过程会产生大量的 String 对象为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和 EXCEL 对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出jvm.gc.time：每分钟的 GC 耗时在 1s 以内，500ms 以内尤佳jvm.gc.meantime：每次 YGC 耗时在 100ms 以内，50ms 以内尤佳jvm.fullgc.count：FGC 最多几小时 1次，1天不到 1次尤佳jvm.fullgc.time：每次 FGC 耗时在 1s 以内，500ms 以内尤佳// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pidSpringSpring 是什么？Spring 是一个 Java 后端开发框架，其最核心的作用是帮我们管理 Java 对象其最重要的特性就是 IoC，也就是控制反转。以前我们要使用一个对象时，都要自己先 new 出来。但有了 Spring 之后，我们只需要告诉 Spr
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ing 我们需要什么对象，它就会自动帮我们创建好并注入到 Spring 容器当中。比如我在一个Service 类里需要用到 Dao 对象，只需要加个 @Autowired 注解，Spring 就会自动把 Dao 对象注入到 Spring 容器当中，这样就不需要我们手动去管理这些对象之间的依赖关系了。另外，Spring 还提供了 AOP，也就是面向切面编程，在我们需要做一些通用功能的时候特别有用，比如说日志记录、权限校验、事务管理这些，我们不用在每个方法里都写重复的代码，直接用 AOP 就能统一处理。Spring 的生态也特别丰富，像 Spring Boot 能让我们快速搭建项目，Spring MVC能帮我们处理 web 请求，Spring Data 能帮我们简化数据库操作，Spring Cloud能帮我们做微服务架构等等Spring 有哪些特性？首先最核心的就是 IoC 控制反转和 DI 依赖注入。这个我前面也提到了，就是Spring 能帮我们管理对象的创建和依赖关系。第二个就是 AOP 面向切面编程。这个在我们处理一些横切关注点的时候特别有用，比如说我们要给某些 Controller 方法都加上权限控制，如
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果没有 AOP 的话，每个方法都要写一遍加权代码，维护起来很麻烦。用了 AOP 之后，我们只需要写一个切面类，定义好切点和通知，就能统一处理了。事务管理也是同样的道理，加个 @Transactional 注解就搞定了。简单说一下什么是 AOP 和 IoC？AOP 面向切面编程，简单点说就是把一些通用的功能从业务代码里抽取出来，统一处理。比如说技术派中的 @MdcDot 注解的作用是配合 AOP 在日志中加入 MDC信息，方便进行日志追踪。IoC 控制反转是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。Spring 有哪些模块呢？首先是 Spring Core 模块，这是整个 Spring 框架的基础，包含了 IoC 容器和依赖注入等核心功能。还有 Spring Beans 模块，负责 Bean 的配置和管理。这两个模块基本上是其他所有模块的基础，不管用 Spring 的哪个功能都会用到。然后是 Spring Context 上下文模块，它在 Core 的基础上提供了更多企业级的功能，比如国际化、事件传播、资源加载这些。ApplicationContext 就是在这
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个模块里面的。Spring AOP 模块提供了面向切面编程的支持，我们用的@Transactional、自定义切面这些都是基于这个模块。Web 开发方面，Spring Web 模块提供了基础的 Web 功能，Spring WebMVC 就是我们常用的 MVC 框架，用来处理 HTTP 请求和响应。现在还有 Spring WebFlux，支持响应式编程。还有一些其他的模块，比如 Spring Security 负责安全认证，Spring Batch 处理批处理任务等等。现在我们基本都是用 Spring Boot 来开发，它把这些模块都整合好了，用起来更方便。Spring 有哪些常用注解呢？Spring 的注解挺多的，我按照不同的功能分类来说一下平时用得最多的那些。首先是 Bean 管理相关的注解。@Component 是最基础的，用来标识一个类是Spring 组件。像 @Service、@Repository、@Controller 这些都是 @Component的特化版本，分别用在服务层、数据访问层和控制器层。依赖注入方面，@Autowired 是用得最多的，可以标注在字段、setter 方法或者构造方法上。
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: @Qualifier 在有多个同类型 Bean 的时候用来指定具体注入哪一个。@Resource 和 @Autowired 功能差不多，不过它是按名称注入的。配置相关的注解也很常用。@Configuration 标识配置类，@Bean 用来定义 Bean，@Value 用来注入配置文件中的属性值。我们项目里的数据库连接信息、Redis 配置这些都是用 @Value 来注入的。@PropertySource 用来指定配置文件的位置。@RequestMapping 及其变体@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 用来映射 HTTP 请求。@PathVariable 获取路径参数，@RequestParam 获取请求参数，@RequestBody 接收 JSON 数据。、AOP 相关的注解，@Aspect 定义切面，@Pointcut 定义切点，@Before、@After、@Around 这些定义通知类型不过我们用得最多的还是@Transactional，基本上 Service 层需要保证事务原子性的方法都会加上这个注解。Spring 用了哪些设计模
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 式？Spring 框架里面确实用了很多设计模式，我从平时工作中能观察到的几个来说说。首先是工厂模式，这个在 Spring 里用得非常多。BeanFactory 就是一个典型的工厂，它负责创建和管理所有的 Bean 对象。我们平时用的 ApplicationContext其实也是 BeanFactory 的一个实现。当我们通过 @Autowired 获取一个 Bean的时候，底层就是通过工厂模式来创建和获取对象的。单例模式也是 Spring 的默认行为。默认情况下，Spring 容器中的 Bean 都是单例的，整个应用中只会有一个实例。这样可以节省内存，提高性能。当然我们也可以通过 @Scope 注解来改变 Bean 的作用域，比如设置为 prototype 就是每次获取都创建新实例。代理模式在 AOP 中用得特别多。Spring AOP 的底层实现就是基于动态代理的，对于实现了接口的类用 JDK 动态代理，没有实现接口的类用 CGLIB 代理。比如我们用 @Transactional 注解的时候，Spring 会为我们的类创建一个代理对象，在方法执行前后添加事务处理逻辑。Spring 如何实现单例模式？传统的
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 单例模式是在类的内部控制只能创建一个实例，比如用 private 构造方法加 static getInstance() 这种方式。但是 Spring 的单例是容器级别的，同一个 Bean 在整个 Spring 容器中只会有一个实例。具体的实现机制是这样的：Spring 在启动的时候会把所有的 Bean 定义信息加载进来，然后在 DefaultSingletonBeanRegistry 这个类里面维护了一个叫singletonObjects 的 ConcurrentHashMap，这个 Map 就是用来存储单例 Bean的。key 是 Bean 的名称，value 就是 Bean 的实例对象。当我们第一次获取某个 Bean 的时候，Spring 会先检查 singletonObjects 这个 Map 里面有没有这个 Bean，如果没有就会创建一个新的实例，然后放到 Map 里面。后面再获取同一个 Bean 的时候，直接从 Map 里面取就行了，这样就保证了单例。还有一个细节就是 Spring 为了解决循环依赖的问题，还用了三级缓存。除了singletonObjects 这个一级缓存，还有 earlySingl
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: etonObjects 二级缓存和singletonFactories 三级缓存。这样即使有循环依赖，Spring 也能正确处理Spring 容器和 Web 容器之间的区别知道吗？（补充）首先从概念上来说，Spring 容器是一个 IoC 容器，主要负责管理 Java 对象的生命周期和依赖关系。而 Web 容器，比如 Tomcat、Jetty 这些，是用来运行 Web应用的容器，负责处理 HTTP 请求和响应，管理 Servlet 的生命周期。从功能上看，Spring 容器专注于业务逻辑层面的对象管理，比如我们的 Service、Dao、Controller 这些 Bean 都是由 Spring 容器来创建和管理的。而 Web 容器主要处理网络通信，比如接收 HTTP 请求、解析请求参数、调用相应的 Servlet，然后把响应返回给客户端在实际项目中，这两个容器是相辅相成的。我们的 Web 项目部署在 Tomcat 上的时候，Tomcat 会负责接收 HTTP 请求，然后把请求交给 DispatcherServlet处理，而 DispatcherServlet 又会去 Spring 容器中查找相应的 Cont
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: roller来处理业务逻辑。现在我们都用 Spring Boot 了，Spring Boot 内置了 Tomcat，把 Web 容器和Spring 容器都整合在一起了，我们只需要运行一个 jar 包就可以了说一说什么是 IoC？IoC 的全称是 Inversion of Control，也就是控制反转。这里的“控制”指的是对象创建和依赖关系管理的控制权。以前我们写代码的时候，如果 A 类需要用到 B 类，我们就在 A 类里面直接 new一个 B 对象出来，这样 A 类就控制了 B 类对象的创建。有了 IoC 之后，这个控制权就“反转”了，不再由 A 类来控制 B 对象的创建，而是交给外部的容器来管理。DI 和 IoC 的区别了解吗？IoC 的思想是把对象创建和依赖关系的控制权由业务代码转移给 Spring 容器。这是一个比较抽象的概念，告诉我们应该怎么去设计系统架构。而 DI，也就是依赖注入，它是实现 IoC 这种思想的具体技术手段。在 Spring 里，我们用 @Autowired 注解就是在使用 DI 的字段注入方式。为什么要使用 IoC 呢？在日常开发中，如果我们需要实现某一个功能，可能至少需要两个以上
2025-08-11 10:55:45.108 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系能说一下 IoC 的实现机制吗？好的，Spring IoC 的实现机制还是比较复杂的，我尽量用比较通俗的方式来解释一下整个流程。第一步是加载 Bean 的定义信息。Spring 会扫描我们配置的包路径，找到所有标注了 @Component、@Service、@Repository 这些注解的类，然后把这些类的元信息封装成 BeanDefinition 对象。第二步是 Bean 工厂的准备。Spring 会创建一个 DefaultListableBeanFactory作为 Bean 工厂来负责 Bean 的创建和管理。第三步是 Bean 的实例化和初始化。这个过程比较复杂，Spring 会根据BeanDefinition 来创建 Bean 实例。对于单例 Bean，Spring 会先检查缓存中是否已经存在，如果不存在就创建新实例。创建实例的时候会通过反射调用构造方法，然后进行属性注入，最后执行初始化回调方法。依赖注入的实现主
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 要是通过反射来完成的。比如我们用 @Autowired 标注了一个字段，Spring 在创建 Bean 的时候会扫描这个字段，然后从容器中找到对应类型的 Bean，通过反射的方式设置到这个字段上。你是怎么理解 Spring IoC 的？IoC 本质上一个超级工厂，这个工厂的产品就是各种 Bean 对象。我们通过 @Component、@Service 这些注解告诉工厂：“我要生产什么样的产品，这个产品有什么特性，需要什么原材料”。然后工厂里各种生产线，在 Spring 中就是各种 BeanPostProcessor。比如AutowiredAnnotationBeanPostProcessor 专门负责处理 @Autowired 注解。工厂里还有各种缓存机制用来存放产品，比如说 singletonObjects 是成品仓库，存放完工的单例 Bean；earlySingletonObjects 是半成品仓库，用来解决循环依赖问题。说说 BeanFactory 和 ApplicantContext 的区别?BeanFactory 算是 Spring 的“心脏”，而 ApplicantContext 可以说是Spri
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ng 的完整“身躯”。BeanFactory 提供了最基本的 IoC 能力。它就像是一个 Bean 工厂，负责 Bean的创建和管理。他采用的是懒加载的方式，也就是说只有当我们真正去获取某个Bean 的时候，它才会去创建这个 Bean。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上扩展了很多企业级的功能。它不仅包含了 BeanFactory 的所有功能，还提供了国际化支持、事件发布机制、AOP、JDBC、ORM 框架集成等等。ApplicationContext 采用的是饿加载的方式，容器启动的时候就会把所有的单例 Bean 都创建好，虽然这样会导致启动时间长一点，但运行时性能更好。另外一个重要的区别是生命周期管理。ApplicationContext 会自动调用 Bean的初始化和销毁方法，而 BeanFactory 需要我们手动管理。在 Spring Boot 项目中，我们可以通过 @Autowired 注入 ApplicationContext，或者通过实现 ApplicationContextAware 接口来获取 Applicatio
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: nContext。项目启动时 Spring 的 IoC 会做什么？第一件事是扫描和注册 Bean。IoC 容器会根据我们的配置，比如@ComponentScan 指定的包路径，去扫描所有标注了 @Component、@Service、@Controller 这些注解的类。然后把这些类的元信息包装成 BeanDefinition 对象，注册到容器的 BeanDefinitionRegistry 中。这个阶段只是收集信息，还没有真正创建对象。第二件事是 Bean 的实例化和注入。这是最核心的过程，IoC 容器会按照依赖关系的顺序开始创建 Bean 实例。对于单例 Bean，容器会通过反射调用构造方法创建实例，然后进行属性注入，最后执行初始化回调方法在依赖注入时，容器会根据 @Autowired、@Resource 这些注解，把相应的依赖对象注入到目标 Bean 中。比如 UserService 需要 UserDao，容器就会把UserDao 的实例注入到 UserService 中。说说 Spring 的 Bean 实例化方式？Spring 提供了 4 种方式来实例化 Bean，以满足不同场景下的需求第一种是通过
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法实例化，这是最常用的方式。当我们用 @Component、@Service 这些注解标注类的时候，Spring 默认通过无参构造器来创建实例的。如果类只有一个有参构造方法，Spring 会自动进行构造方法注入。第二种是通过静态工厂方法实例化。有时候对象的创建比较复杂，我们会写一个静态工厂方法来创建，然后用 @Bean 注解来标注这个方法。Spring 会调用这个静态方法来获取 Bean 实例。第三种是通过实例工厂方法实例化。这种方式是先创建工厂对象，然后通过工厂对象的方法来创建 Bean：第四种是通过 FactoryBean 接口实例化。这是 Spring 提供的一个特殊接口，当我们需要创建复杂对象的时候特别有用：你是怎么理解 Bean 的？在我看来，Bean 本质上就是由 Spring 容器管理的 Java 对象，但它和普通的Java 对象有很大区别。普通的 Java 对象我们是通过 new 关键字创建的。而Bean 是交给 Spring 容器来管理的，从创建到销毁都由容器负责。从实际使用的角度来说，我们项目里的 Service、Dao、Controller 这些都是Bean。比如 UserServ
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ice 被标注了 @Service 注解，它就成了一个 Bean，Spring会自动创建它的实例，管理它的依赖关系，当其他地方需要用到 UserService 的时候，Spring 就会把这个实例注入进去。这种依赖注入的方式让对象之间的关系变得松耦合。Spring 提供了多种 Bean 的配置方式，基于注解的方式是最常用的。@Component 和 @Bean 有什么区别？首先从使用上来说，@Component 是标注在类上的，而 @Bean 是标注在方法上的。@Component 告诉 Spring 这个类是一个组件，请把它注册为 Bean，而 @Bean 则告诉 Spring 请将这个方法返回的对象注册为 Bean。从控制权的角度来说，@Component 是由 Spring 自动创建和管理的。而 @Bean 则是由我们手动创建的，然后再交给 Spring 管理，我们对对象的创建过程有完全的控制权。能说一下 Bean 的生命周期吗？Bean 的生命周期可以分为 5 个主要阶段，我按照实际的执行顺序来说一下。第一个阶段是实例化。Spring 容器会根据 BeanDefinition，通过反射调用 Bean的
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法创建对象实例。如果有多个构造方法，Spring 会根据依赖注入的规则选择合适的构造方法。第二阶段是属性赋值。这个阶段 Spring 会给 Bean 的属性赋值，包括通过@Autowired、@Resource 这些注解注入的依赖对象，以及通过 @Value 注入的配置值第三阶段是初始化。这个阶段会依次执行：@PostConstruct 标注的方法InitializingBean 接口的 afterPropertiesSet 方法通过 @Bean 的 initMethod 指定的初始化方法初始化后，Spring 还会调用所有注册的 BeanPostProcessor 后置处理方法。这个阶段经常用来创建代理对象，比如 AOP 代理。第五阶段是使用 Bean。比如我们的 Controller 调用 Service，Service 调用DAO。最后是销毁阶段。当容器关闭或者 Bean 被移除的时候，会依次执行：@PreDestroy 标注的方法DisposableBean 接口的 destroy 方法通过 @Bean 的 destroyMethod 指定的销毁方法Aware 类型的接口有什么作用？Aware 
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 接口在 Spring 中是一个很有意思的设计，它们的作用是让 Bean 能够感知到 Spring 容器的一些内部组件。从设计理念来说，Aware 接口实现了一种“回调”机制。正常情况下，Bean 不应该直接依赖 Spring 容器，这样可以保持代码的独立性。但有些时候，Bean 确实需要获取容器的一些信息或者组件，Aware 接口就提供了这样一个能力。什么是自动装配？自动装配的本质就是让 Spring 容器自动帮我们完成 Bean 之间的依赖关系注入，而不需要我们手动去指定每个依赖。简单来说，就是“我们不用告诉 Spring具体怎么注入，Spring 自己会想办法找到合适的 Bean 注入进来”。自动装配的工作原理简单来说就是，Spring 容器在启动时自动扫描@ComponentScan 指定包路径下的所有类，然后根据类上的注解，比如@Autowired、@Resource 等，来判断哪些 Bean 需要被自动装配。之后分析每个 Bean 的依赖关系，在创建 Bean 的时候，根据装配规则自动找到合适的依赖 Bean，最后根据反射将这些依赖注入到目标 Bean 中Bean 的作用域有哪些Bean 的作用域决
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 定了 Bean 实例的生命周期和创建策略，singleton 是默认的作用域。整个 Spring 容器中只会有一个 Bean 实例。不管在多少个地方注入这个 Bean，拿到的都是同一个对象。生命周期和 Spring 容器相同，容器启动时创建，容器销毁时销毁。实际开发中，像 Service、Dao 这些业务组件基本都是单例的，因为单例既能节省内存，又能提高性能。当把 scope 设置为 prototype 时，每次从容器中获取 Bean 的时候都会创建一个新的实例。当需要处理一些有状态的 Bean 时会用到 prototype，比如每个订单处理器需要维护不同的状态信息如果作用于是 request，表示在 Web 应用中，每个 HTTP 请求都会创建一个新的 Bean 实例，请求结束后 Bean 就被销毁。如果作用于是 session，表示在 Web 应用中，每个 HTTP 会话都会创建一个新的 Bean 实例，会话结束后 Bean 被销毁。application 作用域表示在整个应用中只有一个 Bean 实例，类似于 singleton，但它的生命周期与 ServletContext 绑定。Spring 中的单
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例 Bean 会存在线程安全问题吗？首先要明确一点。Spring 容器本身保证了 Bean 创建过程的线程安全，也就是说不会出现多个线程同时创建同一个单例 Bean 的情况。但是 Bean 创建完成后的使用过程，Spring 就不管了换句话说，单例 Bean 在被创建后，如果它的内部状态是可变的，那么在多线程环境下就可能会出现线程安全问题单例 Bean 的线程安全问题怎么解决呢？第一种，使用局部变量，也就是使用无状态的单例 Bean，把所有状态都通过方法参数传递：第二种，当确实需要维护线程相关的状态时，可以使用 ThreadLocal 来保存状态。ThreadLocal 可以保证每个线程都有自己的变量副本，互不干扰。第三种，如果需要缓存数据或者计数，使用 JUC 包下的线程安全类，比如说AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。第四种，对于复杂的状态操作，可以使用 synchronized 或 Lock：第五种，如果 Bean 确实需要维护状态，可以考虑将其改为 prototype 作用域，这样每次注入都会创建一个新的实例，避免了多线程共享同
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个实例的问题。说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖：一级缓存：存放完全初始化好的单例 Bean。二级缓存：存放正在创建但未完全初始化的 Bean 实例。三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。三级缓存解决循环依赖的过程是什么样的？实例化 Bean 时，将其早期引用放入三级缓存。其他依赖该 Bean 的对象，可以从缓存中获取其引用。初始化完成后，将 Bean 移入一级缓存。假如 A、B 两个类发生循环依赖：A 实例的初始化过程：1 、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道2 、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。3 、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。4 、发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 已经实例化并且初始化完成了，把 B 放入⼀级缓存5 、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存6 、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor在 Bean 的初始化阶段，会通过 
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Bean 后置处理器来进行一些前置和后置的处理。实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是AutowiredAnnotationBeanPostProcessor。Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。AOP 有哪些核心概念？切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法切点（Pointcut）：对连接点进行拦截的定位通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强目标对象 （Target）：代理的目标对象引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。在 Spring 容器初始化 Bean的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 理对象中织入切面逻辑。这一过程发生在Spring 容器的后处理器（BeanPostProcessor）阶段。简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像@Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现AOP。③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。AOP 的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。第一步，自定义注解作为切点第二
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间第三步，在使用的地方加上自定义注解第四步，当接口被调用时，就可以看到对应的执行日志。说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。说说 Spring AOP 和 AspectJ AOP 区别?说
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。反射：运行时操作类的元信息的底层能力动态代理：基于反射实现方法拦截的设计模式Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚.我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部实现事务的控制。介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。说说两者的区别？编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务说说 Spring 的事务隔离级别？好，事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: TransactionDefinition 接口中。①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 ），MySQL默认的是可重复读，Oracle 默认的读已提交。②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 下文如何在方法调用链中传播。Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。事务传播机制是使用ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。第一步，在 Bean 初始化阶段创建代理对象：Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性切面，也就是@Transactional 注解及其属性值。然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。第二步，在执行目标方法时进行事务增强操作：当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为 MethodInterceptor，事务增强对该接口的实现为 TransactionInterceptor，@Transactional 应用在非 public 修饰的方法上如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。Spring MVC 的核心组件？DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping将不同
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的请求映射到不同的 Handler。HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。Spring MVC 的
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。然后 DispatcherServlet 通过 ViewResolver 解析视图。最后，DispatcherServlet 渲染视图并将响应返回给客户端①、发起请求：客户端通过 HTTP 协议向服务器发起请求。②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个Controller 应该处理这个请求。通常会根据请求的 URL 来确定。④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用HandlerAdapter 来调用 Controller 的处理方法。⑤、执行处理器：Controller 处理请求，处理完后
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用ViewResolver 来解析视图名称，找到具体的视图页面。⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。⑧、响结果：DispatcherServlet 将视图结果返回给客户端。Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller 注解的类，还可以是实现了特定接口的其他类（如
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  HttpRequestHandler 或SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。SpringMVC Restful 风格的接口的流程是什么样的呢？我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同：客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServletDispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个 ControllerController 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理HandlerAdapter 执行完对 Controller 的请求，会调用HandlerMethodReturnValueHandler 去处理返回值，主要的过程：5.1. 调用 RequestResponseBodyMethodProcessor，创建ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入ServletServerHttpResponse 的 OutputStream 输出流中5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化执行完请求后，返回的 ModealAndView 为 null，ServletServerHtt
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: pResponse里也已经写入了响应，所以不用关心 View 的处理介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架Spring Boot 的优点非常多，比如说：Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和Spring MVC。Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控Spring Boot 常用注解有哪些？@SpringBootApplication：Spring Boot 应用的入口，用在启动类上。还有一些 Spring 框架本身的注解，比如 @Component、@RestController、@Service、@ConfigurationProperties、@Transact
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ional 等。SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。SpringBoot 的自动装配机制主要通过 @EnableAutoConfiguration 注解实现，这个注解是 @SpringBootApplication 注解的一部分，后者是一个组合注解，包括 @SpringBootConfiguration、@ComponentScan 和@EnableAutoConfiguration。@EnableAutoConfiguration 注解通过AutoConfigurationImportSelector 类来加载自动装配类，这个类实现了ImportSelector 接口的 selectImports 方法，该方法负责获取所有符
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat和 Jackson 等依赖。自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。为什么使用 Spring Boot？Spring Boot 解决了传统 Spring 开发的三大痛点：简化配置：自动装配 + 起步依赖，告别 XML 配置地狱快速启动：内嵌 Tomcat/Jetty，一键启动独立运行应用生产就绪：Actua
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: tor 提供健康检查、监控等运维能力@Import 的作用实现模块化配置导入，三种用法导入普通配置类：@Import(MyConfig.class)导入 ImportSelector 实现（自动装配核心）：@Import(AutoConfigurationImportSelector.class)导入 ImportBeanDefinitionRegistrar 实现（动态注册 Bean）Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责：第一步，创建 SpringApplication 实例，负责应用的启动和初始化；第二步，从 application.yml 中加载配置文件和环境变量；第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入；第四步，启动内嵌的 Web 容器。第五步，发布启动完成事件 ApplicationReadyEvent，并调用ApplicationRunner 的 run 方法完成启动后的逻辑。了解@SpringBootApplication 注解吗？@SpringBootApplic
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ation 是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解：@SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。@EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。@ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication 注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部的一系列处理流程Spring Boot 利用 Java 反射机制来读取传递给 run 方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication@SpringBootApplication 是一个组合注解，它里面的@ComponentScan 注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。如果需要自定义包扫描路径，可以在@SpringBootApplication 注解上添加@ComponentScan 注解，指定要扫描的包路径。这种方式会覆盖默认的包扫描路径，只扫描 com.github.paicoding.forum 包及其子包下的所有组件。SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种Model-View-Controller（模型-视图-控制器）
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的开发模式。Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个SpringMVC 的 Web 项目。Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。微服务架构主要要解决哪些问题？服务很多，客户端怎么访问，如何提供对外网关?这么多服务，服务之间如何通信? HTTP 还是 RPC?这
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 么多服务，如何治理? 服务的注册和发现。服务挂了怎么办？熔断机制SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml：用 SpringTask资源占用太高，有什么其他的方式解决？（补充）第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。第二，使用数据库调度器（如 Quartz）Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。Spring Cache 和 Redis 有什么区别？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。Redis 是一个分布式的缓存中间件，支持多种数
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。说说什么是 MyBatis?Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL语句的功底有一定要求SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中JDBC 编程有哪些不足之处，MyBatis 是如何解决的？1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xm
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: l文件中与 java 代码分离。3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。Hibernate 和 MyBatis 有什么区别？不同点1）映射关系MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂2）SQL 优化和移植性Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。MyBatis 需要
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步：1）创建 SqlSessionFactory2）通过 SqlSessionFactory 创建 SqlSessionSqlSession（会话）可以理解为程序和数据库之间的桥梁3）通过 sqlsession 执行数据库操作，可以通过 SqlSession 实例来直接执行已映射的 SQL 语句：4）调用 session.commit()提交事务5）调用 session.close()关闭会话说说 MyBatis 生命周期？SqlSessionFactoryBuilder一旦创建了 SqlSessionFactory，就不再需要它了。 因此SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。SqlSessionSqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。Mapper映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。在 mapper 中如何传递多个参数？#{}和${}的区别?①、当使用 #{} 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。由于 #{} 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。<select id="selectUser" resultType="User">SELECT * FROM users WHERE id = #{id}</select>②、当使用 ${} 时，参数的值会
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直接替换到 SQL 语句中去，而不会经过预处理。这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or1=1，这样就会导致查询出所有用户的结果。${} 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。<select id="selectUsersByOrder" resultType="User">SELECT * FROM users ORDER BY ${columnName} ASC</select>模糊查询 like 语句该怎么写?CONCAT('%',#{question},'%') 使用 CONCAT()函数，（推荐 ✨）说说 Mybatis 的一级、二级缓存？一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或close 之后，该 SqlSession 中的所有 Ca
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: che 就将清空，MyBatis 默认打开一级缓存。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。能说说 MyBatis 的工作原理吗？按工作原理，可以分为两大步：生成会话工厂、会话运行构造会话工厂也可以分为两步：获取配置获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括：读取配置文件，包括基础配置文件和映射文件初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等提供一个单例，作为会话工厂构建的重要参数它的构建过程也会初始化一些环境变量，比如数据源构建 SqlSessionFactorySqlSessionFactory 只是一
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合：Executor（执行器）Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法StatementHandler（数据库会话器）StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个handler 去执行 query。ParameterHandler （参数处理器）PreparedStatementHandler 里对 sql 进行了预编译处理ResultSetHandler（结果处理器）我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂SqlSessionFactory。创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。参数处理：对输入参数的类型进行处理，并预编译。结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。MyBatis 的功能架构是什么样的？我们一般把 Mybatis 的功能架构分为三层：API 接口层：提供给外部使用的接口 API，开发人员通
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑Mybatis 都有哪些 Executor 执行器？Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String, Statement>内，供下一次使用。简言之，就是重复使
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 用Statement 对象。BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。说说 JDBC 的执行步骤？Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤：在与数据库建立连接之前，首先需要通过 Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了 DriverManager 类中。Class.forName("com.mysql.cj.jdbc.Driver");第二步，建立数据库连接使用 DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数C
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: onnection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/databaseName", "username","password");第三步，创建 Statement 对象通过建立的数据库连接对象 Connection 创建 Statement、PreparedStatement或 CallableStatement 对象，用于执行 SQL 语句Statement stmt = conn.createStatement();第四步，执行 SQL 语句使用 Statement 或 PreparedStatement 对象执行 SQL 语句。执行查询（SELECT）语句时，使用 executeQuery()方法，它返回 ResultSet 对象；执行更新（INSERT、UPDATE、DELETE）语句时，使用 executeUpdate()方法，它返回一个整数表示受影响的行数。ResultSet rs = stmt.executeQuery("SELECT * FROM tableName");int affectedRow
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: s = stmt.executeUpdate("UPDATE tableName SET column ='value' WHERE condition");第五步，处理结果集如果执行的是查询操作，需要处理 ResultSet 对象来获取数据第六步，关闭资源最后，需要依次关闭 ResultSet、Statement 和 Connection 等资源，释放数据库连接等资源创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是 java.sql.Connection 对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。Connection 对象代表了应用程序和数据库的一个连接会话。通过调用 DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。一旦获得 Connection 对象，就可以使用它来创建执行 SQL 语句的 Statement、PreparedStatement 和 CallableStatement 对象，以及管理事务等。什么是 SQL 注入？如
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。为了防止 SQL 注入，可以采取以下措施：①、使用参数化查询使用参数化查询，即使用 PreparedStatement 对象，通过 setXxx 方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。②、限制用户输入对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或SQL 关键字。③、使用 ORM 框架比如，在 MyBatis 中，使用#{}占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。<select id="selectUser" resultType="User">SELECT * FROM users WHERE username = #{userName}</select>分布式说说 CAP 原则？、CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition toleran
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ce（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。假如现在有这样的场景：用户访问了 N1，修改了 D1 的数据。用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。接下来：保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。ASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性（Eventual consistency）的效果。BASE 的主要含义：Basically Available（基本可用）什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。Soft State（软状态）什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。Eventually Consistent（最终一致性）上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL 分布式锁、ZooKepper 分布式锁、Redis分布式锁。MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。加锁的时候，在锁表中增加一条记录
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 即可；释放锁的时候删除记录就行。如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。基于 Redis 的分布式锁核心思想： 利用 Redis 单线程执行命令的特性以及其丰富的数据结构和命令（尤其是 SETNX, SET with NX/PX/EX, Lua 脚本）来实现高性能锁。当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。什么是分布式事务
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性分布式事务有哪些常见的实现方案？二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。本地消息表：在数据库中存储事务事件，通过定时任务处理消息。基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。7.1 说说 2PC 两阶段提交？两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。优点：尽量保证了数据的强一致，实现成本
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 较低，在各大主流数据库都有自己实现，缺点:单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务：如果所有参与者都返回正确的 ACK 响应，则提交事务如果参与者有一个或多个参与者收到错误的 ACK 响应或者超时，则中断事务可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。Confirm：确
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。执行流程：订单服务，添加一条订单和一条消息，在一个事务里提交订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送库存服务，接收 MQ 消息，修改库存表，需要保证幂等
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 操作如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息如果修改失败，可以不做处理，等待重试MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。分布式算法 paxos 了解么 ？Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一在 Paxos 中有这么几个角色：Proposer（提议者） : 提议者提出提案，用于投票表决。Accecptor（接受者） : 对提案进行投票，并接受达成
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 共识的提案。Learner（学习者） : 被告知投票的结果，接受达成共识的提案Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受)Prepare(准备)阶段提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答：两个承诺：承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求承诺不会再接受提案号小于 Mn 的 Accept 请求一个应答：不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。Accept(接受)阶段如果提议者收到来自
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题笔试题1 为什么使用消息队列？消息队列（Message Queue，简称 MQ）是一种跨进程的通信机制，用于上下游传递消息。它在现代分布式系统中扮演着重要的角色，主要用于系统间的解耦、异步消息处理以及流量削峰。消息队列的使用场景解耦在没有消息队列的系统中，如果
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个系统需要与多个系统交互，它们之间的耦合度会非常高。例如，系统 A直接调用系统 B 和 C的接口，如果未来需要接入系统 D或取消 B 系统，系统 A 需要修改代码，这增加了系统的风险。使用消息队列后，系统 A 只需将消息推送到队列，其他系统根据需要从队列中订阅消息。这样，系统 A 不需要做任何修改，也不需要考虑下游消费失败的情况，从而实现了系统间的解耦。异步处理在同步操作中，一些非关键的业务逻辑可能会消耗大量时间，导致用户体验不佳。例如，系统 A 在处理一个请求时，需要在多个系统中进行操作，这可能导致总延迟增加。通过使用消息队列，系统 A 可以将消息写入队列，而其他业务逻辑可以异步执行，从而显著减少总耗时。流量削峰对于面临突发流量的系统，如果直接将所有请求发送到数据库，可能会导致数据库连接异常或系统崩溃。消息队列可以帮助系统按照下游系统的处理能力从队列中慢慢拉取消息，从而避免因突发流量导致的系统崩溃。消息队列的优缺点优点解耦：使得系统间的依赖关系最小化，降低系统间的耦合度。异步处理：提高系统的响应速度和吞吐量。流量削峰：使系统能够应对高流量压力，避免系统因突发流量而崩溃2.简述数据库的事务，说出事务的特点？
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 在数据库管理系统中，事务是一个非常重要的概念，它指的是一系列的数据库操作，这些操作要么全部成功，要么全部失败，确保数据的完整性和一致性。事务的四大特性通常被称为 ACID属性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。原子性（Atomicity）原子性确保事务中的所有操作要么全部成功，要么全部失败回滚，不会出现只执行了部分操作的情况。这意味着事务是一个不可分割的工作单位，例如，在银行转账的场景中，转账操作需要同时更新两个账户的余额，这两个操作必须要么都执行，要么都不执行一致性（Consistency）一致性意味着数据库在事务开始之前和结束之后，都必须保持一致状态。事务不会破坏数据的完整性和业务规则。例如，如果一个转账事务在执行过程中系统崩溃，事务没有提交，那么事务中所做的修改也不会保存到数据库中，保证了数据的一致性隔离性（Isolation）隔离性保证了当多个用户并发访问数据库时，数据库系统能够为每个用户的事务提供一个独立的运行环境，事务之间不会互相干扰。例如，当一个事务正在处理数据时，其他事务必须等待，直到该事务完成，
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 才能访问同样的数据持久性（Durability）持久性确保一旦事务提交，它对数据库的改变就是永久性的。即使发生系统故障，事务的结果也不会丢失。例如，一旦银行转账事务提交，转账的金额就会永久地反映在各个账户的余额中事务的四大特性是数据库管理系统设计的基础，它们确保了数据库操作的安全性和可靠性，使得用户可以信赖数据库处理复杂的业务逻辑。3. SOA 和微服务之间的区别？SOA（面向服务的架构）[&和微服务架构&]是两种常见的软件架构设计方法，它们在服务划分、通信方式和应用场景等方面存在显著差异。SOA 的特点 SOA 是一种高层次的架构设计理念，旨在通过服务接口实现系统间的松耦合和功能复用。服务通过企业服务总线（ESB）进行通信，ESB负责消息路由、协议转换和服务集成。SOA 的服务粒度较粗，适用于复杂的企业级系统，尤其是需要集成异构系统的场景。微服务的特点 微服务架构是对 SOA 的进一步演进，强调将单一业务系统拆分为多个独立的小型服务。每个服务独立开发、部署和运行，通常通过轻量级协议（如 HTTP/REST）进行通信。微服务更注重快速交付和自动化运维，适合快速变化的互联网系统。主要区别服务粒度：SOA 的服务
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 粒度较粗，通常是一个完整的业务模块；微服务的服务粒度较细，专注于单一功能。通信方式：SOA 依赖 ESB 进行服务间通信，支持多种协议；微服务使用轻量级协议（如 HTTP/REST），去掉了 ESB。部署方式：SOA 通常整体部署，微服务则支持独立部署，便于快速迭代。应用场景：SOA 适用于复杂的企业级系统，微服务更适合轻量级、基于 Web 的系统。总结 SOA 和微服务各有优劣，选择哪种架构取决于具体的业务需求和系统复杂性。SOA 更适合需要集成异构系统的大型企业应用，而微服务则适合快速变化的互联网应用。4.深拷贝和浅拷贝的区别？在编程中，深复制和浅复制是两种不同的对象复制方式。它们主要用于处理对象和数组等引用数据类型。浅复制浅复制只复制对象的引用，而不复制对象本身。也就是说，新旧对象共享同一块内存空间，对其中一个对象的修改会影响到另一个对象。浅复制适用于对象的属性是基本数据类型的情况，但如果属性是引用类型，则会出现共享内存的问题。深复制深复制会创建一个新的对象，并递归复制所有层级的属性和数组元素。新对象与原对象不共享内存，修改新对象不会影响到原对象。深复制适用于需要完全独立的对象副本的情况。5.有了关系型
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据库，为什么还需要 NOSQL 数据库？关系型数据库（RDBMS）在数据存储和管理方面表现出色，但在某些场景下存在局限性。随着互联网和大数据时代的到来，传统的关系型数据库在处理大量非结构化和半结构化数据时显得力不从心。为了解决这些问题，非关系型数据库（NoSQL）应运而生。关系型数据库的局限性扩展性：关系型数据库通常采用垂直扩展（Scale-Up）的方式，通过增加硬件资源来提升性能。然而，这种方式在高并发、大数据量的情况下成本高昂且效果有限。灵活性：关系型数据库要求预先定义数据模式（Schema），在需求频繁变化的应用场景中显得不够灵活。每次修改数据模式都需要停机或复杂的迁移操作。性能：在高并发读写、大规模数据处理的情况下，关系型数据库的性能可能无法满足需求，特别是在分布式环境下。成本：关系型数据库通常需要昂贵的硬件和专业的维护团队，对于中小型企业和初创公司来说，成本压力较大。非关系型数据库的优势水平扩展（Scale-Out）：NoSQL 数据库通常设计为支持水平扩展，通过增加更多的服务器节点来提升性能。这种方式在大规模数据和高并发场景下非常有效。灵活的数据模型：NoSQL数据库通常采用无模式（Schema
2025-08-11 10:55:45.109 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -less）或弱模式（Schema-flexible）的设计，允许数据以更灵活的方式存储。高性能：NoSQL 数据库针对特定的应用场景进行了优化，通常具有更高的读写性能。低成本：NoSQL数据库通常采用开源软件，硬件要求较低，适合在云环境中部署，降低了总体拥有成本（TCO）。分布式架构：NoSQL 数据库通常采用分布式架构，能够更好地处理大规模数据和高并发请求。关系型数据库与非关系型数据库的对比数据模型：关系型数据库以表格形式存储数据，而 NoSQL数据库以键值对、文档、列族或图的形式存储数据。扩展性：关系型数据库采用垂直扩展，而 NoSQL数据库采用水平扩展。数据一致性：关系型数据库强调强一致性（ACID），而 NoSQL 数据库通常采用最终一致性（BASE）。数据模式：关系型数据库需要固定模式，而 NoSQL 数据库通常无模式或弱模式。性能：关系型数据库适合事务处理和小规模数据，而 NoSQL数据库适合大规模数据和高并发。成本：关系型数据库成本较高，而 NoSQL数据库成本较低。结论NoSQL数据库的出现并不是为了取代关系型数据库，而是为了填补关系型数据库在某些场景下的不足。NoSQL 数据库提供了更高的
2025-08-11 10:55:45.110 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 扩展性、灵活性和性能，适合处理大规模数据和高并发请求。然而，关系型数据库在事务处理和企业应用中仍然具有不可替代的优势。
2025-08-11 10:55:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:45.467 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.yizhaoqi.smartpai.service.VectorizationService - 开始向量化文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:45.472 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.client.EmbeddingClient - 开始生成向量，文本数量: 476
2025-08-11 10:55:45.473 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:45.473 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:45.670 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] [10d4b7b0-5] Response 400 BAD_REQUEST
2025-08-11 10:55:45.671 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] [10d4b7b0-5] Read 312 bytes
2025-08-11 10:55:46.685 [parallel-4] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:46.686 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:46.894 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] [10d4b7b0-6] Response 400 BAD_REQUEST
2025-08-11 10:55:46.895 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] [10d4b7b0-6] Read 312 bytes
2025-08-11 10:55:47.909 [parallel-5] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:47.912 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:48.057 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] [10d4b7b0-7] Response 400 BAD_REQUEST
2025-08-11 10:55:48.058 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] [10d4b7b0-7] Read 312 bytes
2025-08-11 10:55:49.063 [parallel-6] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:49.063 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:49.243 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] [10d4b7b0-8] Response 400 BAD_REQUEST
2025-08-11 10:55:49.244 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [2f61edda] [10d4b7b0-8] Read 312 bytes
2025-08-11 10:55:49.245 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [2f61edda] Cancel signal (to close connection)
2025-08-11 10:55:49.245 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR com.yizhaoqi.smartpai.client.EmbeddingClient - 调用向量化 API 失败: Retries exhausted: 3/3
reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:49.247 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.yizhaoqi.smartpai.service.VectorizationService - 向量化失败，fileMd5: c8f8cebf90c764b93d862694096a2af9
java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:49.248 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.y.smartpai.consumer.FileProcessingConsumer - Error processing task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:52.281 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR o.s.kafka.listener.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(com.yizhaoqi.smartpai.model.FileProcessingTask)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: java.lang.RuntimeException: Error processing task
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:67)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	... 25 common frames omitted
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:52.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Received task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
2025-08-11 10:55:52.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件权限信息: userId=1, orgTag=PRIVATE_sy, isPublic=true
2025-08-11 10:55:52.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Downloading file from storage: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:52.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Detected remote URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:52.294 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Successfully connected to URL, starting download...
2025-08-11 10:55:52.294 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 开始解析文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文件元数据:
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:unmappedUnicodeCharsPerPage: 0
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:PDFVersion: 1.7
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmp:CreatorTool: WPS 文字
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXFA: false
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:modify_annotations: true
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print_degraded: true
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By-Full-Set: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:creator: SongYu
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:num3DAnnotations: 0
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:52.763 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:format: application/pdf; version=1.7
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator_tool: WPS 文字
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:overallPercentageUnmappedUnicodeChars: 0.0
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:fill_in_form: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:modified: 2025-08-04T09:36:05Z
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasCollection: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:encrypted: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsNonEmbeddedFont: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:custom:SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasMarkedContent: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - Content-Type: application/pdf
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator: SongYu
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:totalUnmappedUnicodeChars: 0
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_for_accessibility: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:assemble_document: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmpTPg:NPages: 133
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXMP: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:charsPerPage: 1441
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_content: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:trapped: False
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - SourceModified: D:20250804173605+08'00'
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By: org.apache.tika.parser.DefaultParser
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_modify: true
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:created: 2025-08-04T09:36:05Z
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsDamagedFont: false
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 提取的文本内容长度: 121403
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆状态的检查、为游客和已登录用户展示不同界面与功能。实现不同用户的权限控制和网站数据统计(UV、DAU)，管理员可以查看网站数据统计和网站监控信息。支持用户上传头像，实现发布帖子、评论帖子、热帖排行、发送私信与敏感词过滤等功能。实现了点赞关注与系统通知功能。支持全局搜索帖子信息的功能。核心功能具体实现1. 通过对登录用户颁发登录凭证，将登陆凭证存进 Redis 中来记录登录用户登录状态，使用拦截器进行登录状态检查，使用 Spring Security 实现权限控制，解决了 http 无状态带来的缺陷，保护需登录或权限才能使用的特定资源。（登入时将生成的 Ticket存入 Redies, 然后在登入请求成功时，将 Redies中的Ticket存入新建的 Cookie 中，然后反馈给浏览器，随后在该浏览器访问其他请求时，会先经过 LoginTicketInterceptor，判断请求中是否有 Ticket，是否和 Redies中的 T
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: icket一致，如果一致会将用户信息存入到 hostHolder（属于线程局部缓存）中，以便后续在请求处理过程中可以方便地获取到当前登录用户的信息，在请求处理完成后，清除当前线程中存储的用户信息。通过这种方式，确保每个请求都是独立处理的，不会因为线程复用而导致用户信息泄露或混淆。 注意可以将用户信息存入 Redis缓存中来减少 DB 的访问量，但是当用户数据更新时，必须即使删除 Redis中的用户数据，以保证数据的一致性和准确性。Spring Security 的用户认证是在自定义的过滤器中，也是获取请求 Cookie 中的 Ticket 和 Redis中的Ticket是否一致，然后将认证用户存到安全上下文中，在 Security 配置类中根据安全上下文获取用户信息，判断用户对各个资源的访问权限。Security 认证应当放到过滤器中而不是拦截器，因为过滤器比拦截器先执行，在拦截器中配置安全上下文会导致 Security 配置类获取不到用户信息，因为此时还没执行拦截器。）2. 使用 ThreadLocal 在当前线程中存储用户数据，代替 session 的功能便于分布式部署。在拦截器的 preHandle 中
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存储用户数据，在 postHandle 中将用户数据存入 Model，在 afterCompletion 中清理用户数据。（ThreadLocal 为每个线程提供独立的变量副本。每个线程操作自己的副本，互不干扰。在 Web 应用中，一个请求从开始到结束通常由同一个线程处理。因此，在拦截器的 preHandle 中存储的数据，可在整个请求链路（Controller、Service、Dao）中通过 ThreadLocal 获取。 在分布式部署中，由于 Session需要共享，使用 ThreadLocal存储用户数据，我们并不需要在多个服务器之间共享这些数据。因为每个请求都是独立的，处理完一个请求后，数据就被清除了。所以，在分布式环境下，我们只需要确保每个服务器能够独立处理请求即可，不需要考虑多个服务器之间的 Session同步问题。）3. 使用 Redis 的集合数据类型来解决踩赞、相互关注功能，采用事务管理，保证数据的正确，采用“先更新数据库，再删除缓存”策略保证数据库与缓存数据的一致性。采用 Redis 存储验证码，解决性能问题和分布式部署时的验证码需求。采用 Redis 的 HyperLogLog 存储每日
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  UV、Bitmap 存储 DAU，实现网站数据统计的需求。（使用 RedisTemplate执行一个 Redis 事务。SessionCallback 接口的 execute方法会在一个事务中执行所有操作，确保操作的原子性。通过调用 multi()方法，开启一个 Redis事务。在这个事务中执行的命令会被缓存，直到 exec()方法被调用时才会一次性提交。数据结构：HyperLogLog，12KB 内存可计算 2^64 个不重复元素 误差率仅 0.81% 数据结构：Bitmap 一连串二进制数组，可以进行二值状态统计）4. 使用 Kafka 作为消息队列，在用户被点赞、评论、关注后以系统通知的方式推送给用户，用户发布或删除帖子后向 elasticsearch 同步，对系统进行解耦、削峰。（在这个系统中 kafka 就办了三件事，一是用户在被点赞、评论、关注后会借助kafka 消费者来异步的生成系统通知，二三是在用户发布帖子和删除帖子时，将内容添加到 elasticsearch或者从 elasticsearch 中删除）5. 使用 elasticsearch + ik 分词插件实现全局搜索功能，当用户发布、修
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 改或删除帖子时，使用 Kafka 消息队列去异步将帖子信息给 elasticsearch 同步。（Elasticsearch（简称 ES）是一个开源的分布式搜索和分析引擎，它专为处理海量数据设计，提供近实时的全文搜索能力。IK Analyzer 是专为中文设计的开源分词插件，解决中文文本分析的核心难题。ik_smart：粗粒度切分 ik_max_word 细粒度切分）6. 使用分布式定时任务 Quartz 定时计算帖子分数，来实现热帖排行的业务功能。对频繁需要访问的数据，如用户信息、帖子总数、热帖的单页帖子列表，使用Caffeine 本地缓存 + Redis 分布式缓存的多级缓存，提高服务器性能，实现系统的高可用。（上面三个部分就是 Quartz的基本组成部分：调度器：Scheduler任务：JobDetail触发器：Trigger，包括 SimpleTrigger 和 CronTrigger定义一个 Quartz定时任务及其触发器。具体来说，它配置了一个名为postScoreRefreshJob的任务，该任务属于 communityJobGroup 组，并且被设置为持久化和请求恢复。同时，它还配置了一个名为
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  postScoreRefreshTrigger 的触发器，该触发器也属于 communityTriggerGroup组，并且每 5 分钟触发一次postScoreRefreshJob任务。因此，这段代码的主要目的是确保 PostScoreRefreshJob类中定义的任务每 5分钟执行一次）核心技术Spring Boot、SSMRedis、Kafka、ElasticsearchSpring Security、Quartz、Caffeine项目亮点项⽬构建在 Spring Boot+SSM 框架之上，并统⼀的进⾏了状态管理、事务管理、异常处理；利⽤ Redis 实现了点赞和关注功能，单机可达 5000TPS；利⽤ Kafka 实现了异步的站内通知，单机可达 7000TPS；利⽤ Elasticsearch 实现了全⽂搜索功能，可准确匹配搜索结果，并⾼亮显示关键词；利⽤ Caffeine+Redis 实现了两级缓存，并优化了热⻔帖⼦的访问，单机可达8000QPS。利⽤ Spring Security 实现了权限控制，实现了多重⻆⾊、URL 级别的权限管理；利⽤ HyperLogLog、Bitmap 分别实现了 
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: UV、DAU 的统计功能，100 万⽤户数据只需*M 内存空间；利⽤ Quartz 实现了任务调度功能，并实现了定时计算帖⼦分数、定时清理垃圾⽂件等功能；利⽤ Actuator 对应⽤的 Bean、缓存、⽇志、路径等多个维度进⾏了监控，并通过⾃定义的端点对数据库连接进⾏了监控。面试题：1.你提到使用 Spring Security 实现权限控制。能具体说明如何整合登录凭证（Redis存储）与 Spring Security？如何实现 URL 级别的动态权限管理？答：用户登入成功时系统会生成一个 Ticket并存入 Redis，新建一个 cookie 存入Ticket；在自定义的过滤器中验证请求携带的 Ticket与 Redis内的 Ticket是否一致，构建 Authentication对象存入 SecurityContextHolder；在 Security 的配置类中对固定 URL 如/admin/**）使用 antMatchers().hasRole("ADMIN")，即根据用户权限赋予访问资源的能力。2. 你提到点赞功能采用‘先更新 DB 再删缓存’策略。如果删除缓存失败导致不一致，如何解决？为何不用
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ‘更新缓存’方案？答：“删除失败兜底方案：设置缓存短过期时间（如 30s），容忍短期不一致异步重试：将失败操作推入 Kafka，消费者重试删除监听 MySQL Binlog（如 Canal）触发缓存删除不更新缓存的原因：写冲突： 并发更新可能导致缓存脏数据（如线程 A更新 DB 后未更新缓存时，线程 B又更新）浪费资源： 频繁更新但低读取的数据会占用带宽复杂度： 需维护缓存与 DB的强一致性逻辑（如分布式锁），而删除策略更简单可靠。”3. 系统通知使用 Kafka异步推送。如果通知发送失败（如网络抖动），如何保证用户最终能收到通知？答：“我们通过三级保障实现可靠性：生产者确认： 设置 Kafka acks=all，确保消息写入所有副本；消费者容错：开启手动提交 Offset，业务处理成功后才提交捕获异常后重试（如 3次），仍失败则存入死信队列补偿机制：定时任务扫描未通知记录（DB状态标记）重新投递死信队列消息人工介入处理此外，消息体包含唯一 ID 防重复消费。”4. 热帖列表用了 Caffeine+Redis两级缓存。如何解决缓存穿透？如何同步本地缓存（Caffeine）的数据？答：“缓存穿透防护：布隆过滤器
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ：将所有的 key 提前存入布隆过滤器，在访问缓存层之前，查询前校验 Key 是否存在，不存在返回空值。缓存空值：对不存在的帖子 ID 缓存 NULL（短过期时间）本地缓存同步过期同步： Caffeine设置 refreshAfterWrite=30s自动刷新主动推送： 当帖子更新时，通过 Redis Pub/Sub 广播失效事件，节点监听后删除本地缓存兜底策略： 本地缓存过期时间短于 Redis（如本地 60s vs Redis 300s），确保最终一致。”5. 定时计算帖子分数时，如何避免分布式环境下的重复执行？如果计算耗时过长导致阻塞，如何优化？答：“防重复执行：使用 Quartz集群模式：数据库锁（QRTZ_LOCKS 表）保证同一任务仅一个节点执行性能优化：分片处理： 按帖子 ID 范围分片（如 0-10000, 10001-20000），多线程并行计算增量计算： 仅扫描最近 X 小时变化的帖子（如 last_modified_time > now()-6h）异步化： 将计算任务拆解为多个子任务投递到 Kafka，消费者并发处理降级策略： 超时后记录断点，下次任务从断点继续。”6. 你使用 Elas
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ticsearch 实现全文搜索并高亮关键词。请说明：如何设计索引映射（Mapping）以优化搜索效率和准确性？如何实现搜索结果的高亮显示？遇到 HTML 标签转义问题如何处理？搜索性能瓶颈可能在哪里？如何优化？答: 1. 索引映射设计：分词策略： 对帖子标题和内容字段使用 ik_max_word 分词器进行细粒度分词（索引时），搜索时结合 ik_smart 提高相关性。字段类型： 标题用 text（分词） + keyword（不分词，用于精确匹配/聚合），ID 用 keyword，发布时间用 date。副本分片： 设置合理副本数（如 1-2）提高查询吞吐量和容错性。关闭不必要特性： 对不需聚合/排序的字段关闭 doc_values 节省存储。2. 高亮实现与转义：高亮请求： 在搜索请求中添加 highlight 部分，指定字段、pre_tags（如 <em>）、post_tags（如 </em>）。HTML 转义： ES 默认会转义高亮片段中的 HTML。我们确保存入 ES 的内容是纯文本（不含用户输入的原始 HTML），避免 XSS 同时解决转义混乱。前端渲染高亮片段时使用 textContent 而非 
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: innerHTML。3. 性能优化点：瓶颈： 复杂查询（多条件+聚合）、深度分页（from + size 过大）、索引设计不佳。优化措施：避免深度分页：使用 search_after + 唯一排序值（如 ID+时间戳）替代 from/size。限制查询范围： 使用 filter 缓存（如时间范围、状态）减少 query 计算量。冷热数据分离： 历史数据迁移到低性能节点或归档索引。合理硬件： SSD、充足内存（ES 堆内存 ≈ 50% 物理内存，不超过 31GB）。7. 项目用 HyperLogLog (HLL) 统计 UV，Bitmap 统计 DAU。请解释：HLL 如何用极小空间估算大基数？它的误差范围是多少？Bitmap 如何统计 DAU？如何解决用户 ID 非连续导致的空间浪费？如果某天 UV 突增，HLL 合并结果会怎样？如何验证其准确性？答：“1. HLL 原理与误差：原理： 对每个用户 ID 做哈希，计算哈希值二进制表示中 ‘1’ 的最高位位置（如 0001... 最高位=4），维护一个 ‘寄存器数组’ 记录每个桶的最大位置。最终通过调和平均数估算基数。核心是利用概率分布。误差： Redis 的 
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: PF 实现标准误差约 0.81%（使用 16384 个寄存器时）。空间仅需 12KB（固定大小）。2. Bitmap 与 DAU：实现： 每天一个 Bitmap Key（如 dau:20230702），用户 ID 作为 Offset，访问则设位为 1。BITCOUNT 获取当日活跃用户数。稀疏优化： 使用 RLE (Run-Length Encoding) 压缩的 Bitmap 库（如 RoaringBitmap）或 Redis 的 BITFIELD 命令动态管理非连续 ID，避免传统 Bitmap 的空间浪费。3. HLL 突增与验证：突增影响：HLL 是基数估计，突增时估算值会上升，误差仍在理论范围内（0.81%）。合并多个 HLL（如按小时合并成天）误差会累积但可控。验证： 定期抽样对比：对某小段时间用 SET 精确计算 UV，与 HLL 结果对比，监控误差是否符合预期。业务上接受近似值是其使用前提。8. 敏感词过滤是社区必备功能。你如何实现它？如何平衡过滤效率和敏感词库的更新？“技术选型：Trie 树 (前缀树)实现：初始化： 服务启动时将敏感词库（DB 或文件）加载到内存中的 Trie 树。节点标记
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是否为词尾。过滤过程： 对用户输入（帖子/评论）进行滑动窗口扫描。匹配到 Trie 树路径且到达词尾时，替换为 *** 或阻断提交。优化： 结合 DFA (确定有限状态自动机) 减少回溯，支持跳过无关字符（如 敏*感*词）。词库更新：热更新： 后台管理添加敏感词后，通过 ZooKeeper 配置中心 或 Redis Pub/Sub广播到所有服务节点，节点异步重建 Trie 树。降级： 更新期间短暂使用旧词库，避免服务中断。词库版本号控制。效率：Trie 树查询时间复杂度 O(n) (n=文本长度)，内存占用可控（可压缩节点）。避免正则表达式（性能差）。”9. 你提到用 Spring Boot Actuator 进行监控并自定义了数据库监控端点。请说明：暴露了哪些关键内置端点？（至少 3个）如何自定义一个端点监控数据库连接池状态（如活跃连接数、等待连接数）？如何保证这些监控接口的安全？答：“1. 关键内置端点：/health：应用健康状态（DB, Redis, Disk 等）/metrics：JVM 内存、线程、HTTP 请求指标等/loggers：动态查看/调整日志级别/threaddump：获取线程快照（排
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 查死锁）创建一个类实现 Endpoint 接口或使用 @Endpoint(id = "dbpool") 注解。注入连接池对象（如 HikariDataSource）。在 @ReadOperation 方法中返回关键指标：安全保障：访问控制：通过 management.endpoints.web.exposure.include/exclude 精确控制暴露的端点。安全加固：集成 Spring Security：只允许管理员角色访问 /actuator/** 路径。修改默认端口：management.server.port 使用与管理网络隔离的端口。HTTPS： 强制要求监控端点使用 HTTPS。10. 在“点赞后发通知”这个场景，涉及更新数据库点赞数 (DB) 和发送 Kafka 消息 (通知) 。如何保证这两个操作的原子性？如果 Kafka 发送失败，如何处理？答：“核心思路：最终一致性 + 本地事务 + 可靠消息原子性保障： 将 ‘更新点赞状态/计数’ 和 ‘写入待通知消息’ 放在同一个数据库事务中。使用 ‘本地消息表’ 方案：在业务数据库创建 message_event 表 (含业务 ID、消息体、状态
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: [PENDING, SENT])。事务内操作：更新点赞相关 DB 数据。向 message_event 插入一条 PENDING 状态的通知记录。事务提交。Kafka 发送与补偿：后台定时任务扫描 PENDING 的消息。发送消息到 Kafka，成功后将状态改为 SENT。发送失败处理：记录重试次数和错误信息，下次任务重试（指数退避）。超过最大重试则标记为失败，告警人工介入。消费者幂等： 通知消费者根据业务 ID 去重，避免重复处理。为什么不强一致？ 跨系统（DB 与 MQ）的强一致（如 2PC）成本高且降低可用性。本方案在 CAP 中优先保证 AP，通过可靠消息实现最终一致，满足业务需求。”11.你提到使用 ThreadLocal 存储用户数据以替代 Session。这在单机中可行，但分布式部署时（如多台 Tomcat 节点）会失效。如何解决分布式场景下的用户状态共享问题？业界主流方案是什么？答：“ThreadLocal 的局限： 它绑定于单个 JVM 线程，无法跨节点共享。在负载均衡（如 Nginx 轮询）下，用户请求落到不同节点会导致状态丢失。方案演进：Session 复制： 利用 Tomcat Red
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: is Session Manager 等工具将 Session 存入Redis。所有节点从 Redis 读写 Session，实现共享。无状态 Token (JWT)： 当前项目采用的核心方案。用户登录后生成包含用户 ID和权限的 JWT Token 返回客户端（通常存于 Cookie 或 Header）。后续请求携带 Token，服务端无需存储 Session，仅需验证 Token 签名和有效期并从 Token中解析用户信息（如注入到 SecurityContext）。这天然支持分布式。项目整合： 我们实际采用了 JWT + Redis 黑名单 的增强方案：JWT 本身无状态，解析快速。主动登出/失效： 将需提前失效的 Token ID 存入 Redis 并设置 TTL（作为黑名单）。校验 Token 时额外检查黑名单。安全性： Token 使用强密钥签名（如 HMAC-SHA256），防止篡改。优点： 彻底解决分布式状态问题，减轻服务端存储压力，更适合 RESTful API。12. 热帖列表使用了 Caffeine 本地缓存。请说明：你选择了哪种缓存淘汰策略（如 LRU、LFU）？依据是什么？如何配置缓
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存大小和过期时间？如何监控缓存命中率？本地缓存导致不同节点数据不一致的风险如何缓解？（例如帖子被删除）答：“1. 淘汰策略与依据：策略： 使用 Window TinyLFU (W-TinyLFU)，Caffeine 的默认算法。它结合了 LRU（近期使用）和 LFU（频率统计）的优点，对突发流量和长期热点都有良好表现。依据：论坛热帖访问模式既有突发（新热帖），也有长尾（持续热帖）。W-TinyLFU在有限空间内能最大化命中率，优于纯 LRU/LFU。2. 配置与监控：配置：javaCaffeine.newBuilder().maximumSize(10_000) // 最大条目数.expireAfterWrite(5, TimeUnit.MINUTES) // 写入后 5 分钟过期.recordStats() // 开启统计.build();监控： 通过 Cache.stats() 获取 CacheStats 对象，关键指标：hitRate()：命中率evictionCount()：淘汰数量averageLoadPenalty()：平均加载耗时可定期输出到日志或监控系统（如 Prometheus）。3. 数据
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不一致风险缓解：主动失效： 核心策略！当帖子被删除或更新时：更新数据库。删除 Redis 中的缓存 Key。通过 Redis Pub/Sub 或 专门的广播方案（如 RabbitMQ Fanout Exchange） 发布“帖子失效”事件。所有服务节点监听到事件后，删除本地 Caffeine 缓存中对应的条目。兜底： 设置较短的本地缓存过期时间（如 5分钟），确保最终一致。”13.你提到点赞功能单机 TPS 达 5000，通知单机 TPS 7000，热帖访问 QPS 8000。请说明：这些数据是如何测试得到的？（工具、场景、环境）TPS 和 QPS 的区别是什么？测试中发现了哪些性能瓶颈？如何定位和优化的？（如 GC、慢 SQL）1. 测试方法：工具： JMeter（模拟并发用户）。场景：点赞 TPS： 持续模拟用户对随机帖子点赞（高并发写）。通知 TPS： 模拟触发通知事件（评论/关注），测量 Kafka 生产者吞吐量。热帖 QPS： 持续请求热帖列表接口（高并发读）。环境： 明确标注是 单机测试（如 4C8G Linux, JDK 17, Tomcat, Redis/Kafka 同机或独立）。2. TPS
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  vs QPS：QPS (Queries Per Second)： 服务器每秒处理的查询请求数（如 HTTP 请求）。适用于读场景。TPS (Transactions Per Second)： 服务器每秒处理的事务数（一个事务可能包含多个操作/请求）。点赞（写 DB + Redis + 发 Kafka）是一个事务。此处 5000 TPS 指每秒完成 5000 次点赞事务。3. 瓶颈发现与优化：发现工具： Arthas (监控方法耗时)、JVisualVM/PerfMa (GC 分析)、Redis Slowlog、MySQL Slow Query Log。典型瓶颈 & 优化：GC 频繁 (Young GC >1s)： 优化 JVM 参数（如 -XX:+UseG1GC, 调整MaxGCPauseMillis），减少大对象分配（如缓存 DTO 复用）。慢 SQL (全表扫描)： 添加索引（如 post_id 在点赞表），优化查询（避免 SELECT*）。Redis 单线程阻塞： 避免长命令（如 KEYS *），分片（Cluster），热点 Key 本地缓存（Caffeine）。Kafka 生产瓶颈： 调优 batc
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: h.size 和 linger.ms，增加分区数，提高 acks 级别换取可靠性（需权衡）。”14.如果这个论坛用户量增长 100 倍（如日活千万级），当前架构在哪些地方可能最先遇到瓶颈？你会如何改造？（请结合你已用技术栈思考）“潜在瓶颈与改造方向：数据库 (MySQL)：瓶颈： 写压力（点赞、发帖）、复杂查询（搜索、统计）、单表数据量过大。改造：读写分离： 主库写，多个从库读（评论列表、用户信息查询）。分库分表： 按 user_id 或 post_id 分片（如 ShardingSphere）。将点赞/关注等高频写操作分离到独立库。冷热数据分离： 归档旧帖到分析型数据库（如 HBase）。Redis：瓶颈： 单机内存容量、带宽、单线程处理。改造：集群化： Redis Cluster 自动分片。区分数据类型： 热点数据（用户信息）用集群；超大 Value（如长帖缓存）考虑其他存储或压缩；统计类（UV/DAU）可保留。Elasticsearch：瓶颈： 索引过大导致查询慢、写入堆积。改造：分片策略优化： 增加主分片数（提前规划）。按时间分索引： 如 posts-202307，便于管理/查询/删除旧数据。Kafk
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: a：瓶颈： 单个 Topic 分区数限制吞吐量。改造： 增加分区数，生产者根据 Key（如 user_id）分区保证顺序性。应用层：瓶颈： 单节点处理能力。改造： 水平扩展无状态节点（更多 Tomcat 实例），通过 Nginx 负载均衡。微服务化拆分（如独立用户服务、帖子服务、消息服务），便于独立伸缩。监控与治理：加强：引入 APM（如 SkyWalking）、集中日志（ELK）、更强健的配置中心（Nacos）和熔断限流（Sentinel）。RedisIO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？”同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。举例子说一下阻塞 IO 和 IO
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  多路复用的差别？假设我是一名老师，让学生解答一道题目。我的第一种选择：按顺序逐个检查，先检查 A 同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种就是阻塞 IO，不具有并发能力我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 ARedis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。等子进
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。AOF 重写期间命令会同时写入现有 AOF 文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。缺点是可能丢失两次快照期间的所有数据变更AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。缺点是文件体积较大，恢复速度慢。在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。如果是缓存场景，可以接受一定程度的数据丢失，
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布/订阅机制通知客户端主节点已经发生变化。主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。当
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。常见的数据分区有三种：节点取余、一致性哈希和哈希槽。节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过CRC16 算法计算后对 16384 取模，确定它属于哪个槽。布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来解决 Redis 缓存穿透的问题。布隆过滤器并不支持删除操作，这是它的一个重要限制。如何保证缓存和数据库数据的一致性？具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题：线程 A 要更新用户信息，先删除了缓存线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值线程 B 将查到的旧值重新放入缓存线程 A 完成数据库更新结果就是数据库是新的值，但缓存中还是旧值当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。即使缓存删除失败，消息队列的重试机制也能保证最终一致性。第二种
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。如何保证本地缓存和分布式缓存的一致性？为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pub/sub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内存压力。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis虽然不是专业的消息队列，但可以很好地实现延时队列功能。核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不存在时才能创建成功，EX 设置过期时间用以防止死锁。Kafka，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景Kafka 的设计Kafka 将消息以 topic 为单位进行归纳，发布消息的程序称为 Producer，消费消息的程序称为 Consumer。它是以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 Broker，Producer 通过网络将消息发送到 kafka 集群，集群向消费者提供消息，broker 在中间起到一个代理保存消息的中转站。Kafka 性能高原因利用了 PageCache 缓存磁盘顺序写零拷贝技术pull 拉模式优点高性能、高吞吐量、低延迟：Kafka 生产和消费消息的速度都达到每秒 10 万级高可用：所有消息持久化存储到磁盘，并支持数据备份防止数据丢失高并发：支持数千个客户端同时读写容错性：允许集群中节点失败（若副本数量为 n，则允许 n-1 个节点失败）高扩展性：Kafka 集群支持热伸缩，无须停机缺点没有完整的监控工具集不支持通配符主
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 题选择Kafka 的应用场景日志聚合：可收集各种服务的日志写入 kafka 的消息队列进行存储消息系统：广泛用于消息中间件系统解耦：在重要操作完成后，发送消息，由别的服务系统来完成其他操作流量削峰：一般用于秒杀或抢购活动中，来缓冲网站短时间内高流量带来的压力异步处理：通过异步处理机制，可以把一个消息放入队列中，但不立即处理它，在需要的时候再进行处理Kafka 为什么要把消息分区方便扩展：因为一个 topic 可以有多个 partition，每个 Partition 可用通过调整以适应它所在的机器，而一个 Topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率Kafka 中生产者运行流程一条消息发过来首先会被封装成一个 ProducerRecord 对象对该对象进行序列化处理（可以使用默认，也可以自定义序列化）对消息进行分区处理，分区的时候需要获取集群的元数据，决定这个消息会被发送到哪个主题的哪个分区分好区的消息不会直接发送到服务端，而是放入生产者的缓存区，多条消息会被封装成一个批次（
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Batch），默认一个批次的大小是 16KBSender 线程启动以后会从缓存里面去获取可以发送的批次Sender 线程把一个一个批次发送到服务端Kafka采用大部分消息系统遵循的传统模式：Producer 将消息推送到 Broker，Consumer 从 Broker 获取消息。负载均衡是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度保证系统整体运行效率与稳定性负载均衡Kakfa 的负载均衡就是每个 Broker 都有均等的机会为 Kafka 的客户端（生产者与消费者）提供服务，可以负载分散到所有集群中的机器上。Kafka 通过智能化的分区领导者选举来实现负载均衡，提供智能化的 Leader 选举算法，可在集群的所有机器上均匀分散各个 Partition的 Leader，从而整体上实现负载均衡。故障转移Kafka 的故障转移是通过使用会话机制实现的，每台 Kafka 服务器启动后会以会话的形式把自己注册到 Zookeeper 服务器上。一旦服务器运转出现问题，就会导致与 Zookeeper 的会话不能维持从而超时断连，此时 Kafka 集群会选举出另一台服务器来完全替代这台服务
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 器继续提供服务。Kafka 中 Zookeeper 的作用Kafka 是一个使用 Zookeeper 构建的分布式系统。Kafka 的各 Broker 在启动时都要在 Zookeeper上注册，由 Zookeeper统一协调管理。如果任何节点失败，可通过Zookeeper从先前提交的偏移量中恢复，因为它会做周期性提交偏移量工作。同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也是 Zookeeper在维护Kafka 中消费者与消费者组的关系与负载均衡实现Consumer Group 是 Kafka 独有的可扩展且具有容错性的消费者机制。一个组内可以有多个 Consumer，它们共享一个全局唯一的 Group ID。组内的所有 Consumer协调在一起来消费订阅主题（Topic）内的所有分区（Partition）。当然，每个 Partition只能由同一个 Consumer Group内的一个 Consumer 来消费。消费组内的消费者可以使用多线程的方式实现，消费者的数量通常不超过分区的数量，且二者最好保持整数倍的关系，这样不会造成有空
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲的消费者。Consumer 订阅的是 Topic 的 Partition，而不是 Message。所以在同一时间点上，订阅到同一个分区的 Consumer 必然属于不同的 Consumer GroupConsumer Group与 Consumer的关系是动态维护的，当一个 Consumer 进程挂掉或者是卡住时，该 Consumer 所订阅的 Partition会被重新分配到改组内的其他Consumer 上，当一个 Consumer加入到一个 Consumer Group中时，同样会从其他的 Consumer 中分配出一个或者多个 Partition到这个新加入的 Consumer。当生产者试图发送消息的速度快于 Broker 可以处理的速度时，通常会发生QueueFullException首先先进行判断生产者是否能够降低生产速率，如果生产者不能阻止这种情况，为了处理增加的负载，用户需要添加足够的 Broker。或者选择生产阻塞，设置Queue.enQueueTimeout.ms 为 -1，通过这样处理，如果队列已满的情况，生产者将组织而不是删除消息。或者容忍这种异常，进行消息丢弃。Consumer 如何
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 消费指定分区消息Cosumer 消费消息时，想 Broker 发出 fetch 请求去消费特定分区的消息，Consumer 可以通过指定消息在日志中的偏移量 offset，就可以从这个位置开始消息消息，Consumer 拥有了 offset 的控制权，也可以向后回滚去重新消费之前的消息。也可以使用 seek(Long topicPartition) 来指定消费的位置。Replica、Leader 和 Follower 三者的概念:Kafka 中的 Partition 是有序消息日志，为了实现高可用性，需要采用备份机制，将相同的数据复制到多个 Broker 上，而这些备份日志就是 Replica，目的是为了防止数据丢失。所有 Partition 的副本默认情况下都会均匀地分布到所有 Broker 上,一旦领导者副本所在的 Broker 宕机，Kafka 会从追随者副本中选举出新的领导者继续提供服务。Leader： 副本中的领导者。负责对外提供服务，与客户端进行交互。生产者总是向 Leader 副本些消息，消费者总是从 Leader 读消息Follower： 副本中的追随者。被动地追随 Leader，不能与外界进
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行交付。只是向 Leader 发送消息，请求 Leader把最新生产的消息发给它，进而保持同步。Kafka 中 AR、ISR、OSR 三者的概念AR：分区中所有副本称为 ARISR：所有与主副本保持一定程度同步的副本（包括主副本）称为 ISROSR：与主副本滞后过多的副本组成 OSR分区副本什么情况下会从 ISR 中剔出Leader 会维护一个与自己基本保持同步的 Replica列表，该列表称为 ISR，每个Partition都会有一个 ISR，而且是由 Leader 动态维护。所谓动态维护，就是说如果一个 Follower比一个 Leader 落后太多，或者超过一定时间未发起数据复制请求，则 Leader 将其从 ISR 中移除。当 ISR 中所有 Replica 都向 Leader 发送 ACK（Acknowledgement确认）时，Leader 才 commit分区副本中的 Leader 如果宕机但 ISR 却为空该如何处理可以通过配置 unclean.leader.election ：true：允许 OSR 成为 Leader，但是 OSR 的消息较为滞后，可能会出现消息不一致的问题false：会一
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直等待旧 leader 恢复正常，降低了可用性Kafka的 Producer 有三种 ack机制，参数值有 0、1 和 -10： 相当于异步操作，Producer 不需要 Leader 给予回复，发送完就认为成功，继续发送下一条（批）Message。此机制具有最低延迟，但是持久性可靠性也最差，当服务器发生故障时，很可能发生数据丢失。1： Kafka 默认的设置。表示 Producer 要 Leader 确认已成功接收数据才发送下一条（批）Message。不过 Leader 宕机，Follower 尚未复制的情况下，数据就会丢失。此机制提供了较好的持久性和较低的延迟性。-1： Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些Follower都确认消息已同步，Producer 才发送下一条（批）Message。此机制持久性可靠性最好，但延时性最差Kafka 的 consumer 如何消费数据在 Kafka中，Producers 将消息推送给 Broker 端，在 Consumer 和 Broker 建立连接之后，会主动去 Pull（或者说 Fetch）消息。这种模式有些优点
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，首先 Consumer端可以根据自己的消费能力适时的去 fetch消息并处理，且可以控制消息消费的进度（offset）；此外，消费者可以控制每次消费的数，实现批量消费。Kafka 的 Topic 中 Partition 数据是怎么存储到磁盘的用磁盘顺序写+内存页缓存+稀疏索引Topic 中的多个 Partition 以文件夹的形式保存到 Broker，每个分区序号从 0递增，且消息有序。Partition 文件下有多个 Segment（xxx.index，xxx.log），Segment文件里的大小和配置文件大小一致。默认为 1GB，但可以根据实际需要修改。如果大小大于 1GB时，会滚动一个新的 Segment并且以上一个 Segment 最后一条消息的偏移量命名。生产者发送消息│▼Leader Partition│▼ (追加写入)当前活跃 Segment: [00000000000000.log]│▼ (每隔 4KB 数据)更新 .index/.timeindex│▼ (Segment 满 1GB)创建新 Segment: [00000000000015.log]消费者请求 offset=520│▼查 .
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: index 文件 → 找到 offset=500 → position=10240│▼从 .log 文件 10240 位置顺序扫描│▼找到 offset=520 的消息返回Kafka 创建 Topic 后如何将分区放置到不同的 Broker 中Kafka创建 Topic 将分区放置到不同的 Broker 时遵循以下规则：副本因子不能大于 Broker 的个数。第一个分区（编号为 0）的第一个副本放置位置是随机从 Broker List 中选择的。其他分区的第一个副本放置位置相对于第 0个分区依次往后移。也就是如果有 3个 Broker，3 个分区，假设第一个分区放在第二个 Broker 上，那么第二个分区将会放在第三个 Broker 上；第三个分区将会放在第一个 Broker 上，更多 Broker 与更多分区依此类推。剩余的副本相对于第一个副本放置位置其实是由nextReplicaShift决定的，而这个数也是随机产生的。Kafka 中如何进行主从同步Kafka动态维护了一个同步状态的副本的集合（a set of In-SyncReplicas），简称 ISR，在这个集合中的结点都是和 Leader 保持高
2025-08-11 10:55:52.764 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度一致的，任何一条消息只有被这个集合中的每个结点读取并追加到日志中，才会向外部通知“这个消息已经被提交”同步复制Producer 会先通过 Zookeeper识别到 Leader，然后向 Leader 发送消息，Leader收到消息后写入到本地 log文件。这个时候 Follower 再向 Leader Pull 消息，Pull回来的消息会写入的本地 log 中，写入完成后会向 Leader 发送 Ack 回执，等到 Leader 收到所有 Follower 的回执之后，才会向 Producer 回传 Ack。异步复制Kafka 中 Producer 异步发送消息是基于同步发送消息的接口来实现的，异步发送消息的实现很简单，客户端消息发送过来以后，会先放入一个 BlackingQueue队列中然后就返回了。Producer 再开启一个线程 ProducerSendTread 不断从队列中取出消息，然后调用同步发送消息的接口将消息发送给 Broker。Kafka 中什么情况下会出现消息丢失/不一致的问题消息发送时消息发送有两种方式：同步 - sync 和 异步 - async。默认是同步的方式，可以通过 prod
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ucer.type 属性进行配置，kafka 也可以通过配置 acks 属性来确认消息的生产0：表示不进行消息接收是否成功的确认1：表示当 leader 接收成功时的确认-1：表示 leader 和 follower 都接收成功的确认当 acks = 0 时，不和 Kafka 进行消息接收确认，可能会因为网络异常，缓冲区满的问题，导致消息丢失当 acks = 1 时，只有 leader 同步成功而 follower 尚未完成同步，如果 leader挂了，就会造成数据丢失消息消费时Kafka 有两个消息消费的 consumer 接口，分别是 low-level 和 hign-levellow-level：消费者自己维护 offset 等值，可以实现对 kafka 的完全控制high-level：封装了对 partition 和 offset，使用简单如果使用高级接口，可能存在一个消费者提取了一个消息后便提交了 offset，那么还没来得及消费就已经挂了，下次消费时的数据就是 offset + 1 的位置，那么原先 offset 的数据就丢失了Kafa 中如何保证顺序消费Kafka 的消费单元是 Partitio
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n，同一个 Partition 使用 offset 作为唯一标识保证顺序性，但这只是保证了在 Partition 内部的顺序性而不是 Topic 中的顺序，因此我们需要将所有消息发往统一 Partition 才能保证消息顺序消费，那么可以在发送的时候指定 MessageKey，同一个 key 的消息会发到同一个 Partition 中。Java介绍一下 javajava 是一门开源的跨平台的面向对象的计算机语言.跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而虚拟机是不同平台有不同版本,所以说 java 是跨平台的.面向对象有几个特点:1.封装两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者'封装'在一个不可分割的独立单元(即对象)中另一层含义指'信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能隐藏对象的功能实现细节。2.继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有父类相同的行为。3.多态多态是同一个行为具有多个不同表现形式或形态的能力。Java 语言中含有方法重载与对象多态两种形式的多态：1.方法重载：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。2.对象多态：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。Java有哪些数据类型？java 主要有两种数据类型1.基本数据类型基本数据有八个,byte,short,int,long 属于数值型中的整数型float,double属于数值型中的浮点型char属于字符型boolean属于布尔型2.引用数据类型引用数据类型有三个,分别是类,接口和数组接口和抽象类有什么区别？1.接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类是声明方法的存在而不去实现它的类。2.接口可以多继承，抽象类不行。3.接口定义方法，不能实现，默认是 public abstract，而抽象类可以实现部分方法。4.接口中基本数据类型为 public static final 并且需要给出初始值，而抽类象不是的。重载和重写什么区别？重写：1.参数列表必须完全与被重写的方法相同，
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否则不能称其为重写而是重载.2.返回的类型必须一直与被重写的方法的返回类型相同，否则不能称其为重写而是重载。3.访问修饰符的限制一定要大于被重写方法的访问修饰符4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。重载：1.必须具有不同的参数列表；2.可以有不同的返回类型，只要参数列表不同就可以了；3.可以有不同的访问修饰符；4.可以抛出不同的异常；常见的异常有哪些？NullPointerException 空指针异常ArrayIndexOutOfBoundsException 索引越界异常InputFormatException 输入类型不匹配SQLException SQL 异常IllegalArgumentException 非法参数NumberFormatException 类型转换异常 等等....异常要怎么解决？Java标准库内建了一些通用的异常，这些类以 Throwable 为顶层父类。Throwable又派生出 Error 类和 Exception类。错误：Error类以及他的子类的实例，代表了 JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，程序员应该关注 Exception 为父类的分支下的各种异常类。异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被 Java异常处理机制使用，是异常处理的核心。hashMap 线程不安全体现在哪里？在 hashMap1.7 中扩容的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法在任意版本的 hashMap 中，如果在插入数据时多个线程命中了同一个槽，可能会有数据覆盖的情况发生，导致线程不安全。说说进程和线程的区别？进程是系统资源分配和调度的基本单位，它能并发执行较高系统资源的利用率.线程是比进程更小的能独立运行的基本单位,创建、销毁、切换成本要小于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为 1，结果又是什么？Integer a = 1000，Integer b = 1000，a==b 结果为 falseInteger a = 1，Integer b = 1，a==b 结
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果为 true这道题主要考察 Integer 包装类缓存的范围,在-128~127 之间会缓存起来,比较的是直接缓存的数据,在此之外比较的是对象JMM 就是 Java 内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以 java 内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 java 程序在各种平台下都能达到一致的并发效果。Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。线程不能直接读写主内存中的变量。每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式cas 是什么？cas 叫做 CompareAndSwap，比较并交换，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是通过处理器的指令来保证操作的原子性，它主要包含三
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个变量：当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，如果相同就认为其他线程没有进行修改，而将新值写入内存聊聊 ReentrantLock 吧ReentrantLock 意为可重入锁，说起 ReentrantLock 就不得不说 AQS ，因为其底层就是使用 AQS 去实现的。ReentrantLock有两种模式，一种是公平锁，一种是非公平锁。公平模式下等待线程入队列后会严格按照队列顺序去执行非公平模式下等待线程入队列后有可能会出现插队情况公平锁第一步：获取状态的 state 的值如果 state=0 即代表锁没有被其它线程占用，执行第二步。如果 state!=0 则代表锁正在被其它线程占用，执行第三步。第二步：判断队列中是否有线程在排队等待如果不存在则直接将锁的所有者设置成当前线程，且更新状态 state 。如果存在就入队。第三步：判断锁的所有者是不是当前线程如果是则更新状态 state 的值。如果不是，线程进入队列排队等待。非公平锁获取状态的 state 的值如果 state=0 即代表锁
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 没有被其它线程占用，则设置当前锁的持有者为当前线程，该操作用 CAS 完成。如果不为 0或者设置失败，代表锁被占用进行下一步。此时获取 state 的值如果是，则给 state+1，获取锁如果不是，则进入队列等待如果是 0，代表刚好线程释放了锁，此时将锁的持有者设为自己如果不是 0，则查看线程持有者是不是自己多线程的创建方式有哪些？继承 Thread类，重写 run()方法public class Demo extends Thread{//重写父类 Thread的 run()public void run() {}public static void main(String[] args) {Demo d1 = new Demo();Demo d2 = new Demo();d1.start();d2.start();}}实现 Runnable接口，重写 run()public class Demo2 implements Runnable{//重写 Runnable接口的 run()public void run() {}public static void main(String[] args) {Th
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: read t1 = new Thread(new Demo2());Thread t2 = new Thread(new Demo2());t1.start();t2.start();}}实现 Callable 接口public class Demo implements Callable<String>{public String call() throws Exception {System.out.println("正在执行新建线程任务");Thread.sleep(2000);return "结果";}public static void main(String[] args) throws InterruptedException,ExecutionException {Demo d = new Demo();FutureTask<String> task = new FutureTask<>(d);Thread t = new Thread(task);t.start();//获取任务执行后返回的结果String result = task.get();}}使用线程池创建public class 
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Demo {public static void main(String[] args) {Executor threadPool = Executors.newFixedThreadPool(5);for(int i = 0 ;i < 10 ; i++) {threadPool.execute(new Runnable() {public void run() {//todo}});}}}线程池有哪些参数？corePoolSize：核心线程数，线程池中始终存活的线程数。2.maximumPoolSize: 最大线程数，线程池中允许的最大线程数。3.keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。4.unit: 单位，参数 keepAliveTime 的时间单位，7种可选。5.workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种可选。6.threadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。7.handler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicy。线程池的执行流程？判断线程池中的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程数是否大于设置的核心线程数如果小于，就创建一个核心线程来执行任务如果大于，就会判断缓冲队列是否满了如果没有满，则放入队列，等待线程空闲时执行任务如果队列已经满了，则判断是否达到了线程池设置的最大线程数如果没有达到，就创建新线程来执行任务如果已经达到了最大线程数，则执行指定的拒绝策略深拷贝、浅拷贝是什么？浅拷贝并不是真的拷贝，只是复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象聊聊 ThreadLocal 吧ThreadLocal其实就是线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。一个对象的内存布局是怎么样的?1.对象头: 对象头又分为 MarkWord 和 Class Pointer 两部分。MarkWord:包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位,gc 记录信息等等。ClassPointer:用来指向对象对应的 Class 对象（其对应的元数据对象）的内存地址。在 32 位系统占 4 字节，在 64 位系统中占 8 字节
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 。2.Length:只在数组对象中存在，用来记录数组的长度，占用 4 字节3.Instance data: 对象实际数据，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括静态成员变量，因为其是在方法区维护的)4.Padding:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，padding 的作用就是补充字节，保证对象是 8 字节的整数倍。HashMapHashMap的底层数据结构是什么？JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。链表过长，会严重影响HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是糟糕的 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：当链
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 表超过 8 且数据总量超过 64 时会转红黑树。将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。解决 hash冲突的办法有哪些？HashMap用的哪种？解决 Hash 冲突方法有：开放定址法：也称为再散列法，基本思想就是，如果 p=H(key)出现冲突时，则以p为基础，再次 hash，p1=H(p),如果 p1再次出现冲突，则以 p1为基础，以此类推，直到找到一个不冲突的哈希地址 pi。因此开放定址法所需要的 hash表的长度要大于等于所需要存放的元素，而且因为存在再次 hash，所以只能在删除的节点上做标记，而不能真正删除节点。再哈希法：双重散列，多重散列，提供多个不同的 hash函数，当 R1=H1(key1)发生冲突时，再计算 R2=H2(key1)，直到没有冲突为止。这样做虽然不易产生堆集，但增加了计算的时间。链地址法：拉链法，将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第 i 个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。建立公共溢出区：将哈希表分为公共表和
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。HashMap中采用的是链地址法为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。为什么 hash 值要与 length-1 相与？把 hash 值对数组长度取模运算，模运算的消耗很大，没有位运算快。当 length 总是 2 的 n次方时，h& (length-1) 运算等价于对 length取模，也就是 h%length，但是 & 比 % 具有更高的效率HashMap数组的长度为什么是 2 的幂次方？2 的 N 次幂有助于减少碰撞的几率。如果 length 为 2的幂次方，则 length-1 转化为二进制必定是 11111……的形式，在与 h 的二进制与操作效率会非
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 常的快，而且空间不浪费。HashMap 的 put方法流程？首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；2、如果数组是空的，则调用 resize 进行初始化；3、如果没有哈希冲突直接放在对应的数组下标里；4、如果冲突了，且 key 已经存在，就覆盖掉 value；5、如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；6、如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。一般用什么作为 HashMap的 key?一般用 Integer、String 这种不可变类当作 HashMap 的 key，String 最为常见。因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的。Integer、String 这些类已经很规范的重写了hashCode() 以及 
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: equals() 方法。MySQL关系型和非关系型数据库的区别？关系型数据库的优点容易理解，因为它采用了关系模型来组织数据。可以保持数据的一致性。数据更新的开销比较小。支持复杂查询（带 where 子句的查询）非关系型数据库（NOSQL）的优点无需经过 SQL 层的解析，读写效率高。基于键值对，读写性能很高，易于扩展可以支持多种类型数据的存储，如图片，文档等等。扩展（可分为内存性数据库以及文档型数据库，比如 Redis，MongoDB，HBase 等，适合场景：数据量大高可用的日志系统/地理位置存储系统）。详细说一下一条 MySQL 语句执行的步骤Server 层按顺序执行 SQL 的步骤为：客户端请求 -> 连接器（验证用户身份，给予权限）查询缓存（存在缓存则直接返回，不存在则执行后续操作）分析器（对 SQL 进行词法分析和语法分析操作）优化器（主要对执行的 SQL 优化选择最优的执行方案方法）执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-> 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）MySQL 使用索引的原因？根本原因索引的出现，就是为了提高数据查询的效率，就像书的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 目录一样。对于数据库的表而言，索引其实就是它的“目录”。扩展创建唯一性索引，可以保证数据库表中每一行数据的唯一性。帮助引擎层避免排序和临时表将随机 IO 变为顺序 IO，加速表和表之间的连接索引的三种常见底层数据结构以及优缺点三种常见的索引底层数据结构：分别是哈希表、有序数组和搜索树。哈希表这种适用于等值查询的场景，比如 memcached 以及其它一些 NoSQL 引擎，不适合范围查询，哈希表的数据是完全无序存储的。它只能回答“某个键值等于多少”的记录在哪，无法高效地查询“键值在某个范围之间”的所有记录（如WHERE id BETWEEN 10 AND 20）。需要扫描全表或遍历所有桶，效率极低 (O(n))。有序数组索引只适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高。N 叉树由于读写上的性能优点以及适配磁盘访问模式以及广泛应用在数据库引擎中。扩展（以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。）索引的常见类型以及它是如何发挥作用的？根据叶子节点的内容，索引类型分为主键索引和非主键索引。主键索引的叶子节点存的整行数据，在 InnoDB 里也被称为聚簇索引。非主键索引叶子节点存的主键的值，在 InnoDB 里也被称为二级索引MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件。MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data域存放的是数据记录的地址，索引文件和数据文件是分离的InnoDB 为什么设计 B+ 树索引？两个考虑因素：InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。CPU 将磁盘上的数据加载到内存中需要花费大量时间。为什么选择 B+ 树：哈希索引虽然能提供 O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机IO。而 B+ 树的所有叶节点可以通过指针来相互连接
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，减少顺序遍历带来的随机 IO。普通索引还是唯一索引？由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。什么是覆盖索引和索引下推？覆盖索引：在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。索引下推：MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。MySQL 的 change buffer 是什么？当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。注意唯一索引的更新就不
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 能使用 change buffer，实际上也只有普通索引可以使用。适用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 changebuffer 的维护代价。MySQL 是如何判断一行扫描数的？MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。redo log 和 binlog 的区别？为什么需要 redo log？redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash 后 内存
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据丢失的隐患，而 redo log 具备 crash safe 崩溃恢复 的能力。为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让innoDB 判断哪些数据已经刷盘，哪些数据还没有。但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交redo log 每次更新操作
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。redo log 内部结构是基于页的，记录了这个页的字段值变化，只要 crash 后读取redo log 进行重放，就可以恢复数据。这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备当数据库 crash 后，如何恢复未刷盘的数据到内存中？根据 redo log 和 binlog 的两阶段提交，未持久化的数据分为几种情况：change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从binlog 恢复 redo log，再从 redo log 恢复 change buffer。change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。redo log 写入方式？redo log 包括两部分内容，分别是内存中的日志缓冲(re
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: do log buffer)和磁盘上的日志文件(redo log file)。MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是 WAL。可以发现，redo log buffer 写入到 redo log file，是经过 OS buffer 中转的。其实可以通过参数 innodb_flush_log_at_trx_commit 进行配置，参数值含义如下：0：称为延迟写，事务提交时不会将 redo log buffer 中日志写入到 OS buffer，而是每秒写入 OS buffer 并调用写入到 redo log file 中。1：称为实时写，实时刷”，事务每次提交都会将 redo log buffer 中的日志写入 OS buffer 并保存到 redo log file 中。2： 称为实时写，延迟刷。每次事务提交写入到 OS buffer，然后是每秒将日志写入到 redo log file。
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: redo log 的执行流程?MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server层。MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。InnoDB 存储引擎层将 a修改为 1的这个操作记录到内存中。记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。binlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗?binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。但是
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。什么是两阶段提交？MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog，这就是"两阶段提交"。而两阶段提交就是让这两个状态保持逻辑上的一致。redolog 用于恢复主机故障时的未更新的物理数据，binlog 用于备份操作。两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。为什么需要两阶段提交呢?如果不用两阶段提交的话，可能会出现这样情况先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log备份恢复时，数据不一致。两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。在恢复数据时，redolog 状态为 com
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: mit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。MySQL 怎么知道 binlog 是完整的?一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event什么是 WAL 技术，有什么优点？WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过 redo log恢复，所以能够实现快速响应 SQL 语句redo log 日志格式redo log buffer (内存中)是由首尾相连的四个文件组成的，它们分别是：ib_logfile_1、ib_logfile_2、ib_logfile_3、ib_logfile_4。write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。chec
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: kpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。有了 redo log，当数据库发生宕机重启后，可通过 redo log 将未落盘的数据（check point 之后的数据）恢复，保证已经提交的事务记录不会丢失，这种能力称为 crash-safe。InnoDB 数据页结构一个数据页大致划分七个部分File Header：表示页的一些通用信息，占固定的 38 字节。page Header：表示数据页专有信息，占固定的 56 字节。inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的 26 字节。User Records：真正存储我们插入的数据，大小不固定。Free Space：页中尚未使用的部分，大小不固定。Page Directory：页中某些记录的相对位置，也就是各个槽对
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 应的记录在页面中的地址偏移量。File Trailer：用于检验页是否完整，占固定大小 8 字节。MySQL 是如何保证数据不丢失的？只要 redolog 和 binlog 保证持久化磁盘就能确保 MySQL 异常重启后回复数据在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。28、误删数据怎么办？DBA 的最核心的工作就是保证数据的完整性，先要做好预防，预防的话大概是通过这几个点：权限控制与分配(数据库和服务器权限)制作操作规范定期给开发进行培训搭建延迟备库做好 SQL 审计，只要是对线上数据有更改操作的语句(DML 和 DDL)都需要进行审核做好备份。备份的话又分为两个点 (1)如果数据量比较大，用物理备份xtrabackup。定期对数据库进行全量备份，也可以做增量备份。 (2)如果数据量较少，用 mysqldump 或者 mysqldumper。再利用 binlog 来恢复或者搭建主从的方式来恢复数据。 定期备份 binlog 文件也是很有必要的如果发
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 生了数据删除的操作，又可以从以下几个点来恢复:DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的myflash，也是一个不错的工具，本质都差不多，都是先解析 binlog event，然后在进行反转。把 delete 反转为 insert，insert 反转为 delete，update 前后 image 对调。所以必须设置 binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上。DDL 语句误操作(truncate 和 drop)，由于 DDL 语句不管 binlog_format 是 row还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长rm 删除：使用备份跨机房，或者最好是跨城市保存。DDL（数据定义语言） DML（数据操纵语言） DQL（数据查询语言）drop、truncate 和 delete 的区别DELETE 语句执行删除的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。drop 语句将表所占用的空间全释放掉。在速度上，一般来说，drop> truncate > delete。如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大；如果想删除表，当然用 drop； 如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可；如果和事务有关，或者想触发 trigger，还是用 delete； 如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 W
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: eb、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。都说 InnoDB 好，那还要不要使用 MEMORY 引擎？内存表就是使用 memory 引擎创建的表为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉MySQL 是如何保证主备同步？主备关系的建立：一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从 binlog 文件 A的位置 P”开始同步，主库就从这个指定的位置开始往后发。而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。MySQL 主备切换流程：客户端读写都是直接访问 A，而节点 B是备库，只要将 A的更新都同步过来，到本地执行就可以保证数据是相同的。当需要切换的时候就
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 把节点换一下，A的节点 B的备库一个事务完整的同步过程：备库 B和主库 A建立来了长链接，主库 A内部专门线程用于维护了这个长链接。在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求 binlog 包括文件名和日志偏移量在备库 B上执行 start-slave 命令备库会启动两个线程：io_thread 和sql_thread 分别负责建立连接和读取中转日志进行解析执行备库读取主库传过来的 binlog 文件备库收到文件写到本地成为中转日志后来由于多线程复制方案的引入，sql_thread 演化成了多个线程什么是主备延迟主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有：有些部署条件下，备库所在机器的性能要比主库性能差。备库的压力较大。大事务，一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10分钟MySQL 的一主一备和一主多从有什么区别？在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上短时间提高 MySQL 性能的方法第一种方法：先处理掉那
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。 kill connection + id第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL选错了索引（force index）。InnoDB 为什么要用自增 ID 作为主键？自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。说一下 MySQL 的锁MySQL 在 server 层 和 存储引擎层 都运用了大量的锁MySQL server 层需要讲两种锁，第一种是 MDL(metadata lock) 元数据锁，第二种则 Table Lock 表锁。MDL 又名元数据锁，那么什么是元数据呢，任何描述数据库的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内容就是元数据，比如我们的表结构、库结构等都是元数据。那为什么需要 MDL 呢？主要解决两个问题：事务隔离问题；数据复制问题InnoDB 有五种表级锁：IS（意向读锁）；IX（意向写锁）；S（读）；X（写）；AUTO-INC在对表进行 select/insert/delete/update 语句时候不会加表级锁IS 和 IX 的作用是为了判断表中是否有已经被加锁的记录自增主键的保障就是有 AUTO-INC 锁，是语句级别的：为表的某个列添加AUTO_INCREMENT 属性，之后在插⼊记录时，可以不指定该列的值，系统会⾃动为它赋上单调递增的值。InnoDB 4 种行级锁RecordLock：记录锁GapLock：间隙锁解决幻读；前一次查询不存在的东西在下一次查询出现了，其实就是事务 A中的两次查询之间事务 B执行插入操作被事务 A感知了Next-KeyLock：锁住某条记录又想阻止其它事务在改记录前面的间隙插入新纪录InsertIntentionLock：插入意向锁;如果插入到同一行间隙中的多个事务未插入到间隙内的同一位置则无须等待行锁和表锁的抉择全表扫描用行级锁索引是一种能提高数据库查询效率的数据结构。它可
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 以比作一本字典的目录，可以帮你快速找到对应的记录。索引一般存储在磁盘的文件中，它是占用物理空间的。正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。数据结构维度B+树索引：所有数据存储在叶子节点，复杂度为 O(logn)，适合范围查询。哈希索引: 适合等值查询，检索效率高，一次到位。全文索引：MyISAM 和 InnoDB 中都支持使用全文索引，一般在文本类型char,text,varchar 类型上创建。R-Tree 索引: 用来对 GIS 数据类型创建 SPATIAL 索引物理存储维度聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。（Innodb 存储引擎）非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。（Innodb 存储引擎）逻辑维度主键索引：一种特殊的唯一索引，不允许有空值。普通索引：MySQL 中基本索引类型，允许空值和重复值。联合索引：多个字段创建的索引，使用时遵循最左前缀原则。唯一索引：索引列中的值必须是唯一的，但是允许为空值。空间索引：MySQL5.7 之后支持空间索引，在空间索引这方面遵循 Op
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: enGIS 几何数据模型规则索引什么时候会失效？查询条件包含 or，可能导致索引失效如果字段类型是字符串，where 时一定用引号括起来，否则索引失效like 通配符可能导致索引失效。联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。在索引列上使用 mysql 的内置函数，索引失效。对索引列运算（如，+、-、*、/），索引失效。索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。索引字段上使用 is null， is not null，可能导致索引失效。左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。mysql 估计使用全表扫描要比使用索引快,则不使用索引哪些场景不适合建立索引？数据量少的表，不适合加索引更新比较频繁的也不适合加索引区分度低的字段不适合加索引（如性别）where、group by、order by 等后面没有使用到的字段，不需要建立索引已经有冗余的索引的情况（比如已经有 a,b 的联合索引，不需要再单独建立 a索引）为什么不是一般二叉树？如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找 树来说，查找效率更稳定，总体的查
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 找速度也更快。为什么不是平衡二叉树呢？我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作 为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说 的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果 是 B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数 就降下来啦，查询效率就快啦。那为什么不是 B 树而是 B+树呢？B+树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储 键值，也会存储数据。innodb 中页的默认大小是 16KB，如果不存储数据，那 么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就 会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数有会再次减少，数据查 询的效率也会更快。B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链 表连着的。那么 B+树使得范围查找，排序查找，分组查找以及去重查找变得 异常简单。一次 B+树索引树查找过程：select * from Temployee where age=32;这条 SQL 查询语句执行大概流程是这样的：搜索 idx_age 索引
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 树，将磁盘块 1加载到内存，由于 32<43,搜索左路分支，到磁盘寻址磁盘块 2。将磁盘块 2加载到内存中，由于 32<36,搜索左路分支，到磁盘寻址磁盘块 4。将磁盘块 4加载到内存中，在内存继续遍历，找到 age=32 的记录，取得 id = 400.拿到 id=400 后，回到 id 主键索引树。搜索 id 主键索引树，将磁盘块 1加载到内存，因为 300<400<500,所以在选择中间分支，到磁盘寻址磁盘块 3。虽然在磁盘块 3，找到了 id=400，但是它不是叶子节点，所以会继续往下找。到磁盘寻址磁盘块 8。将磁盘块 8加载内存，在内存遍历，找到 id=400 的记录，拿到 R4 这一行的数据，好的，大功告成。什么是回表？如何减少回表？当查询的数据在索引树中，找不到的时候，需要回到主键索引树中去获取，这个过程叫做回表。比如在第 6小节中，使用的查询 SQLselect * from Temployee where age=32;需要查询所有列的数据，idx_age 普通索引不能满足，需要拿到主键 id 的值后，再回到 id 主键索引查找获取，这个过程就是回表。什么是覆盖索引？如果我们查询 SQL 的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  select * 修改为 select id, age 的话，其实是不需要回表的。因为 id 和 age 的值，都在 idx_age 索引树的叶子节点上，这就涉及到覆盖索引的知识点了。覆盖索引是 select 的数据列只用从索引中就能够取得，不必回表，换句话说，查询列要被所建的索引覆盖聊聊索引的最左前缀原则、索引的最左前缀原则，可以是联合索引的最左 N个字段。比如你建立一个组合索引（a,b,c），其实可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。索引下推了解过吗？什么是索引下推select * from employee where name like '小%' and age=28 and sex='0';其中，name 和 age 为联合索引（idx_name_age）。如果是Mysql5.6之前，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键 id，然后回表找出数据行，再去对比年龄和性别等其他字段。有些朋友可能觉得奇怪，idx_name_age（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄 age 
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。因此，MySQL5.6 版本之后，选出包含“小”字后，顺表过滤 age=28大表如何添加索引？如果一张表数据量级是千万级别以上的，那么，如何给这张表添加索引？我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：先创建一张跟原表 A数据结构相同的新表 B。在新表 B添加需要加上的新索引。把原表 A数据导到新表 Brename 新表 B为原表的表名 A，原表 A换别的表名Hash 索引和 B+树区别是什么？你在设计索引是怎么抉择的？B+树可以进行范围查询，Hash 索引不能。B+树支持联合索引的最左侧原则，Hash 索引不支持。B+树支持 order by 排序，Hash 索引不支持。Hash 索引在等值查询上比 B+树效率更高。（但是索引列的重复值很多的话，Hash冲突，效率降低）。B+树使用 like 进行模糊查询的时候，like 后面（比如%开头）的话可以起到优化的作用，Hash 索引根
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 本无法进行模糊查询。索引有哪些优缺点？优点：索引可以加快数据查询速度，减少查询时间唯一索引可以保证数据库表中每一行的数据的唯一性缺点：创建索引和维护索引要耗费时间索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间以表中的数据进行增、删、改的时候，索引也要动态的维护。聚簇索引与非聚簇索引的区别聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它表示索引结构和数据一起存放的索引。非聚集索引是索引结构和数据分开存放的索引。接下来，我们分不同存存储引擎去聊哈~在 MySQL 的 InnoDB 存储引擎中， 聚簇索引与非聚簇索引最大的区别，在于叶节点是否存放一整行记录。聚簇索引叶子节点存储了一整行记录，而非聚簇索引叶子节点存储的是主键信息，因此，一般非聚簇索引还需要回表查询。一个表中只能拥有一个聚集索引（因为一般聚簇索引就是主键索引），而非聚集索引一个表则可以存在多个。一般来说，相对于非聚簇索引，聚簇索引查询效率更高，因为不用回表。而在 MyISM 存储引擎中，它的主键索引，普通索引都是非聚簇索引，因为数据和索引是分开的，叶子节点都使用一个地址指向真正的表数据。Nginx什么是 Ng
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: inx？Nginx 是一个 轻量级/高性能的反向代理 Web 服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。他实现非常高效的反向代理、负载平衡，他可以处理 2-3万并发连接数，官方监测能支持 5万并发，现在中国使用 nginx 网站用户有很多，例如：新浪、网易、 腾讯等。Nginx 怎么处理请求的？首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的 Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与 nginx 进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建nginx 对连接的封装，即 ngx
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: _connection_t 结构体。接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。Nginx 是如何实现高并发的？如果一个 server 采用一个进程(或者线程)负责一个 request 的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。而 Nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。每进来一个 request ，会有一个 worker 进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发 request ，并等待请求返回。那么，这个处理的 worker 不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有 request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker 才会来接手
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，这个 request 才会接着往下走。这就是为什么说，Nginx 基于事件模型。由于 web server 的工作性质决定了每个 request 的大部份生命都是在网络传输中，实际上花费在 server 机器上的时间片不多。这是几个进程就解决高并发的秘密所在。即：webserver 刚好属于网络 IO 密集型应用，不算是计算密集型。异步，非阻塞，使用 epoll ，和大量细节处的优化。也正是 Nginx 之所以然的技术基石。什么是正向代理？一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的 OpenVPN 等等。什么是反向代理？反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet 上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 反向代理总结就一句话：代理端代理的是服务端。反向代理服务器的优点是什么?反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。cookie 和 session 区别？共同：存放用户信息。存放的形式：key-value 格式 变量和变量内容键值对。区别：cookie存放在客户端浏览器每个域名对应一个 cookie，不能跨跃域名访问其他 cookie用户可以查看或修改 cookiehttp 响应报文里面给你浏览器设置钥匙（用于打开浏览器上锁头）session:存放在服务器（文件，数据库，redis）存放敏感信息锁头为什么 Nginx 不使用多线程？Apache: 创建多个进程或线程，而每个进程或线程都会为其分配 cpu 和内存（线程要比进程小的多，所以 worker 支持比 perfork 高的并发），并发过大会榨干服务器资源。Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置 Nginx 主进程的工作进程的数量）(epoll)，不会为每个请求分配 cpu 和内存资源，节省了大量资源，同时也减少了大量的 
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: CPU 的上下文切换。所以才使得 Nginx 支持更高的并发。nginx 和 apache 的区别轻量级，同样起 web 服务，比 apache 占用更少的内存和资源。抗并发，nginx 处理请求是异步非阻塞的，而 apache 则是阻塞性的，在高并发下 nginx 能保持低资源，低消耗高性能。高度模块化的设计，编写模块相对简单。最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，nginx是异步的，多个连接可以对应一个进程什么是动态资源、静态资源分离？动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。为什么要做动、静分离？在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。什么叫 CDN 服务？CDN ，即内容分发网络。其目的是，通过在现有的 Internet 中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务Nginx 怎么做的动静分离？只需要指定路径对应的目录。locatio
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n/可以使用正则表达式匹配。并指定对应的硬盘中的目录Nginx 负载均衡的算法怎么实现的?策略有哪些?为了避免服务器崩溃，大家会通过负载均衡的方式来分担服务器压力。将对台服务器组成一个集群，当用户访问时，先访问到一个转发服务器，再由转发服务器将访问分发到压力更小的服务器。Nginx 负载均衡实现的策略有以下种：1 .轮询(默认)每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。upstream backserver {server 192.168.0.12;server 192.168.0.13;}2. 权重 weightweight 的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。# 权重越高，在被访问的概率越大，如上例，分别是 20%，80%。upstream backserver {server 192.168.0.12 weight=2;server 192.168.0.13 weight=8;}3. ip_hash( IP 绑定)每个请求按访问 IP 的哈希结果分配，
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 使来自同一个 IP 的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的 session 共享问题upstream backserver {ip_hash;server 192.168.0.12:88;server 192.168.0.13:80;}Nginx 虚拟主机怎么配置?1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台3、基于 ip 的虚拟主机。location 的作用是什么？location 指令的作用是根据用户请求的 URI 来执行不同的应用，也就是根据用户请求的网站 URL 进行匹配，匹配成功即进行相关的操作限流怎么做的？Nginx 限流就是限制用户请求速度，防止服务器受不了限流有 3种正常限制访问频率（正常流量）突发限制访问频率（突发流量）限制并发连接数Nginx 的限流都是基于漏桶流算法漏桶流算法和令牌桶算法知道？漏桶算法漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 续进入的水直接溢出（拒绝请求），以此实现限流。令牌桶算法令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。Nginx 配置高可用性怎么配置？当上游服务器(真实访问服务器)，一旦出现故障或者是没有及时相应的话，应该直接轮训到下一台服务器，保证服务器的高可用生产中如何设置 worker 进程的数量呢？在有多个 cpu 的情况下，可以设置多个 worker，worker 进程的数量可以设置到和 cpu 的核心数一样多，如果在单个 cpu 上起多个 worker 进程，那么操作系统会在多个 worker 之间进行调度，这种情况会降低系统性能，如果只有一个 cpu，那么只启动一个 worker 进程就可以了。Java 基础八股文Java 语言具有哪些特点？Java 为纯面向对象的语言。它能够直接反应现实生活中的
2025-08-11 10:55:52.765 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象。具有平台无关性。Java 利用 Java 虚拟机运行字节码，无论是在 Windows、Linux还是 MacOS 等其它平台对 Java 程序进行编译，编译后的程序可在其它平台运行。Java 为解释型语言，编译器把 Java 代码编译成平台无关的中间代码，然后在JVM 上解释运行，具有很好的可移植性。Java 提供了很多内置类库。如对多线程支持，对网络通信支持，最重要的一点是提供了垃圾回收器。Java 具有较好的安全性和健壮性。Java 提供了异常处理和垃圾回收机制，去除了 C++中难以理解的指针特性JDK 与 JRE 有什么区别？JDK：Java 开发工具包（Java Development Kit），提供了 Java 的开发环境和运行环境。JRE：Java 运行环境(Java Runtime Environment)，提供了 Java 运行所需的环境。JDK 包含了 JRE。如果只运行 Java 程序，安装 JRE 即可。要编写 Java 程序需安装 JDK简述 Java 基本数据类型byte: 占用 1 个字节，取值范围-128 ~ 127short: 占用 2 个字节，取值范围-215 ~ 21
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 5-1int：占用 4 个字节，取值范围-231 ~ 231-1long：占用 8 个字节float：占用 4 个字节double：占用 8 个字节char: 占用 2 个字节boolean：占用大小根据实现虚拟机不同有所差异简述自动装箱拆箱对于 Java 基本数据类型，均对应一个包装类。装箱就是自动将基本数据类型转换为包装器类型，如 int->Integer拆箱就是自动将包装器类型转换为基本数据类型，如 Integer->int简述 Java 访问修饰符default: 默认访问修饰符，在同一包内可见private: 在同一类内可见，不能修饰类protected : 对同一包内的类和所有子类可见，不能修饰类public: 对所有类可见构造方法、成员变量初始化以及静态成员变量三者的初始化顺序？先后顺序：静态成员变量、成员变量、构造方法。详细的先后顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。面向对象的三大特性？继承：对象的一个新类可以从现有的类中派生，派生类可以从它的基类那继承方法和实例变量，且
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 派生类可以修改或新增新的方法使之更适合特殊的需求。封装：将客观事物抽象成类，每个类可以把自身数据和方法只让可信的类或对象操作，对不可信的进行信息隐藏。多态：允许不同类的对象对同一消息作出响应。不同对象调用相同方法即使参数也相同，最终表现行为是不一样的为什么 Java 语言不支持多重继承？为了程序的结构能够更加清晰从而便于维护。假设 Java 语言支持多重继承，类C 继承自类 A 和类 B，如果类 A 和 B 都有自定义的成员方法 f()，那么当代码中调用类 C 的 f() 会产生二义性。Java 语言通过实现多个接口间接支持多重继承，接口由于只包含方法定义，不能有方法的实现，类 C 继承接口 A 与接口 B 时即使它们都有方法 f()，也不能直接调用方法，需实现具体的 f()方法才能调用，不会产生二义性。多重继承会使类型转换、构造方法的调用顺序变得复杂，会影响到性能Java 提供的多态机制？Java 提供了两种用于多态的机制，分别是重载与覆盖(重写)。重载：重载是指同一个类中有多个同名的方法，但这些方法有不同的参数，在编译期间就可以确定调用哪个方法。覆盖：覆盖是指派生类重写基类的方法，使用基类指向其子类的实例
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象，或接口的引用变量指向其实现类的实例对象，在程序调用的运行期根据引用变量所指的具体实例对象调用正在运行的那个对象的方法，即需要到运行期才能确定调用哪个方法重载与覆盖的区别？覆盖是父类与子类之间的关系，是垂直关系；重载是同一类中方法之间的关系，是水平关系。覆盖只能由一个方法或一对方法产生关系；重载是多个方法之间的关系。覆盖要求参数列表相同；重载要求参数列表不同。覆盖中，调用方法体是根据对象的类型来决定的，而重载是根据调用时实参表与形参表来对应选择方法体。重载方法可以改变返回值的类型，覆盖方法不能改变返回值的类型。接口和抽象类的相同点和不同点？相同点:都不能被实例化。接口的实现类或抽象类的子类需实现接口或抽象类中相应的方法才能被实例化。不同点：接口只能有方法定义，不能有方法的实现，而抽象类可以有方法的定义与实现。实现接口的关键字为 implements，继承抽象类的关键字为 extends。一个类可以实现多个接口，只能继承一个抽象类。当子类和父类之间存在逻辑上的层次结构，推荐使用抽象类，有利于功能的累积。当功能不需要，希望支持差别较大的两个或更多对象间的特定交互行为，推荐使用接口。使用接口能降低软件系统的耦合
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度，便于日后维护或添加删除方法Java 语言中关键字 static 的作用是什么？static 的主要作用有两个：为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。具体而言 static 又可分为 4 种使用方式：修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用。修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法。修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次。修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方法为什么要把 String 设计为不可变？节省空间：字符串常量存储在 JVM 的字符串池中可以被用户共享。提高效率：String 可以被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。安全：String 常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改简述 String/StringBuffer 与 StringBuilderString 类采用利用 final 修饰的字符数组进行字符串保存，因此不可变。如果对 String 类型对象修改，需要新建对象，将老字符和新增加的字符一并存进去。StringBuilder，采用无 final 修饰的字符数组进行保存，因此可变。但线程不安全。StringBuffer，采用无 final 修饰的字符数组进行保存，可理解为实现线程安全的 StringBuilder。判等运算符==与 equals 的区别？== 比较的是引用，equals 比较的是内容。如果变量是基础数据类型，== 用于比较其对应值是否相等。如果变量指向的是对象，== 用于比较两个对象是否指向同一块存储空间。equals 是 Object 类提供的方法之一，每个 J
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 类都继承自 Object 类，所以每个对象都具有 equals 这个方法。Object 类中定义的 equals 方法内部是直接调用 == 比较对象的。但通过覆盖的方法可以让它不是比较引用而是比较数据内容简述 Java 异常的分类Java 异常分为 Error（程序无法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。Error 常见的有 StackOverFlowError、OutOfMemoryError 等等。Exception 可分为运行时异常和非运行时异常。对于运行时异常，可以利用 trycatch 的方式进行处理，也可以不处理。对于非运行时异常，必须处理，不处理的话程序无法通过编译final、finally 和 finalize 的区别是什么？final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承。finally 作为异常处理的一部分，只能在 try/catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下。finalize 是 Object 类的一个方法
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存简述 Java 中 Class 对象java 中对象可以分为实例对象和 Class 对象，每一个类都有一个 Class 对象，其包含了与该类有关的信息。获取 Class 对象的方法：Class.forName(“类的全限定名”)实例对象.getClass()类名.classJava 反射机制是什么？Java 反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得 Java 具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射 API。简述 Java 序列化与反序列化的实现序列化：将 java 对象转化为字节序列，由此可以通过网络对象进行传输。反序列化：将字节序列转化为 java 对象。具体实现：实现 Serializable 接口，或实现 Externalizable 接口中的writeExte
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: rnal()与 readExternal()方法。简述 Java 的 ListList 是一个有序队列，在 Java 中有两种实现方式:ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。Java 中线程安全的基本数据结构有哪些HashTable: 哈希表的线程安全版，效率低ConcurrentHashMap：哈希表的线程安全版，效率高，用于替代 HashTableVector：线程安全版 ArraylistStack：线程安全版栈BlockingQueue 及其子类：线程安全版队列简述 Java 的 SetSet 即集合，该数据结构不允许元素重复且无序。Java 对 Set 有三种实现方式：HashSet 通过 HashMap 实现，HashMap 的 Key 即 HashSet 存储的元素，Value系统自定义一个名为 PRESENT 的 Object 类型常量。判断元素是否相同时，先比较 hashCode，相同后再利用 equ
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: als 比较，查询 O(1)LinkedHashSet 继承自 HashSet，通过 LinkedHashMap 实现，使用双向链表维护元素插入顺序。TreeSet 通过 TreeMap 实现的，底层数据结构是红黑树，添加元素到集合时按照比较规则将其插入合适的位置，保证插入后的集合仍然有序。查询 O(logn)简述 Java 的 HashMapJDK8 之前底层实现是数组 + 链表，JDK8 改为数组 + 链表/红黑树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因子 loadFactor。HashMap 中数据以键值对的形式存在，键对应的 hash 值用来计算数组下标，如果两个元素 key 的 hash 值一样，就会发生哈希冲突，被放到同一个链表上。table 数组记录 HashMap 的数据，每个下标对应一条链表，所有哈希冲突的数据都会被存放到同一条链表，Node/Entry 节点包含四个成员变量：key、value、next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红黑树为何 HashMap 线程不安全在 JDK1.7 中，HashMap 采用头插法插入元素
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，因此并发情况下会导致环形链表，产生死循环。虽然 JDK1.8 采用了尾插法解决了这个问题，但是并发下的 put 操作也会使前一个 key 被后一个 key 覆盖。由于 HashMap 有扩容机制存在，也存在 A 线程进行扩容后，B 线程执行 get 方法出现失误的情况。简述 Java 的 TreeMapTreeMap 是底层利用红黑树实现的 Map 结构，底层实现是一棵平衡的排序二叉树，由于红黑树的插入、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表无法提供键值对的有序输出，红黑树可以按照键的值的大小有序输出ArrayList、Vector 和 LinkedList 有什么共同点与区别？ArrayList、Vector 和 LinkedList 都是可伸缩的数组，即可以动态改变长度的数组。ArrayList 和 Vector 都是基于存储元素的 Object[] array 来实现的，它们会在内存中开辟一块连续的空间来存储，支持下标、索引访问。但在涉及插入元素时可能需要移动容器中的元素，插入效率较低。当存储元素超过容器的初始化容量大小，ArrayList 与 Vector 均会进
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行扩容。Vector 是线程安全的，其大部分方法是直接或间接同步的。ArrayList 不是线程安全的，其方法不具有同步性质。LinkedList 也不是线程安全的。LinkedList 采用双向列表实现，对数据索引需要从头开始遍历，因此随机访问效率较低，但在插入元素的时候不需要对数据进行移动，插入效率较高HashMap 和 Hashtable 有什么区别？HashMap 是 Hashtable 的轻量级实现，HashMap 允许 key 和 value 为 null，但最多允许一条记录的 key 为 null.而 HashTable 不允许。HashTable 中的方法是线程安全的，而 HashMap 不是。在多线程访问 HashMap需要提供额外的同步机制。Hashtable 使用 Enumeration 进行遍历，HashMap 使用 Iterator 进行遍历。如何决定使用 HashMap 还是 TreeMap?如果对 Map 进行插入、删除或定位一个元素的操作更频繁，HashMap 是更好的选择。如果需要对 key 集合进行有序的遍历，TreeMap 是更好的选择Collection 和 Colle
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ctions 有什么区别？Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。Collections 是一个包装类，包含了很多静态方法、不能被实例化，而是作为工具类使用，比如提供的排序方法：Collections.sort(list);提供的反转方法：Collections.reverse(list)。Java 并发编程1.并行跟并发有什么区别？并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。你是如何理解线程安全的？如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。可以从三个要素来确保线程安全：1 、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态2 、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行2.说说进程和线程的区别？进
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤：线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中3. 说说线程有几种创建方式？分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象。还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。调用 start 方法时会执行 run 方法，那怎么不直接调用 run 方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。说说 wait 方法和 notify 方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到：线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法、当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。如果调用的是 notifyAll 方法，会唤醒所
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有在这个共享变量上调用 wait 系列方法而被挂起的线程。说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。6.线程有几种状态？6 种。new 代表线程被创建但未启动；runnable 代表线程处于就绪或正在运行状态，由操作系统调度；blocked 代表线程被阻塞，等待获取锁；waiting 代表线程等待其他线程的通知或中断；timed_waiting 代表线程会等待一段时间，超时后自动恢复；terminated 代表线程执行完毕，生命周期结束。什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程使用完时间片后，就会让出 CPU 让其他线程占用。守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM内部，同时还启动了很多守护线程，比如垃圾回收线程。守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。sleep() 方法专属于 Thread 类。wait() 方法专属于 Object 类。waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。而当线程执行 wait 方法时，它会释放持有的对象锁，
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 因此其他线程也有机会获取该对象的锁。有个 int 的变量为 0，十个线程轮流对其进行++操作（循环 10000 次），结果大于 10 万还是小于等于 10 万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。读取变量的值。将读取到的值加 1。将结果写回变量。这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。详细解释下。多个线程在并发执行 ++ 操作时，可能出现以下竞态条件：线程 1 读取变量值为 0。线程 2 也读取变量值为 0。线程 1 进行加法运算并将结果 1 写回变量。线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。.ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程隔离。在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry数组，key 是 Thread
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Local 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value设置为 null，这样可以在很大程度上避免内存泄漏。ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。3、Map 的大小由 ThreadLocal 对象的多少决定。15.ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove()
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法释放内存空间。那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。ThreadLocalMap 的源码看过吗？有研究过。ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位：如果 i 位置被占用，尝试 i+1。如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。如果到达数组末尾，则回到数组头部，继续寻找空位。为什么要用线性探测法而不是 HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。拉链法还需要单独维护一个链表，甚至红黑树，
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不适合 ThreadLocal 这种场景ThreadLocalMap 扩容机制了解吗？了解。与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。父线程能用 ThreadLocal 给子线程传值吗？不能。因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。可以使用 InheritableThreadLocal 来解决这个问题。InheritableThreadLocal 的原理了解吗？了解。在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap：普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 newThread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有inheritableThreadLocals，
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 如果有，就会拷贝 InheritableThreadLocal 变量到子线程：说一下你对 Java 内存模型的理解？Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突volatile 了解吗？了解。第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为：StoreStore 屏障可以禁止普通写操作与 volatile 写操作的重排StoreLoad 屏障会禁止 volatile 写与 volatile 读重排LoadLoad 屏障会禁止 volatile 读与后续普通读操作重排LoadStore 屏障会禁止 volatile 读与后续普通写操作重排volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。锁synchronized 用过吗？用过，频率还
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 很高。synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象this。synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步：前者表示线程正在尝试获取 lock 对象的 Monitor；后者表示线程执行完了同步代码块，正在释放锁。你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——MarkWord，用于存储锁的状态，以及 Monito
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: r 对象的指针。synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。synchronized 怎么保证可见性？通过两步操作：加锁时，线程必须从主内存读取最新数据。释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 标记为未持有状态，并清除线程 ID 信息。synchronized 锁升级了解吗？JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。了解 synchronized 四种锁状态吗？了解。①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。未持有锁的线程通过 CAS 自旋等待锁释放④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，轻量级锁就会升级为重量级锁。此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。随着 JDK 版本的更新，synchronized 的性能得到了极大的优化：①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销详细解释一下：①、从无锁到偏向锁：当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。②、偏向锁的轻量级锁：进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。③、轻量级锁到重量级锁：轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。30.synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。synchronized 可以自动加锁和解锁，ReentrantLoc
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: k 需要手动 lock() 和 unlock()。并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为：ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制AQS 了解多少？AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为ReentrantLock 等类提供底层支持。AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。内部通过一个计数器 state 来跟踪锁的状态和
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。非公平锁和公平锁有什么不同？两句话回答：公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序CAS 了解多少？CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。在 CAS 中，有三个值：V：要更新的变量(var)E：预期值(expected)N：新值(new)先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 当前线程就放弃更新。这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe类实现的。怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS会误认为这个值没有发生变化。可以使用版本号/时间戳的方式来解决 ABA 问题。比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新Java 有哪些保证原子性的方法？比如说以 Atomic 开头的原子类，synchroni
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: zed 关键字，ReentrantLock 锁等原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有AtomicInteger、AtomicLong、AtomicReference 等。线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁R2；线程 2 持有锁 R2，等待锁 R1。第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请聊聊线程同步和互斥？
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: （补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程A 先执行，线程 B 再执行。互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费CPU 资源。聊聊悲观锁和乐观锁？（补充）悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。乐观锁认为冲突不
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。CountDownLatch 了解吗？CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。场景题：假如要查 10万多条数据，用线程池分成 20 个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。第三步，主线程调用 await() 方法，等待所有线程执行完毕。CyclicBarrier 和 CountDownLatch 有什么
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。说一下 JDK 8 中 ConcurrentHashMap 的实现原理？JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS的方式初始化，以确保只有一个线程在初始化数组。第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为synchronized 代码块来插入节点。插入的过程中会判断桶的哈希是否小于 0（f.hash >= 0），小于 0 说明是红黑树，大于等于 0 说明是链表。这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash值固定为 -2。第三步，如果链表长度超过 8，转换为红黑树。第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了ReentrantL
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ock，这样可以保证每个 Segment 都可以独立地加锁。而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized关键字。而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。比如说 put 的时候优先使用 CAS 尝试
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 插入，如果失败再使用 synchronized 代码块加锁。get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。说一下线程池的工作流程？可以简单总结为：任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。第一步，线程池通过 submit() 提交任务。第二步，线程池会先创建核心线程来执行任务。第三步，如果核心线程都在忙，任务会被放入任务队列中。第四步，如果任务
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。另外一版回答。第一步，创建线程池。第二步，调用线程池的 execute()方法，准备执行任务。如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务；如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列；如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务；如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。①、corePoolSize：核心线程数，长期存活，执行任务的主力。②、maxim
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: umPoolSize：线程池允许的最大线程数。③、workQueue：任务队列，存储等待执行的任务。④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。线程池的拒绝策略有哪些？AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute方法的线程。DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedB
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: lockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列SynchronousQueue。1 、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。2 、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是Integer.MAX_VALUE，几乎相当于一个无界队列。3 、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务4 、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。5 、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。submit 有返回值，适用于需要获取结果或
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 处理异常的场景。线程池怎么关闭知道吗？可以调用线程池的 shutdown 或 shutdownNow方法来关闭线程池。shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？1 、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。2 、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。有哪几种常见的线程池？主要有四种：固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等缓存线程池 Executors.newCach
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: edThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。能说一下四种常见线程池的原理吗？说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 & 周期性任务执行，使用DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate()方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 延迟执行。缺点是，如果任务执行时间 > 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。状态由 RUNNING→ SHUTDOWN→ STOP → TIDYING → TERMINATED 依次流转。RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，比如说setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点：第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中JVMJVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件；程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高说说 JVM 的组织架构（补充）JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器能说一下 JVM 的内存区域吗？按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 C/C++编写。在本地方法栈中，主要存放了 native
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。详细说明一下：比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。介绍一下方法区？方法区并不真实存在，属于 J
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 虚拟机规范中的一个逻辑概念，用于存储已被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的GC 分代年龄等信息。最后，JVM 会执行构造方法 <init> 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。new 对象时，堆会发生抢占吗？new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存，两个线程就发生了抢占JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。能说一下对象的内存布局吗？对象在内存中包括三部分：对象头、实例数据和对齐填充说说对象头的作用？对象头是对象存储在内存中的元信息，包含了 Mark Word、类型指针等信息。对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。比如说，如果对象头 + 实例数据 = 14 字节，则需要填充 2 个字节，使总长度变为 16 字节为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 就会降低。new Object() 对象的内存大小是多少？一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16字节；如果开启了压缩指针，就是 12 字节。实例数据的大小取决于对象的成员变量和它们的类型。对于 new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。对象的引用大小了解吗？在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。、在直接指针访问中，引用直接存储对象的内存地址；对象的实
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例数据和类型信息都存储在堆中固定的内存区域。说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。对
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。STW 了解吗？了解。JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为 Stop TheWorld。简称 STW。如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：JVM 发出暂停信号；线程执行到安全点后，挂起自身并等待垃圾收集完成；垃圾回收器完成 GC 操作；线程恢复执行对象一定分配在堆中吗？不一定。默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程逃逸分析会带来什么好处？主要有三个。第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 无法被回收而导致的内存泄漏可能由哪些原因导致呢？静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。数据库、IO、Socket 等连接资源没有及时关闭；ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。有没有处理过内存泄漏问题？当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出StackOverflowError。讲讲 JVM 的垃圾回收机制（补充）垃圾回收就
2025-08-11 10:55:52.766 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩/整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。这也是 G1、CMS 等主流垃圾收集器使用的主要算法。什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失Java 中可作为 GC Roots 的引用有哪几种？所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种虚拟机栈中的引用（方法的参数、局部变量等）本地方法栈中 JNI 的引用类静态变量运行时常量池中的常量（String 或 Class 类型）finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。筛选的条件是对象是否有必要执行 finalize()方法。如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法说说标记-清除算法？标记-清除算法分为两个阶段：标记：标记所有需要回收的对
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象清除：回收所有被标记的对象优点是实现简单，缺点是回收过程中会产生内存碎片说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。缺点是浪费了一半的内存空间。说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。缺点是移动对象的成本比较高。说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 集。年轻代包含 Eden 区以及两个 Survivor 区Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生FULL gc 怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 < 新生代历次Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一次Full GC。知道哪些垃圾收集器？JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。说说 CMS 收集器？CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC能详细说一下 CMS 的垃圾收集过程吗？CMS 使用标记-清除算法进行垃圾收集，分 4 大步：初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。并发清除：清除未被标记的对象，回收它们占用的内存空间。三色标记法的工作流程：①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。②、并发标记（Concurrent Marking）：在此
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。说说 G1 收集器？G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。G1 收集器的运行过程大致可划分为这几个步骤：①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区域和老年代区域。选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。3 、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1会尽可能地在这个时间内完成垃圾回收。CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 序使用的，并释放这些内存以便重新使用。这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。G1 非常适合大内存、多核处理器的环境。工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间JVM 调优37. 用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。你一般都怎么用 jmap？我一般会使用 jmap -heap <pid> 查看堆内存摘要，包括新生代、老年代、元空间等。
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 或者使用 jmap -histo <pid> 查看对象分布了解哪些可视化的性能监控工具？JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？-Xms：初始堆大小-Xmx：最大堆大小-XX:NewSize=n：设置年轻代大小-XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 1/4-XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n=3表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 1/5做过 JVM 调优吗？JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。在项目运行期间，我会使用 JVisualVM
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等CPU 占用过高怎么排查？首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。接着，使用 jstack 命令查看对应进程的线程堆栈信息。然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程ID接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收排查的方法主要分为以下几步：第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。或者使用 jmap 
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。第二步，通过 jmap 命令 dump 出堆内存信息第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过-XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。调整为 6 的话，会减少
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象该怎么排查 Full GC 频繁问题？通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过-XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。或者将大对象拆分成小对象，减少大对象的创建。比如说分页。假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说ThreadLocal、数据库连接、IO 资源等。了解类的加载机制吗？（补充）JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。类加载过程：包括加载、验证、准备、解析和初始化等步骤。双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。以下是整理自网络的一些 JVM 调优实例：网站流量浏览量暴增后，网站反应页面响很慢问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。定位：为了确认推测的正确性，在线上通过 jstat -gc 指令 看到 JVM 进行 GC 次数频率非常高，GC 所占用的时间非常长，所以基本推断就是因为 GC 频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。解决
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁 GC，所以这里问题在于新生代内存太小，所以这里可以增加 JVM 内存就行了，所以初步从原来的 2G 内存增加到 16G 内存。第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次 GC 的时间变长从而导致间接性的卡顿。定位：还是通过 jstat -gc 指令 查看到 的确 FGC 次数并不是很高，但是花费在 FGC 上的时间是非常高的,根据 GC 日志 查看到单次 FGC 的时间有达到几十秒的。解决方案： 因为 JVM 默认使用的是 PS+PO 的组合，PS+PO 垃圾标记和收集阶段都是 STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次 GC 时间过长，所以需要更换并发类的收集器，因为当前的 JDK 版本为 1.7，所以最后选择 CMS 垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。公司的后台系统
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，偶发性的引发 OOM 异常，堆内存溢出。因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从 4G 调整到 8G。但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError 参数 获得堆内存的 dump 文件。VisualVM 对 堆 dump 文件进行分析，通过 VisualVM 查看到占用内存最大的对象是 String 对象，本来想跟踪着 String 对象找到其引用的地方，但 dump 文件太大，跟踪进去的时候总是卡死，而 String 对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成 excel，这个过程会产生大量的 String 对象为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和 EXCEL 对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出jvm.gc.time：每分钟的 GC 耗时在 1s 以内，500ms 以内尤佳jvm.gc.meantime：每次 YGC 耗时在 100ms 以内，50ms 以内尤佳jvm.fullgc.count：FGC 最多几小时 1次，1天不到 1次尤佳jvm.fullgc.time：每次 FGC 耗时在 1s 以内，500ms 以内尤佳// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pidSpringSpring 是什么？Spring 是一个 Java 后端开发框架，其最核心的作用是帮我们管理 Java 对象其最重要的特性就是 IoC，也就是控制反转。以前我们要使用一个对象时，都要自己先 new 出来。但有了 Spring 之后，我们只需要告诉 Spr
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ing 我们需要什么对象，它就会自动帮我们创建好并注入到 Spring 容器当中。比如我在一个Service 类里需要用到 Dao 对象，只需要加个 @Autowired 注解，Spring 就会自动把 Dao 对象注入到 Spring 容器当中，这样就不需要我们手动去管理这些对象之间的依赖关系了。另外，Spring 还提供了 AOP，也就是面向切面编程，在我们需要做一些通用功能的时候特别有用，比如说日志记录、权限校验、事务管理这些，我们不用在每个方法里都写重复的代码，直接用 AOP 就能统一处理。Spring 的生态也特别丰富，像 Spring Boot 能让我们快速搭建项目，Spring MVC能帮我们处理 web 请求，Spring Data 能帮我们简化数据库操作，Spring Cloud能帮我们做微服务架构等等Spring 有哪些特性？首先最核心的就是 IoC 控制反转和 DI 依赖注入。这个我前面也提到了，就是Spring 能帮我们管理对象的创建和依赖关系。第二个就是 AOP 面向切面编程。这个在我们处理一些横切关注点的时候特别有用，比如说我们要给某些 Controller 方法都加上权限控制，如
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果没有 AOP 的话，每个方法都要写一遍加权代码，维护起来很麻烦。用了 AOP 之后，我们只需要写一个切面类，定义好切点和通知，就能统一处理了。事务管理也是同样的道理，加个 @Transactional 注解就搞定了。简单说一下什么是 AOP 和 IoC？AOP 面向切面编程，简单点说就是把一些通用的功能从业务代码里抽取出来，统一处理。比如说技术派中的 @MdcDot 注解的作用是配合 AOP 在日志中加入 MDC信息，方便进行日志追踪。IoC 控制反转是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。Spring 有哪些模块呢？首先是 Spring Core 模块，这是整个 Spring 框架的基础，包含了 IoC 容器和依赖注入等核心功能。还有 Spring Beans 模块，负责 Bean 的配置和管理。这两个模块基本上是其他所有模块的基础，不管用 Spring 的哪个功能都会用到。然后是 Spring Context 上下文模块，它在 Core 的基础上提供了更多企业级的功能，比如国际化、事件传播、资源加载这些。ApplicationContext 就是在这
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个模块里面的。Spring AOP 模块提供了面向切面编程的支持，我们用的@Transactional、自定义切面这些都是基于这个模块。Web 开发方面，Spring Web 模块提供了基础的 Web 功能，Spring WebMVC 就是我们常用的 MVC 框架，用来处理 HTTP 请求和响应。现在还有 Spring WebFlux，支持响应式编程。还有一些其他的模块，比如 Spring Security 负责安全认证，Spring Batch 处理批处理任务等等。现在我们基本都是用 Spring Boot 来开发，它把这些模块都整合好了，用起来更方便。Spring 有哪些常用注解呢？Spring 的注解挺多的，我按照不同的功能分类来说一下平时用得最多的那些。首先是 Bean 管理相关的注解。@Component 是最基础的，用来标识一个类是Spring 组件。像 @Service、@Repository、@Controller 这些都是 @Component的特化版本，分别用在服务层、数据访问层和控制器层。依赖注入方面，@Autowired 是用得最多的，可以标注在字段、setter 方法或者构造方法上。
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: @Qualifier 在有多个同类型 Bean 的时候用来指定具体注入哪一个。@Resource 和 @Autowired 功能差不多，不过它是按名称注入的。配置相关的注解也很常用。@Configuration 标识配置类，@Bean 用来定义 Bean，@Value 用来注入配置文件中的属性值。我们项目里的数据库连接信息、Redis 配置这些都是用 @Value 来注入的。@PropertySource 用来指定配置文件的位置。@RequestMapping 及其变体@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 用来映射 HTTP 请求。@PathVariable 获取路径参数，@RequestParam 获取请求参数，@RequestBody 接收 JSON 数据。、AOP 相关的注解，@Aspect 定义切面，@Pointcut 定义切点，@Before、@After、@Around 这些定义通知类型不过我们用得最多的还是@Transactional，基本上 Service 层需要保证事务原子性的方法都会加上这个注解。Spring 用了哪些设计模
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 式？Spring 框架里面确实用了很多设计模式，我从平时工作中能观察到的几个来说说。首先是工厂模式，这个在 Spring 里用得非常多。BeanFactory 就是一个典型的工厂，它负责创建和管理所有的 Bean 对象。我们平时用的 ApplicationContext其实也是 BeanFactory 的一个实现。当我们通过 @Autowired 获取一个 Bean的时候，底层就是通过工厂模式来创建和获取对象的。单例模式也是 Spring 的默认行为。默认情况下，Spring 容器中的 Bean 都是单例的，整个应用中只会有一个实例。这样可以节省内存，提高性能。当然我们也可以通过 @Scope 注解来改变 Bean 的作用域，比如设置为 prototype 就是每次获取都创建新实例。代理模式在 AOP 中用得特别多。Spring AOP 的底层实现就是基于动态代理的，对于实现了接口的类用 JDK 动态代理，没有实现接口的类用 CGLIB 代理。比如我们用 @Transactional 注解的时候，Spring 会为我们的类创建一个代理对象，在方法执行前后添加事务处理逻辑。Spring 如何实现单例模式？传统的
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 单例模式是在类的内部控制只能创建一个实例，比如用 private 构造方法加 static getInstance() 这种方式。但是 Spring 的单例是容器级别的，同一个 Bean 在整个 Spring 容器中只会有一个实例。具体的实现机制是这样的：Spring 在启动的时候会把所有的 Bean 定义信息加载进来，然后在 DefaultSingletonBeanRegistry 这个类里面维护了一个叫singletonObjects 的 ConcurrentHashMap，这个 Map 就是用来存储单例 Bean的。key 是 Bean 的名称，value 就是 Bean 的实例对象。当我们第一次获取某个 Bean 的时候，Spring 会先检查 singletonObjects 这个 Map 里面有没有这个 Bean，如果没有就会创建一个新的实例，然后放到 Map 里面。后面再获取同一个 Bean 的时候，直接从 Map 里面取就行了，这样就保证了单例。还有一个细节就是 Spring 为了解决循环依赖的问题，还用了三级缓存。除了singletonObjects 这个一级缓存，还有 earlySingl
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: etonObjects 二级缓存和singletonFactories 三级缓存。这样即使有循环依赖，Spring 也能正确处理Spring 容器和 Web 容器之间的区别知道吗？（补充）首先从概念上来说，Spring 容器是一个 IoC 容器，主要负责管理 Java 对象的生命周期和依赖关系。而 Web 容器，比如 Tomcat、Jetty 这些，是用来运行 Web应用的容器，负责处理 HTTP 请求和响应，管理 Servlet 的生命周期。从功能上看，Spring 容器专注于业务逻辑层面的对象管理，比如我们的 Service、Dao、Controller 这些 Bean 都是由 Spring 容器来创建和管理的。而 Web 容器主要处理网络通信，比如接收 HTTP 请求、解析请求参数、调用相应的 Servlet，然后把响应返回给客户端在实际项目中，这两个容器是相辅相成的。我们的 Web 项目部署在 Tomcat 上的时候，Tomcat 会负责接收 HTTP 请求，然后把请求交给 DispatcherServlet处理，而 DispatcherServlet 又会去 Spring 容器中查找相应的 Cont
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: roller来处理业务逻辑。现在我们都用 Spring Boot 了，Spring Boot 内置了 Tomcat，把 Web 容器和Spring 容器都整合在一起了，我们只需要运行一个 jar 包就可以了说一说什么是 IoC？IoC 的全称是 Inversion of Control，也就是控制反转。这里的“控制”指的是对象创建和依赖关系管理的控制权。以前我们写代码的时候，如果 A 类需要用到 B 类，我们就在 A 类里面直接 new一个 B 对象出来，这样 A 类就控制了 B 类对象的创建。有了 IoC 之后，这个控制权就“反转”了，不再由 A 类来控制 B 对象的创建，而是交给外部的容器来管理。DI 和 IoC 的区别了解吗？IoC 的思想是把对象创建和依赖关系的控制权由业务代码转移给 Spring 容器。这是一个比较抽象的概念，告诉我们应该怎么去设计系统架构。而 DI，也就是依赖注入，它是实现 IoC 这种思想的具体技术手段。在 Spring 里，我们用 @Autowired 注解就是在使用 DI 的字段注入方式。为什么要使用 IoC 呢？在日常开发中，如果我们需要实现某一个功能，可能至少需要两个以上
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系能说一下 IoC 的实现机制吗？好的，Spring IoC 的实现机制还是比较复杂的，我尽量用比较通俗的方式来解释一下整个流程。第一步是加载 Bean 的定义信息。Spring 会扫描我们配置的包路径，找到所有标注了 @Component、@Service、@Repository 这些注解的类，然后把这些类的元信息封装成 BeanDefinition 对象。第二步是 Bean 工厂的准备。Spring 会创建一个 DefaultListableBeanFactory作为 Bean 工厂来负责 Bean 的创建和管理。第三步是 Bean 的实例化和初始化。这个过程比较复杂，Spring 会根据BeanDefinition 来创建 Bean 实例。对于单例 Bean，Spring 会先检查缓存中是否已经存在，如果不存在就创建新实例。创建实例的时候会通过反射调用构造方法，然后进行属性注入，最后执行初始化回调方法。依赖注入的实现主
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 要是通过反射来完成的。比如我们用 @Autowired 标注了一个字段，Spring 在创建 Bean 的时候会扫描这个字段，然后从容器中找到对应类型的 Bean，通过反射的方式设置到这个字段上。你是怎么理解 Spring IoC 的？IoC 本质上一个超级工厂，这个工厂的产品就是各种 Bean 对象。我们通过 @Component、@Service 这些注解告诉工厂：“我要生产什么样的产品，这个产品有什么特性，需要什么原材料”。然后工厂里各种生产线，在 Spring 中就是各种 BeanPostProcessor。比如AutowiredAnnotationBeanPostProcessor 专门负责处理 @Autowired 注解。工厂里还有各种缓存机制用来存放产品，比如说 singletonObjects 是成品仓库，存放完工的单例 Bean；earlySingletonObjects 是半成品仓库，用来解决循环依赖问题。说说 BeanFactory 和 ApplicantContext 的区别?BeanFactory 算是 Spring 的“心脏”，而 ApplicantContext 可以说是Spri
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ng 的完整“身躯”。BeanFactory 提供了最基本的 IoC 能力。它就像是一个 Bean 工厂，负责 Bean的创建和管理。他采用的是懒加载的方式，也就是说只有当我们真正去获取某个Bean 的时候，它才会去创建这个 Bean。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上扩展了很多企业级的功能。它不仅包含了 BeanFactory 的所有功能，还提供了国际化支持、事件发布机制、AOP、JDBC、ORM 框架集成等等。ApplicationContext 采用的是饿加载的方式，容器启动的时候就会把所有的单例 Bean 都创建好，虽然这样会导致启动时间长一点，但运行时性能更好。另外一个重要的区别是生命周期管理。ApplicationContext 会自动调用 Bean的初始化和销毁方法，而 BeanFactory 需要我们手动管理。在 Spring Boot 项目中，我们可以通过 @Autowired 注入 ApplicationContext，或者通过实现 ApplicationContextAware 接口来获取 Applicatio
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: nContext。项目启动时 Spring 的 IoC 会做什么？第一件事是扫描和注册 Bean。IoC 容器会根据我们的配置，比如@ComponentScan 指定的包路径，去扫描所有标注了 @Component、@Service、@Controller 这些注解的类。然后把这些类的元信息包装成 BeanDefinition 对象，注册到容器的 BeanDefinitionRegistry 中。这个阶段只是收集信息，还没有真正创建对象。第二件事是 Bean 的实例化和注入。这是最核心的过程，IoC 容器会按照依赖关系的顺序开始创建 Bean 实例。对于单例 Bean，容器会通过反射调用构造方法创建实例，然后进行属性注入，最后执行初始化回调方法在依赖注入时，容器会根据 @Autowired、@Resource 这些注解，把相应的依赖对象注入到目标 Bean 中。比如 UserService 需要 UserDao，容器就会把UserDao 的实例注入到 UserService 中。说说 Spring 的 Bean 实例化方式？Spring 提供了 4 种方式来实例化 Bean，以满足不同场景下的需求第一种是通过
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法实例化，这是最常用的方式。当我们用 @Component、@Service 这些注解标注类的时候，Spring 默认通过无参构造器来创建实例的。如果类只有一个有参构造方法，Spring 会自动进行构造方法注入。第二种是通过静态工厂方法实例化。有时候对象的创建比较复杂，我们会写一个静态工厂方法来创建，然后用 @Bean 注解来标注这个方法。Spring 会调用这个静态方法来获取 Bean 实例。第三种是通过实例工厂方法实例化。这种方式是先创建工厂对象，然后通过工厂对象的方法来创建 Bean：第四种是通过 FactoryBean 接口实例化。这是 Spring 提供的一个特殊接口，当我们需要创建复杂对象的时候特别有用：你是怎么理解 Bean 的？在我看来，Bean 本质上就是由 Spring 容器管理的 Java 对象，但它和普通的Java 对象有很大区别。普通的 Java 对象我们是通过 new 关键字创建的。而Bean 是交给 Spring 容器来管理的，从创建到销毁都由容器负责。从实际使用的角度来说，我们项目里的 Service、Dao、Controller 这些都是Bean。比如 UserServ
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ice 被标注了 @Service 注解，它就成了一个 Bean，Spring会自动创建它的实例，管理它的依赖关系，当其他地方需要用到 UserService 的时候，Spring 就会把这个实例注入进去。这种依赖注入的方式让对象之间的关系变得松耦合。Spring 提供了多种 Bean 的配置方式，基于注解的方式是最常用的。@Component 和 @Bean 有什么区别？首先从使用上来说，@Component 是标注在类上的，而 @Bean 是标注在方法上的。@Component 告诉 Spring 这个类是一个组件，请把它注册为 Bean，而 @Bean 则告诉 Spring 请将这个方法返回的对象注册为 Bean。从控制权的角度来说，@Component 是由 Spring 自动创建和管理的。而 @Bean 则是由我们手动创建的，然后再交给 Spring 管理，我们对对象的创建过程有完全的控制权。能说一下 Bean 的生命周期吗？Bean 的生命周期可以分为 5 个主要阶段，我按照实际的执行顺序来说一下。第一个阶段是实例化。Spring 容器会根据 BeanDefinition，通过反射调用 Bean的
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法创建对象实例。如果有多个构造方法，Spring 会根据依赖注入的规则选择合适的构造方法。第二阶段是属性赋值。这个阶段 Spring 会给 Bean 的属性赋值，包括通过@Autowired、@Resource 这些注解注入的依赖对象，以及通过 @Value 注入的配置值第三阶段是初始化。这个阶段会依次执行：@PostConstruct 标注的方法InitializingBean 接口的 afterPropertiesSet 方法通过 @Bean 的 initMethod 指定的初始化方法初始化后，Spring 还会调用所有注册的 BeanPostProcessor 后置处理方法。这个阶段经常用来创建代理对象，比如 AOP 代理。第五阶段是使用 Bean。比如我们的 Controller 调用 Service，Service 调用DAO。最后是销毁阶段。当容器关闭或者 Bean 被移除的时候，会依次执行：@PreDestroy 标注的方法DisposableBean 接口的 destroy 方法通过 @Bean 的 destroyMethod 指定的销毁方法Aware 类型的接口有什么作用？Aware 
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 接口在 Spring 中是一个很有意思的设计，它们的作用是让 Bean 能够感知到 Spring 容器的一些内部组件。从设计理念来说，Aware 接口实现了一种“回调”机制。正常情况下，Bean 不应该直接依赖 Spring 容器，这样可以保持代码的独立性。但有些时候，Bean 确实需要获取容器的一些信息或者组件，Aware 接口就提供了这样一个能力。什么是自动装配？自动装配的本质就是让 Spring 容器自动帮我们完成 Bean 之间的依赖关系注入，而不需要我们手动去指定每个依赖。简单来说，就是“我们不用告诉 Spring具体怎么注入，Spring 自己会想办法找到合适的 Bean 注入进来”。自动装配的工作原理简单来说就是，Spring 容器在启动时自动扫描@ComponentScan 指定包路径下的所有类，然后根据类上的注解，比如@Autowired、@Resource 等，来判断哪些 Bean 需要被自动装配。之后分析每个 Bean 的依赖关系，在创建 Bean 的时候，根据装配规则自动找到合适的依赖 Bean，最后根据反射将这些依赖注入到目标 Bean 中Bean 的作用域有哪些Bean 的作用域决
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 定了 Bean 实例的生命周期和创建策略，singleton 是默认的作用域。整个 Spring 容器中只会有一个 Bean 实例。不管在多少个地方注入这个 Bean，拿到的都是同一个对象。生命周期和 Spring 容器相同，容器启动时创建，容器销毁时销毁。实际开发中，像 Service、Dao 这些业务组件基本都是单例的，因为单例既能节省内存，又能提高性能。当把 scope 设置为 prototype 时，每次从容器中获取 Bean 的时候都会创建一个新的实例。当需要处理一些有状态的 Bean 时会用到 prototype，比如每个订单处理器需要维护不同的状态信息如果作用于是 request，表示在 Web 应用中，每个 HTTP 请求都会创建一个新的 Bean 实例，请求结束后 Bean 就被销毁。如果作用于是 session，表示在 Web 应用中，每个 HTTP 会话都会创建一个新的 Bean 实例，会话结束后 Bean 被销毁。application 作用域表示在整个应用中只有一个 Bean 实例，类似于 singleton，但它的生命周期与 ServletContext 绑定。Spring 中的单
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例 Bean 会存在线程安全问题吗？首先要明确一点。Spring 容器本身保证了 Bean 创建过程的线程安全，也就是说不会出现多个线程同时创建同一个单例 Bean 的情况。但是 Bean 创建完成后的使用过程，Spring 就不管了换句话说，单例 Bean 在被创建后，如果它的内部状态是可变的，那么在多线程环境下就可能会出现线程安全问题单例 Bean 的线程安全问题怎么解决呢？第一种，使用局部变量，也就是使用无状态的单例 Bean，把所有状态都通过方法参数传递：第二种，当确实需要维护线程相关的状态时，可以使用 ThreadLocal 来保存状态。ThreadLocal 可以保证每个线程都有自己的变量副本，互不干扰。第三种，如果需要缓存数据或者计数，使用 JUC 包下的线程安全类，比如说AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。第四种，对于复杂的状态操作，可以使用 synchronized 或 Lock：第五种，如果 Bean 确实需要维护状态，可以考虑将其改为 prototype 作用域，这样每次注入都会创建一个新的实例，避免了多线程共享同
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个实例的问题。说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖：一级缓存：存放完全初始化好的单例 Bean。二级缓存：存放正在创建但未完全初始化的 Bean 实例。三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。三级缓存解决循环依赖的过程是什么样的？实例化 Bean 时，将其早期引用放入三级缓存。其他依赖该 Bean 的对象，可以从缓存中获取其引用。初始化完成后，将 Bean 移入一级缓存。假如 A、B 两个类发生循环依赖：A 实例的初始化过程：1 、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道2 、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。3 、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。4 、发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 已经实例化并且初始化完成了，把 B 放入⼀级缓存5 、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存6 、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor在 Bean 的初始化阶段，会通过 
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Bean 后置处理器来进行一些前置和后置的处理。实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是AutowiredAnnotationBeanPostProcessor。Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。AOP 有哪些核心概念？切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法切点（Pointcut）：对连接点进行拦截的定位通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强目标对象 （Target）：代理的目标对象引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。在 Spring 容器初始化 Bean的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 理对象中织入切面逻辑。这一过程发生在Spring 容器的后处理器（BeanPostProcessor）阶段。简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像@Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现AOP。③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。AOP 的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。第一步，自定义注解作为切点第二
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间第三步，在使用的地方加上自定义注解第四步，当接口被调用时，就可以看到对应的执行日志。说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。说说 Spring AOP 和 AspectJ AOP 区别?说
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。反射：运行时操作类的元信息的底层能力动态代理：基于反射实现方法拦截的设计模式Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚.我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部实现事务的控制。介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。说说两者的区别？编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务说说 Spring 的事务隔离级别？好，事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: TransactionDefinition 接口中。①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 ），MySQL默认的是可重复读，Oracle 默认的读已提交。②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 下文如何在方法调用链中传播。Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。事务传播机制是使用ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。第一步，在 Bean 初始化阶段创建代理对象：Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性切面，也就是@Transactional 注解及其属性值。然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。第二步，在执行目标方法时进行事务增强操作：当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为 MethodInterceptor，事务增强对该接口的实现为 TransactionInterceptor，@Transactional 应用在非 public 修饰的方法上如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。Spring MVC 的核心组件？DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping将不同
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的请求映射到不同的 Handler。HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。Spring MVC 的
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。然后 DispatcherServlet 通过 ViewResolver 解析视图。最后，DispatcherServlet 渲染视图并将响应返回给客户端①、发起请求：客户端通过 HTTP 协议向服务器发起请求。②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个Controller 应该处理这个请求。通常会根据请求的 URL 来确定。④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用HandlerAdapter 来调用 Controller 的处理方法。⑤、执行处理器：Controller 处理请求，处理完后
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用ViewResolver 来解析视图名称，找到具体的视图页面。⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。⑧、响结果：DispatcherServlet 将视图结果返回给客户端。Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller 注解的类，还可以是实现了特定接口的其他类（如
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  HttpRequestHandler 或SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。SpringMVC Restful 风格的接口的流程是什么样的呢？我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同：客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServletDispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个 ControllerController 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理HandlerAdapter 执行完对 Controller 的请求，会调用HandlerMethodReturnValueHandler 去处理返回值，主要的过程：5.1. 调用 RequestResponseBodyMethodProcessor，创建ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入ServletServerHttpResponse 的 OutputStream 输出流中5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化执行完请求后，返回的 ModealAndView 为 null，ServletServerHtt
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: pResponse里也已经写入了响应，所以不用关心 View 的处理介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架Spring Boot 的优点非常多，比如说：Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和Spring MVC。Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控Spring Boot 常用注解有哪些？@SpringBootApplication：Spring Boot 应用的入口，用在启动类上。还有一些 Spring 框架本身的注解，比如 @Component、@RestController、@Service、@ConfigurationProperties、@Transact
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ional 等。SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。SpringBoot 的自动装配机制主要通过 @EnableAutoConfiguration 注解实现，这个注解是 @SpringBootApplication 注解的一部分，后者是一个组合注解，包括 @SpringBootConfiguration、@ComponentScan 和@EnableAutoConfiguration。@EnableAutoConfiguration 注解通过AutoConfigurationImportSelector 类来加载自动装配类，这个类实现了ImportSelector 接口的 selectImports 方法，该方法负责获取所有符
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat和 Jackson 等依赖。自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。为什么使用 Spring Boot？Spring Boot 解决了传统 Spring 开发的三大痛点：简化配置：自动装配 + 起步依赖，告别 XML 配置地狱快速启动：内嵌 Tomcat/Jetty，一键启动独立运行应用生产就绪：Actua
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: tor 提供健康检查、监控等运维能力@Import 的作用实现模块化配置导入，三种用法导入普通配置类：@Import(MyConfig.class)导入 ImportSelector 实现（自动装配核心）：@Import(AutoConfigurationImportSelector.class)导入 ImportBeanDefinitionRegistrar 实现（动态注册 Bean）Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责：第一步，创建 SpringApplication 实例，负责应用的启动和初始化；第二步，从 application.yml 中加载配置文件和环境变量；第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入；第四步，启动内嵌的 Web 容器。第五步，发布启动完成事件 ApplicationReadyEvent，并调用ApplicationRunner 的 run 方法完成启动后的逻辑。了解@SpringBootApplication 注解吗？@SpringBootApplic
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ation 是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解：@SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。@EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。@ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication 注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部的一系列处理流程Spring Boot 利用 Java 反射机制来读取传递给 run 方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication@SpringBootApplication 是一个组合注解，它里面的@ComponentScan 注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。如果需要自定义包扫描路径，可以在@SpringBootApplication 注解上添加@ComponentScan 注解，指定要扫描的包路径。这种方式会覆盖默认的包扫描路径，只扫描 com.github.paicoding.forum 包及其子包下的所有组件。SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种Model-View-Controller（模型-视图-控制器）
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的开发模式。Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个SpringMVC 的 Web 项目。Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。微服务架构主要要解决哪些问题？服务很多，客户端怎么访问，如何提供对外网关?这么多服务，服务之间如何通信? HTTP 还是 RPC?这
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 么多服务，如何治理? 服务的注册和发现。服务挂了怎么办？熔断机制SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml：用 SpringTask资源占用太高，有什么其他的方式解决？（补充）第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。第二，使用数据库调度器（如 Quartz）Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。Spring Cache 和 Redis 有什么区别？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。Redis 是一个分布式的缓存中间件，支持多种数
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。说说什么是 MyBatis?Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL语句的功底有一定要求SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中JDBC 编程有哪些不足之处，MyBatis 是如何解决的？1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xm
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: l文件中与 java 代码分离。3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。Hibernate 和 MyBatis 有什么区别？不同点1）映射关系MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂2）SQL 优化和移植性Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。MyBatis 需要
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步：1）创建 SqlSessionFactory2）通过 SqlSessionFactory 创建 SqlSessionSqlSession（会话）可以理解为程序和数据库之间的桥梁3）通过 sqlsession 执行数据库操作，可以通过 SqlSession 实例来直接执行已映射的 SQL 语句：4）调用 session.commit()提交事务5）调用 session.close()关闭会话说说 MyBatis 生命周期？SqlSessionFactoryBuilder一旦创建了 SqlSessionFactory，就不再需要它了。 因此SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。SqlSessionSqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。Mapper映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。在 mapper 中如何传递多个参数？#{}和${}的区别?①、当使用 #{} 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。由于 #{} 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。<select id="selectUser" resultType="User">SELECT * FROM users WHERE id = #{id}</select>②、当使用 ${} 时，参数的值会
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直接替换到 SQL 语句中去，而不会经过预处理。这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or1=1，这样就会导致查询出所有用户的结果。${} 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。<select id="selectUsersByOrder" resultType="User">SELECT * FROM users ORDER BY ${columnName} ASC</select>模糊查询 like 语句该怎么写?CONCAT('%',#{question},'%') 使用 CONCAT()函数，（推荐 ✨）说说 Mybatis 的一级、二级缓存？一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或close 之后，该 SqlSession 中的所有 Ca
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: che 就将清空，MyBatis 默认打开一级缓存。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。能说说 MyBatis 的工作原理吗？按工作原理，可以分为两大步：生成会话工厂、会话运行构造会话工厂也可以分为两步：获取配置获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括：读取配置文件，包括基础配置文件和映射文件初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等提供一个单例，作为会话工厂构建的重要参数它的构建过程也会初始化一些环境变量，比如数据源构建 SqlSessionFactorySqlSessionFactory 只是一
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合：Executor（执行器）Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法StatementHandler（数据库会话器）StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个handler 去执行 query。ParameterHandler （参数处理器）PreparedStatementHandler 里对 sql 进行了预编译处理ResultSetHandler（结果处理器）我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂SqlSessionFactory。创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。参数处理：对输入参数的类型进行处理，并预编译。结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。MyBatis 的功能架构是什么样的？我们一般把 Mybatis 的功能架构分为三层：API 接口层：提供给外部使用的接口 API，开发人员通
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑Mybatis 都有哪些 Executor 执行器？Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String, Statement>内，供下一次使用。简言之，就是重复使
2025-08-11 10:55:52.767 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 用Statement 对象。BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。说说 JDBC 的执行步骤？Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤：在与数据库建立连接之前，首先需要通过 Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了 DriverManager 类中。Class.forName("com.mysql.cj.jdbc.Driver");第二步，建立数据库连接使用 DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数C
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: onnection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/databaseName", "username","password");第三步，创建 Statement 对象通过建立的数据库连接对象 Connection 创建 Statement、PreparedStatement或 CallableStatement 对象，用于执行 SQL 语句Statement stmt = conn.createStatement();第四步，执行 SQL 语句使用 Statement 或 PreparedStatement 对象执行 SQL 语句。执行查询（SELECT）语句时，使用 executeQuery()方法，它返回 ResultSet 对象；执行更新（INSERT、UPDATE、DELETE）语句时，使用 executeUpdate()方法，它返回一个整数表示受影响的行数。ResultSet rs = stmt.executeQuery("SELECT * FROM tableName");int affectedRow
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: s = stmt.executeUpdate("UPDATE tableName SET column ='value' WHERE condition");第五步，处理结果集如果执行的是查询操作，需要处理 ResultSet 对象来获取数据第六步，关闭资源最后，需要依次关闭 ResultSet、Statement 和 Connection 等资源，释放数据库连接等资源创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是 java.sql.Connection 对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。Connection 对象代表了应用程序和数据库的一个连接会话。通过调用 DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。一旦获得 Connection 对象，就可以使用它来创建执行 SQL 语句的 Statement、PreparedStatement 和 CallableStatement 对象，以及管理事务等。什么是 SQL 注入？如
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。为了防止 SQL 注入，可以采取以下措施：①、使用参数化查询使用参数化查询，即使用 PreparedStatement 对象，通过 setXxx 方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。②、限制用户输入对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或SQL 关键字。③、使用 ORM 框架比如，在 MyBatis 中，使用#{}占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。<select id="selectUser" resultType="User">SELECT * FROM users WHERE username = #{userName}</select>分布式说说 CAP 原则？、CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition toleran
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ce（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。假如现在有这样的场景：用户访问了 N1，修改了 D1 的数据。用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。接下来：保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。ASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性（Eventual consistency）的效果。BASE 的主要含义：Basically Available（基本可用）什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。Soft State（软状态）什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。Eventually Consistent（最终一致性）上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL 分布式锁、ZooKepper 分布式锁、Redis分布式锁。MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。加锁的时候，在锁表中增加一条记录
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 即可；释放锁的时候删除记录就行。如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。基于 Redis 的分布式锁核心思想： 利用 Redis 单线程执行命令的特性以及其丰富的数据结构和命令（尤其是 SETNX, SET with NX/PX/EX, Lua 脚本）来实现高性能锁。当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。什么是分布式事务
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性分布式事务有哪些常见的实现方案？二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。本地消息表：在数据库中存储事务事件，通过定时任务处理消息。基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。7.1 说说 2PC 两阶段提交？两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。优点：尽量保证了数据的强一致，实现成本
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 较低，在各大主流数据库都有自己实现，缺点:单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务：如果所有参与者都返回正确的 ACK 响应，则提交事务如果参与者有一个或多个参与者收到错误的 ACK 响应或者超时，则中断事务可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。Confirm：确
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。执行流程：订单服务，添加一条订单和一条消息，在一个事务里提交订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送库存服务，接收 MQ 消息，修改库存表，需要保证幂等
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 操作如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息如果修改失败，可以不做处理，等待重试MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。分布式算法 paxos 了解么 ？Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一在 Paxos 中有这么几个角色：Proposer（提议者） : 提议者提出提案，用于投票表决。Accecptor（接受者） : 对提案进行投票，并接受达成
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 共识的提案。Learner（学习者） : 被告知投票的结果，接受达成共识的提案Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受)Prepare(准备)阶段提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答：两个承诺：承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求承诺不会再接受提案号小于 Mn 的 Accept 请求一个应答：不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。Accept(接受)阶段如果提议者收到来自
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题笔试题1 为什么使用消息队列？消息队列（Message Queue，简称 MQ）是一种跨进程的通信机制，用于上下游传递消息。它在现代分布式系统中扮演着重要的角色，主要用于系统间的解耦、异步消息处理以及流量削峰。消息队列的使用场景解耦在没有消息队列的系统中，如果
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个系统需要与多个系统交互，它们之间的耦合度会非常高。例如，系统 A直接调用系统 B 和 C的接口，如果未来需要接入系统 D或取消 B 系统，系统 A 需要修改代码，这增加了系统的风险。使用消息队列后，系统 A 只需将消息推送到队列，其他系统根据需要从队列中订阅消息。这样，系统 A 不需要做任何修改，也不需要考虑下游消费失败的情况，从而实现了系统间的解耦。异步处理在同步操作中，一些非关键的业务逻辑可能会消耗大量时间，导致用户体验不佳。例如，系统 A 在处理一个请求时，需要在多个系统中进行操作，这可能导致总延迟增加。通过使用消息队列，系统 A 可以将消息写入队列，而其他业务逻辑可以异步执行，从而显著减少总耗时。流量削峰对于面临突发流量的系统，如果直接将所有请求发送到数据库，可能会导致数据库连接异常或系统崩溃。消息队列可以帮助系统按照下游系统的处理能力从队列中慢慢拉取消息，从而避免因突发流量导致的系统崩溃。消息队列的优缺点优点解耦：使得系统间的依赖关系最小化，降低系统间的耦合度。异步处理：提高系统的响应速度和吞吐量。流量削峰：使系统能够应对高流量压力，避免系统因突发流量而崩溃2.简述数据库的事务，说出事务的特点？
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 在数据库管理系统中，事务是一个非常重要的概念，它指的是一系列的数据库操作，这些操作要么全部成功，要么全部失败，确保数据的完整性和一致性。事务的四大特性通常被称为 ACID属性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。原子性（Atomicity）原子性确保事务中的所有操作要么全部成功，要么全部失败回滚，不会出现只执行了部分操作的情况。这意味着事务是一个不可分割的工作单位，例如，在银行转账的场景中，转账操作需要同时更新两个账户的余额，这两个操作必须要么都执行，要么都不执行一致性（Consistency）一致性意味着数据库在事务开始之前和结束之后，都必须保持一致状态。事务不会破坏数据的完整性和业务规则。例如，如果一个转账事务在执行过程中系统崩溃，事务没有提交，那么事务中所做的修改也不会保存到数据库中，保证了数据的一致性隔离性（Isolation）隔离性保证了当多个用户并发访问数据库时，数据库系统能够为每个用户的事务提供一个独立的运行环境，事务之间不会互相干扰。例如，当一个事务正在处理数据时，其他事务必须等待，直到该事务完成，
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 才能访问同样的数据持久性（Durability）持久性确保一旦事务提交，它对数据库的改变就是永久性的。即使发生系统故障，事务的结果也不会丢失。例如，一旦银行转账事务提交，转账的金额就会永久地反映在各个账户的余额中事务的四大特性是数据库管理系统设计的基础，它们确保了数据库操作的安全性和可靠性，使得用户可以信赖数据库处理复杂的业务逻辑。3. SOA 和微服务之间的区别？SOA（面向服务的架构）[&和微服务架构&]是两种常见的软件架构设计方法，它们在服务划分、通信方式和应用场景等方面存在显著差异。SOA 的特点 SOA 是一种高层次的架构设计理念，旨在通过服务接口实现系统间的松耦合和功能复用。服务通过企业服务总线（ESB）进行通信，ESB负责消息路由、协议转换和服务集成。SOA 的服务粒度较粗，适用于复杂的企业级系统，尤其是需要集成异构系统的场景。微服务的特点 微服务架构是对 SOA 的进一步演进，强调将单一业务系统拆分为多个独立的小型服务。每个服务独立开发、部署和运行，通常通过轻量级协议（如 HTTP/REST）进行通信。微服务更注重快速交付和自动化运维，适合快速变化的互联网系统。主要区别服务粒度：SOA 的服务
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 粒度较粗，通常是一个完整的业务模块；微服务的服务粒度较细，专注于单一功能。通信方式：SOA 依赖 ESB 进行服务间通信，支持多种协议；微服务使用轻量级协议（如 HTTP/REST），去掉了 ESB。部署方式：SOA 通常整体部署，微服务则支持独立部署，便于快速迭代。应用场景：SOA 适用于复杂的企业级系统，微服务更适合轻量级、基于 Web 的系统。总结 SOA 和微服务各有优劣，选择哪种架构取决于具体的业务需求和系统复杂性。SOA 更适合需要集成异构系统的大型企业应用，而微服务则适合快速变化的互联网应用。4.深拷贝和浅拷贝的区别？在编程中，深复制和浅复制是两种不同的对象复制方式。它们主要用于处理对象和数组等引用数据类型。浅复制浅复制只复制对象的引用，而不复制对象本身。也就是说，新旧对象共享同一块内存空间，对其中一个对象的修改会影响到另一个对象。浅复制适用于对象的属性是基本数据类型的情况，但如果属性是引用类型，则会出现共享内存的问题。深复制深复制会创建一个新的对象，并递归复制所有层级的属性和数组元素。新对象与原对象不共享内存，修改新对象不会影响到原对象。深复制适用于需要完全独立的对象副本的情况。5.有了关系型
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据库，为什么还需要 NOSQL 数据库？关系型数据库（RDBMS）在数据存储和管理方面表现出色，但在某些场景下存在局限性。随着互联网和大数据时代的到来，传统的关系型数据库在处理大量非结构化和半结构化数据时显得力不从心。为了解决这些问题，非关系型数据库（NoSQL）应运而生。关系型数据库的局限性扩展性：关系型数据库通常采用垂直扩展（Scale-Up）的方式，通过增加硬件资源来提升性能。然而，这种方式在高并发、大数据量的情况下成本高昂且效果有限。灵活性：关系型数据库要求预先定义数据模式（Schema），在需求频繁变化的应用场景中显得不够灵活。每次修改数据模式都需要停机或复杂的迁移操作。性能：在高并发读写、大规模数据处理的情况下，关系型数据库的性能可能无法满足需求，特别是在分布式环境下。成本：关系型数据库通常需要昂贵的硬件和专业的维护团队，对于中小型企业和初创公司来说，成本压力较大。非关系型数据库的优势水平扩展（Scale-Out）：NoSQL 数据库通常设计为支持水平扩展，通过增加更多的服务器节点来提升性能。这种方式在大规模数据和高并发场景下非常有效。灵活的数据模型：NoSQL数据库通常采用无模式（Schema
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -less）或弱模式（Schema-flexible）的设计，允许数据以更灵活的方式存储。高性能：NoSQL 数据库针对特定的应用场景进行了优化，通常具有更高的读写性能。低成本：NoSQL数据库通常采用开源软件，硬件要求较低，适合在云环境中部署，降低了总体拥有成本（TCO）。分布式架构：NoSQL 数据库通常采用分布式架构，能够更好地处理大规模数据和高并发请求。关系型数据库与非关系型数据库的对比数据模型：关系型数据库以表格形式存储数据，而 NoSQL数据库以键值对、文档、列族或图的形式存储数据。扩展性：关系型数据库采用垂直扩展，而 NoSQL数据库采用水平扩展。数据一致性：关系型数据库强调强一致性（ACID），而 NoSQL 数据库通常采用最终一致性（BASE）。数据模式：关系型数据库需要固定模式，而 NoSQL 数据库通常无模式或弱模式。性能：关系型数据库适合事务处理和小规模数据，而 NoSQL数据库适合大规模数据和高并发。成本：关系型数据库成本较高，而 NoSQL数据库成本较低。结论NoSQL数据库的出现并不是为了取代关系型数据库，而是为了填补关系型数据库在某些场景下的不足。NoSQL 数据库提供了更高的
2025-08-11 10:55:52.768 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 扩展性、灵活性和性能，适合处理大规模数据和高并发请求。然而，关系型数据库在事务处理和企业应用中仍然具有不可替代的优势。
2025-08-11 10:55:53.161 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:53.161 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:55:53.161 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.yizhaoqi.smartpai.service.VectorizationService - 开始向量化文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:55:53.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.client.EmbeddingClient - 开始生成向量，文本数量: 714
2025-08-11 10:55:53.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:53.171 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:53.327 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] [10d4b7b0-9] Response 400 BAD_REQUEST
2025-08-11 10:55:53.328 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] [10d4b7b0-9] Read 312 bytes
2025-08-11 10:55:54.333 [parallel-7] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:54.334 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:54.486 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] [10d4b7b0-10] Response 400 BAD_REQUEST
2025-08-11 10:55:54.487 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] [10d4b7b0-10] Read 312 bytes
2025-08-11 10:55:55.500 [parallel-8] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:55.501 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:55.737 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] [10d4b7b0-11] Response 400 BAD_REQUEST
2025-08-11 10:55:55.738 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] [10d4b7b0-11] Read 312 bytes
2025-08-11 10:55:56.742 [parallel-9] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:55:56.744 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:55:56.923 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] [10d4b7b0-12] Response 400 BAD_REQUEST
2025-08-11 10:55:56.924 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [46138812] [10d4b7b0-12] Read 312 bytes
2025-08-11 10:55:56.924 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [46138812] Cancel signal (to close connection)
2025-08-11 10:55:56.924 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR com.yizhaoqi.smartpai.client.EmbeddingClient - 调用向量化 API 失败: Retries exhausted: 3/3
reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:56.925 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.yizhaoqi.smartpai.service.VectorizationService - 向量化失败，fileMd5: c8f8cebf90c764b93d862694096a2af9
java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:56.926 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.y.smartpai.consumer.FileProcessingConsumer - Error processing task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:59.970 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR o.s.kafka.listener.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(com.yizhaoqi.smartpai.model.FileProcessingTask)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: java.lang.RuntimeException: Error processing task
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:67)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	... 25 common frames omitted
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:55:59.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Received task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
2025-08-11 10:55:59.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件权限信息: userId=1, orgTag=PRIVATE_sy, isPublic=true
2025-08-11 10:55:59.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Downloading file from storage: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:59.972 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Detected remote URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:55:59.979 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Successfully connected to URL, starting download...
2025-08-11 10:55:59.979 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 开始解析文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文件元数据:
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:unmappedUnicodeCharsPerPage: 0
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:PDFVersion: 1.7
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmp:CreatorTool: WPS 文字
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXFA: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:modify_annotations: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print_degraded: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By-Full-Set: org.apache.tika.parser.DefaultParser
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:creator: SongYu
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:num3DAnnotations: 0
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:created: 2025-08-04T09:36:05Z
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:modified: 2025-08-04T09:36:05Z
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:format: application/pdf; version=1.7
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator_tool: WPS 文字
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:overallPercentageUnmappedUnicodeChars: 0.0
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:fill_in_form: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:modified: 2025-08-04T09:36:05Z
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasCollection: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:encrypted: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsNonEmbeddedFont: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:custom:SourceModified: D:20250804173605+08'00'
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasMarkedContent: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - Content-Type: application/pdf
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator: SongYu
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:totalUnmappedUnicodeChars: 0
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_for_accessibility: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:assemble_document: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmpTPg:NPages: 133
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXMP: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:charsPerPage: 1441
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_content: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:trapped: False
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - SourceModified: D:20250804173605+08'00'
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By: org.apache.tika.parser.DefaultParser
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_modify: true
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:created: 2025-08-04T09:36:05Z
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsDamagedFont: false
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 提取的文本内容长度: 121403
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆状态的检查、为游客和已登录用户展示不同界面与功能。实现不同用户的权限控制和网站数据统计(UV、DAU)，管理员可以查看网站数据统计和网站监控信息。支持用户上传头像，实现发布帖子、评论帖子、热帖排行、发送私信与敏感词过滤等功能。实现了点赞关注与系统通知功能。支持全局搜索帖子信息的功能。核心功能具体实现1. 通过对登录用户颁发登录凭证，将登陆凭证存进 Redis 中来记录登录用户登录状态，使用拦截器进行登录状态检查，使用 Spring Security 实现权限控制，解决了 http 无状态带来的缺陷，保护需登录或权限才能使用的特定资源。（登入时将生成的 Ticket存入 Redies, 然后在登入请求成功时，将 Redies中的Ticket存入新建的 Cookie 中，然后反馈给浏览器，随后在该浏览器访问其他请求时，会先经过 LoginTicketInterceptor，判断请求中是否有 Ticket，是否和 Redies中的 T
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: icket一致，如果一致会将用户信息存入到 hostHolder（属于线程局部缓存）中，以便后续在请求处理过程中可以方便地获取到当前登录用户的信息，在请求处理完成后，清除当前线程中存储的用户信息。通过这种方式，确保每个请求都是独立处理的，不会因为线程复用而导致用户信息泄露或混淆。 注意可以将用户信息存入 Redis缓存中来减少 DB 的访问量，但是当用户数据更新时，必须即使删除 Redis中的用户数据，以保证数据的一致性和准确性。Spring Security 的用户认证是在自定义的过滤器中，也是获取请求 Cookie 中的 Ticket 和 Redis中的Ticket是否一致，然后将认证用户存到安全上下文中，在 Security 配置类中根据安全上下文获取用户信息，判断用户对各个资源的访问权限。Security 认证应当放到过滤器中而不是拦截器，因为过滤器比拦截器先执行，在拦截器中配置安全上下文会导致 Security 配置类获取不到用户信息，因为此时还没执行拦截器。）2. 使用 ThreadLocal 在当前线程中存储用户数据，代替 session 的功能便于分布式部署。在拦截器的 preHandle 中
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存储用户数据，在 postHandle 中将用户数据存入 Model，在 afterCompletion 中清理用户数据。（ThreadLocal 为每个线程提供独立的变量副本。每个线程操作自己的副本，互不干扰。在 Web 应用中，一个请求从开始到结束通常由同一个线程处理。因此，在拦截器的 preHandle 中存储的数据，可在整个请求链路（Controller、Service、Dao）中通过 ThreadLocal 获取。 在分布式部署中，由于 Session需要共享，使用 ThreadLocal存储用户数据，我们并不需要在多个服务器之间共享这些数据。因为每个请求都是独立的，处理完一个请求后，数据就被清除了。所以，在分布式环境下，我们只需要确保每个服务器能够独立处理请求即可，不需要考虑多个服务器之间的 Session同步问题。）3. 使用 Redis 的集合数据类型来解决踩赞、相互关注功能，采用事务管理，保证数据的正确，采用“先更新数据库，再删除缓存”策略保证数据库与缓存数据的一致性。采用 Redis 存储验证码，解决性能问题和分布式部署时的验证码需求。采用 Redis 的 HyperLogLog 存储每日
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  UV、Bitmap 存储 DAU，实现网站数据统计的需求。（使用 RedisTemplate执行一个 Redis 事务。SessionCallback 接口的 execute方法会在一个事务中执行所有操作，确保操作的原子性。通过调用 multi()方法，开启一个 Redis事务。在这个事务中执行的命令会被缓存，直到 exec()方法被调用时才会一次性提交。数据结构：HyperLogLog，12KB 内存可计算 2^64 个不重复元素 误差率仅 0.81% 数据结构：Bitmap 一连串二进制数组，可以进行二值状态统计）4. 使用 Kafka 作为消息队列，在用户被点赞、评论、关注后以系统通知的方式推送给用户，用户发布或删除帖子后向 elasticsearch 同步，对系统进行解耦、削峰。（在这个系统中 kafka 就办了三件事，一是用户在被点赞、评论、关注后会借助kafka 消费者来异步的生成系统通知，二三是在用户发布帖子和删除帖子时，将内容添加到 elasticsearch或者从 elasticsearch 中删除）5. 使用 elasticsearch + ik 分词插件实现全局搜索功能，当用户发布、修
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 改或删除帖子时，使用 Kafka 消息队列去异步将帖子信息给 elasticsearch 同步。（Elasticsearch（简称 ES）是一个开源的分布式搜索和分析引擎，它专为处理海量数据设计，提供近实时的全文搜索能力。IK Analyzer 是专为中文设计的开源分词插件，解决中文文本分析的核心难题。ik_smart：粗粒度切分 ik_max_word 细粒度切分）6. 使用分布式定时任务 Quartz 定时计算帖子分数，来实现热帖排行的业务功能。对频繁需要访问的数据，如用户信息、帖子总数、热帖的单页帖子列表，使用Caffeine 本地缓存 + Redis 分布式缓存的多级缓存，提高服务器性能，实现系统的高可用。（上面三个部分就是 Quartz的基本组成部分：调度器：Scheduler任务：JobDetail触发器：Trigger，包括 SimpleTrigger 和 CronTrigger定义一个 Quartz定时任务及其触发器。具体来说，它配置了一个名为postScoreRefreshJob的任务，该任务属于 communityJobGroup 组，并且被设置为持久化和请求恢复。同时，它还配置了一个名为
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  postScoreRefreshTrigger 的触发器，该触发器也属于 communityTriggerGroup组，并且每 5 分钟触发一次postScoreRefreshJob任务。因此，这段代码的主要目的是确保 PostScoreRefreshJob类中定义的任务每 5分钟执行一次）核心技术Spring Boot、SSMRedis、Kafka、ElasticsearchSpring Security、Quartz、Caffeine项目亮点项⽬构建在 Spring Boot+SSM 框架之上，并统⼀的进⾏了状态管理、事务管理、异常处理；利⽤ Redis 实现了点赞和关注功能，单机可达 5000TPS；利⽤ Kafka 实现了异步的站内通知，单机可达 7000TPS；利⽤ Elasticsearch 实现了全⽂搜索功能，可准确匹配搜索结果，并⾼亮显示关键词；利⽤ Caffeine+Redis 实现了两级缓存，并优化了热⻔帖⼦的访问，单机可达8000QPS。利⽤ Spring Security 实现了权限控制，实现了多重⻆⾊、URL 级别的权限管理；利⽤ HyperLogLog、Bitmap 分别实现了 
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: UV、DAU 的统计功能，100 万⽤户数据只需*M 内存空间；利⽤ Quartz 实现了任务调度功能，并实现了定时计算帖⼦分数、定时清理垃圾⽂件等功能；利⽤ Actuator 对应⽤的 Bean、缓存、⽇志、路径等多个维度进⾏了监控，并通过⾃定义的端点对数据库连接进⾏了监控。面试题：1.你提到使用 Spring Security 实现权限控制。能具体说明如何整合登录凭证（Redis存储）与 Spring Security？如何实现 URL 级别的动态权限管理？答：用户登入成功时系统会生成一个 Ticket并存入 Redis，新建一个 cookie 存入Ticket；在自定义的过滤器中验证请求携带的 Ticket与 Redis内的 Ticket是否一致，构建 Authentication对象存入 SecurityContextHolder；在 Security 的配置类中对固定 URL 如/admin/**）使用 antMatchers().hasRole("ADMIN")，即根据用户权限赋予访问资源的能力。2. 你提到点赞功能采用‘先更新 DB 再删缓存’策略。如果删除缓存失败导致不一致，如何解决？为何不用
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ‘更新缓存’方案？答：“删除失败兜底方案：设置缓存短过期时间（如 30s），容忍短期不一致异步重试：将失败操作推入 Kafka，消费者重试删除监听 MySQL Binlog（如 Canal）触发缓存删除不更新缓存的原因：写冲突： 并发更新可能导致缓存脏数据（如线程 A更新 DB 后未更新缓存时，线程 B又更新）浪费资源： 频繁更新但低读取的数据会占用带宽复杂度： 需维护缓存与 DB的强一致性逻辑（如分布式锁），而删除策略更简单可靠。”3. 系统通知使用 Kafka异步推送。如果通知发送失败（如网络抖动），如何保证用户最终能收到通知？答：“我们通过三级保障实现可靠性：生产者确认： 设置 Kafka acks=all，确保消息写入所有副本；消费者容错：开启手动提交 Offset，业务处理成功后才提交捕获异常后重试（如 3次），仍失败则存入死信队列补偿机制：定时任务扫描未通知记录（DB状态标记）重新投递死信队列消息人工介入处理此外，消息体包含唯一 ID 防重复消费。”4. 热帖列表用了 Caffeine+Redis两级缓存。如何解决缓存穿透？如何同步本地缓存（Caffeine）的数据？答：“缓存穿透防护：布隆过滤器
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ：将所有的 key 提前存入布隆过滤器，在访问缓存层之前，查询前校验 Key 是否存在，不存在返回空值。缓存空值：对不存在的帖子 ID 缓存 NULL（短过期时间）本地缓存同步过期同步： Caffeine设置 refreshAfterWrite=30s自动刷新主动推送： 当帖子更新时，通过 Redis Pub/Sub 广播失效事件，节点监听后删除本地缓存兜底策略： 本地缓存过期时间短于 Redis（如本地 60s vs Redis 300s），确保最终一致。”5. 定时计算帖子分数时，如何避免分布式环境下的重复执行？如果计算耗时过长导致阻塞，如何优化？答：“防重复执行：使用 Quartz集群模式：数据库锁（QRTZ_LOCKS 表）保证同一任务仅一个节点执行性能优化：分片处理： 按帖子 ID 范围分片（如 0-10000, 10001-20000），多线程并行计算增量计算： 仅扫描最近 X 小时变化的帖子（如 last_modified_time > now()-6h）异步化： 将计算任务拆解为多个子任务投递到 Kafka，消费者并发处理降级策略： 超时后记录断点，下次任务从断点继续。”6. 你使用 Elas
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ticsearch 实现全文搜索并高亮关键词。请说明：如何设计索引映射（Mapping）以优化搜索效率和准确性？如何实现搜索结果的高亮显示？遇到 HTML 标签转义问题如何处理？搜索性能瓶颈可能在哪里？如何优化？答: 1. 索引映射设计：分词策略： 对帖子标题和内容字段使用 ik_max_word 分词器进行细粒度分词（索引时），搜索时结合 ik_smart 提高相关性。字段类型： 标题用 text（分词） + keyword（不分词，用于精确匹配/聚合），ID 用 keyword，发布时间用 date。副本分片： 设置合理副本数（如 1-2）提高查询吞吐量和容错性。关闭不必要特性： 对不需聚合/排序的字段关闭 doc_values 节省存储。2. 高亮实现与转义：高亮请求： 在搜索请求中添加 highlight 部分，指定字段、pre_tags（如 <em>）、post_tags（如 </em>）。HTML 转义： ES 默认会转义高亮片段中的 HTML。我们确保存入 ES 的内容是纯文本（不含用户输入的原始 HTML），避免 XSS 同时解决转义混乱。前端渲染高亮片段时使用 textContent 而非 
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: innerHTML。3. 性能优化点：瓶颈： 复杂查询（多条件+聚合）、深度分页（from + size 过大）、索引设计不佳。优化措施：避免深度分页：使用 search_after + 唯一排序值（如 ID+时间戳）替代 from/size。限制查询范围： 使用 filter 缓存（如时间范围、状态）减少 query 计算量。冷热数据分离： 历史数据迁移到低性能节点或归档索引。合理硬件： SSD、充足内存（ES 堆内存 ≈ 50% 物理内存，不超过 31GB）。7. 项目用 HyperLogLog (HLL) 统计 UV，Bitmap 统计 DAU。请解释：HLL 如何用极小空间估算大基数？它的误差范围是多少？Bitmap 如何统计 DAU？如何解决用户 ID 非连续导致的空间浪费？如果某天 UV 突增，HLL 合并结果会怎样？如何验证其准确性？答：“1. HLL 原理与误差：原理： 对每个用户 ID 做哈希，计算哈希值二进制表示中 ‘1’ 的最高位位置（如 0001... 最高位=4），维护一个 ‘寄存器数组’ 记录每个桶的最大位置。最终通过调和平均数估算基数。核心是利用概率分布。误差： Redis 的 
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: PF 实现标准误差约 0.81%（使用 16384 个寄存器时）。空间仅需 12KB（固定大小）。2. Bitmap 与 DAU：实现： 每天一个 Bitmap Key（如 dau:20230702），用户 ID 作为 Offset，访问则设位为 1。BITCOUNT 获取当日活跃用户数。稀疏优化： 使用 RLE (Run-Length Encoding) 压缩的 Bitmap 库（如 RoaringBitmap）或 Redis 的 BITFIELD 命令动态管理非连续 ID，避免传统 Bitmap 的空间浪费。3. HLL 突增与验证：突增影响：HLL 是基数估计，突增时估算值会上升，误差仍在理论范围内（0.81%）。合并多个 HLL（如按小时合并成天）误差会累积但可控。验证： 定期抽样对比：对某小段时间用 SET 精确计算 UV，与 HLL 结果对比，监控误差是否符合预期。业务上接受近似值是其使用前提。8. 敏感词过滤是社区必备功能。你如何实现它？如何平衡过滤效率和敏感词库的更新？“技术选型：Trie 树 (前缀树)实现：初始化： 服务启动时将敏感词库（DB 或文件）加载到内存中的 Trie 树。节点标记
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是否为词尾。过滤过程： 对用户输入（帖子/评论）进行滑动窗口扫描。匹配到 Trie 树路径且到达词尾时，替换为 *** 或阻断提交。优化： 结合 DFA (确定有限状态自动机) 减少回溯，支持跳过无关字符（如 敏*感*词）。词库更新：热更新： 后台管理添加敏感词后，通过 ZooKeeper 配置中心 或 Redis Pub/Sub广播到所有服务节点，节点异步重建 Trie 树。降级： 更新期间短暂使用旧词库，避免服务中断。词库版本号控制。效率：Trie 树查询时间复杂度 O(n) (n=文本长度)，内存占用可控（可压缩节点）。避免正则表达式（性能差）。”9. 你提到用 Spring Boot Actuator 进行监控并自定义了数据库监控端点。请说明：暴露了哪些关键内置端点？（至少 3个）如何自定义一个端点监控数据库连接池状态（如活跃连接数、等待连接数）？如何保证这些监控接口的安全？答：“1. 关键内置端点：/health：应用健康状态（DB, Redis, Disk 等）/metrics：JVM 内存、线程、HTTP 请求指标等/loggers：动态查看/调整日志级别/threaddump：获取线程快照（排
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 查死锁）创建一个类实现 Endpoint 接口或使用 @Endpoint(id = "dbpool") 注解。注入连接池对象（如 HikariDataSource）。在 @ReadOperation 方法中返回关键指标：安全保障：访问控制：通过 management.endpoints.web.exposure.include/exclude 精确控制暴露的端点。安全加固：集成 Spring Security：只允许管理员角色访问 /actuator/** 路径。修改默认端口：management.server.port 使用与管理网络隔离的端口。HTTPS： 强制要求监控端点使用 HTTPS。10. 在“点赞后发通知”这个场景，涉及更新数据库点赞数 (DB) 和发送 Kafka 消息 (通知) 。如何保证这两个操作的原子性？如果 Kafka 发送失败，如何处理？答：“核心思路：最终一致性 + 本地事务 + 可靠消息原子性保障： 将 ‘更新点赞状态/计数’ 和 ‘写入待通知消息’ 放在同一个数据库事务中。使用 ‘本地消息表’ 方案：在业务数据库创建 message_event 表 (含业务 ID、消息体、状态
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: [PENDING, SENT])。事务内操作：更新点赞相关 DB 数据。向 message_event 插入一条 PENDING 状态的通知记录。事务提交。Kafka 发送与补偿：后台定时任务扫描 PENDING 的消息。发送消息到 Kafka，成功后将状态改为 SENT。发送失败处理：记录重试次数和错误信息，下次任务重试（指数退避）。超过最大重试则标记为失败，告警人工介入。消费者幂等： 通知消费者根据业务 ID 去重，避免重复处理。为什么不强一致？ 跨系统（DB 与 MQ）的强一致（如 2PC）成本高且降低可用性。本方案在 CAP 中优先保证 AP，通过可靠消息实现最终一致，满足业务需求。”11.你提到使用 ThreadLocal 存储用户数据以替代 Session。这在单机中可行，但分布式部署时（如多台 Tomcat 节点）会失效。如何解决分布式场景下的用户状态共享问题？业界主流方案是什么？答：“ThreadLocal 的局限： 它绑定于单个 JVM 线程，无法跨节点共享。在负载均衡（如 Nginx 轮询）下，用户请求落到不同节点会导致状态丢失。方案演进：Session 复制： 利用 Tomcat Red
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: is Session Manager 等工具将 Session 存入Redis。所有节点从 Redis 读写 Session，实现共享。无状态 Token (JWT)： 当前项目采用的核心方案。用户登录后生成包含用户 ID和权限的 JWT Token 返回客户端（通常存于 Cookie 或 Header）。后续请求携带 Token，服务端无需存储 Session，仅需验证 Token 签名和有效期并从 Token中解析用户信息（如注入到 SecurityContext）。这天然支持分布式。项目整合： 我们实际采用了 JWT + Redis 黑名单 的增强方案：JWT 本身无状态，解析快速。主动登出/失效： 将需提前失效的 Token ID 存入 Redis 并设置 TTL（作为黑名单）。校验 Token 时额外检查黑名单。安全性： Token 使用强密钥签名（如 HMAC-SHA256），防止篡改。优点： 彻底解决分布式状态问题，减轻服务端存储压力，更适合 RESTful API。12. 热帖列表使用了 Caffeine 本地缓存。请说明：你选择了哪种缓存淘汰策略（如 LRU、LFU）？依据是什么？如何配置缓
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存大小和过期时间？如何监控缓存命中率？本地缓存导致不同节点数据不一致的风险如何缓解？（例如帖子被删除）答：“1. 淘汰策略与依据：策略： 使用 Window TinyLFU (W-TinyLFU)，Caffeine 的默认算法。它结合了 LRU（近期使用）和 LFU（频率统计）的优点，对突发流量和长期热点都有良好表现。依据：论坛热帖访问模式既有突发（新热帖），也有长尾（持续热帖）。W-TinyLFU在有限空间内能最大化命中率，优于纯 LRU/LFU。2. 配置与监控：配置：javaCaffeine.newBuilder().maximumSize(10_000) // 最大条目数.expireAfterWrite(5, TimeUnit.MINUTES) // 写入后 5 分钟过期.recordStats() // 开启统计.build();监控： 通过 Cache.stats() 获取 CacheStats 对象，关键指标：hitRate()：命中率evictionCount()：淘汰数量averageLoadPenalty()：平均加载耗时可定期输出到日志或监控系统（如 Prometheus）。3. 数据
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不一致风险缓解：主动失效： 核心策略！当帖子被删除或更新时：更新数据库。删除 Redis 中的缓存 Key。通过 Redis Pub/Sub 或 专门的广播方案（如 RabbitMQ Fanout Exchange） 发布“帖子失效”事件。所有服务节点监听到事件后，删除本地 Caffeine 缓存中对应的条目。兜底： 设置较短的本地缓存过期时间（如 5分钟），确保最终一致。”13.你提到点赞功能单机 TPS 达 5000，通知单机 TPS 7000，热帖访问 QPS 8000。请说明：这些数据是如何测试得到的？（工具、场景、环境）TPS 和 QPS 的区别是什么？测试中发现了哪些性能瓶颈？如何定位和优化的？（如 GC、慢 SQL）1. 测试方法：工具： JMeter（模拟并发用户）。场景：点赞 TPS： 持续模拟用户对随机帖子点赞（高并发写）。通知 TPS： 模拟触发通知事件（评论/关注），测量 Kafka 生产者吞吐量。热帖 QPS： 持续请求热帖列表接口（高并发读）。环境： 明确标注是 单机测试（如 4C8G Linux, JDK 17, Tomcat, Redis/Kafka 同机或独立）。2. TPS
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  vs QPS：QPS (Queries Per Second)： 服务器每秒处理的查询请求数（如 HTTP 请求）。适用于读场景。TPS (Transactions Per Second)： 服务器每秒处理的事务数（一个事务可能包含多个操作/请求）。点赞（写 DB + Redis + 发 Kafka）是一个事务。此处 5000 TPS 指每秒完成 5000 次点赞事务。3. 瓶颈发现与优化：发现工具： Arthas (监控方法耗时)、JVisualVM/PerfMa (GC 分析)、Redis Slowlog、MySQL Slow Query Log。典型瓶颈 & 优化：GC 频繁 (Young GC >1s)： 优化 JVM 参数（如 -XX:+UseG1GC, 调整MaxGCPauseMillis），减少大对象分配（如缓存 DTO 复用）。慢 SQL (全表扫描)： 添加索引（如 post_id 在点赞表），优化查询（避免 SELECT*）。Redis 单线程阻塞： 避免长命令（如 KEYS *），分片（Cluster），热点 Key 本地缓存（Caffeine）。Kafka 生产瓶颈： 调优 batc
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: h.size 和 linger.ms，增加分区数，提高 acks 级别换取可靠性（需权衡）。”14.如果这个论坛用户量增长 100 倍（如日活千万级），当前架构在哪些地方可能最先遇到瓶颈？你会如何改造？（请结合你已用技术栈思考）“潜在瓶颈与改造方向：数据库 (MySQL)：瓶颈： 写压力（点赞、发帖）、复杂查询（搜索、统计）、单表数据量过大。改造：读写分离： 主库写，多个从库读（评论列表、用户信息查询）。分库分表： 按 user_id 或 post_id 分片（如 ShardingSphere）。将点赞/关注等高频写操作分离到独立库。冷热数据分离： 归档旧帖到分析型数据库（如 HBase）。Redis：瓶颈： 单机内存容量、带宽、单线程处理。改造：集群化： Redis Cluster 自动分片。区分数据类型： 热点数据（用户信息）用集群；超大 Value（如长帖缓存）考虑其他存储或压缩；统计类（UV/DAU）可保留。Elasticsearch：瓶颈： 索引过大导致查询慢、写入堆积。改造：分片策略优化： 增加主分片数（提前规划）。按时间分索引： 如 posts-202307，便于管理/查询/删除旧数据。Kafk
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: a：瓶颈： 单个 Topic 分区数限制吞吐量。改造： 增加分区数，生产者根据 Key（如 user_id）分区保证顺序性。应用层：瓶颈： 单节点处理能力。改造： 水平扩展无状态节点（更多 Tomcat 实例），通过 Nginx 负载均衡。微服务化拆分（如独立用户服务、帖子服务、消息服务），便于独立伸缩。监控与治理：加强：引入 APM（如 SkyWalking）、集中日志（ELK）、更强健的配置中心（Nacos）和熔断限流（Sentinel）。RedisIO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？”同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。举例子说一下阻塞 IO 和 IO
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  多路复用的差别？假设我是一名老师，让学生解答一道题目。我的第一种选择：按顺序逐个检查，先检查 A 同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种就是阻塞 IO，不具有并发能力我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 ARedis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。等子进
2025-08-11 10:56:00.284 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。AOF 重写期间命令会同时写入现有 AOF 文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。缺点是可能丢失两次快照期间的所有数据变更AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。缺点是文件体积较大，恢复速度慢。在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。如果是缓存场景，可以接受一定程度的数据丢失，
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布/订阅机制通知客户端主节点已经发生变化。主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。当
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。常见的数据分区有三种：节点取余、一致性哈希和哈希槽。节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过CRC16 算法计算后对 16384 取模，确定它属于哪个槽。布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来解决 Redis 缓存穿透的问题。布隆过滤器并不支持删除操作，这是它的一个重要限制。如何保证缓存和数据库数据的一致性？具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题：线程 A 要更新用户信息，先删除了缓存线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值线程 B 将查到的旧值重新放入缓存线程 A 完成数据库更新结果就是数据库是新的值，但缓存中还是旧值当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。即使缓存删除失败，消息队列的重试机制也能保证最终一致性。第二种
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。如何保证本地缓存和分布式缓存的一致性？为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pub/sub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内存压力。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis虽然不是专业的消息队列，但可以很好地实现延时队列功能。核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不存在时才能创建成功，EX 设置过期时间用以防止死锁。Kafka，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景Kafka 的设计Kafka 将消息以 topic 为单位进行归纳，发布消息的程序称为 Producer，消费消息的程序称为 Consumer。它是以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 Broker，Producer 通过网络将消息发送到 kafka 集群，集群向消费者提供消息，broker 在中间起到一个代理保存消息的中转站。Kafka 性能高原因利用了 PageCache 缓存磁盘顺序写零拷贝技术pull 拉模式优点高性能、高吞吐量、低延迟：Kafka 生产和消费消息的速度都达到每秒 10 万级高可用：所有消息持久化存储到磁盘，并支持数据备份防止数据丢失高并发：支持数千个客户端同时读写容错性：允许集群中节点失败（若副本数量为 n，则允许 n-1 个节点失败）高扩展性：Kafka 集群支持热伸缩，无须停机缺点没有完整的监控工具集不支持通配符主
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 题选择Kafka 的应用场景日志聚合：可收集各种服务的日志写入 kafka 的消息队列进行存储消息系统：广泛用于消息中间件系统解耦：在重要操作完成后，发送消息，由别的服务系统来完成其他操作流量削峰：一般用于秒杀或抢购活动中，来缓冲网站短时间内高流量带来的压力异步处理：通过异步处理机制，可以把一个消息放入队列中，但不立即处理它，在需要的时候再进行处理Kafka 为什么要把消息分区方便扩展：因为一个 topic 可以有多个 partition，每个 Partition 可用通过调整以适应它所在的机器，而一个 Topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率Kafka 中生产者运行流程一条消息发过来首先会被封装成一个 ProducerRecord 对象对该对象进行序列化处理（可以使用默认，也可以自定义序列化）对消息进行分区处理，分区的时候需要获取集群的元数据，决定这个消息会被发送到哪个主题的哪个分区分好区的消息不会直接发送到服务端，而是放入生产者的缓存区，多条消息会被封装成一个批次（
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Batch），默认一个批次的大小是 16KBSender 线程启动以后会从缓存里面去获取可以发送的批次Sender 线程把一个一个批次发送到服务端Kafka采用大部分消息系统遵循的传统模式：Producer 将消息推送到 Broker，Consumer 从 Broker 获取消息。负载均衡是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度保证系统整体运行效率与稳定性负载均衡Kakfa 的负载均衡就是每个 Broker 都有均等的机会为 Kafka 的客户端（生产者与消费者）提供服务，可以负载分散到所有集群中的机器上。Kafka 通过智能化的分区领导者选举来实现负载均衡，提供智能化的 Leader 选举算法，可在集群的所有机器上均匀分散各个 Partition的 Leader，从而整体上实现负载均衡。故障转移Kafka 的故障转移是通过使用会话机制实现的，每台 Kafka 服务器启动后会以会话的形式把自己注册到 Zookeeper 服务器上。一旦服务器运转出现问题，就会导致与 Zookeeper 的会话不能维持从而超时断连，此时 Kafka 集群会选举出另一台服务器来完全替代这台服务
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 器继续提供服务。Kafka 中 Zookeeper 的作用Kafka 是一个使用 Zookeeper 构建的分布式系统。Kafka 的各 Broker 在启动时都要在 Zookeeper上注册，由 Zookeeper统一协调管理。如果任何节点失败，可通过Zookeeper从先前提交的偏移量中恢复，因为它会做周期性提交偏移量工作。同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也是 Zookeeper在维护Kafka 中消费者与消费者组的关系与负载均衡实现Consumer Group 是 Kafka 独有的可扩展且具有容错性的消费者机制。一个组内可以有多个 Consumer，它们共享一个全局唯一的 Group ID。组内的所有 Consumer协调在一起来消费订阅主题（Topic）内的所有分区（Partition）。当然，每个 Partition只能由同一个 Consumer Group内的一个 Consumer 来消费。消费组内的消费者可以使用多线程的方式实现，消费者的数量通常不超过分区的数量，且二者最好保持整数倍的关系，这样不会造成有空
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲的消费者。Consumer 订阅的是 Topic 的 Partition，而不是 Message。所以在同一时间点上，订阅到同一个分区的 Consumer 必然属于不同的 Consumer GroupConsumer Group与 Consumer的关系是动态维护的，当一个 Consumer 进程挂掉或者是卡住时，该 Consumer 所订阅的 Partition会被重新分配到改组内的其他Consumer 上，当一个 Consumer加入到一个 Consumer Group中时，同样会从其他的 Consumer 中分配出一个或者多个 Partition到这个新加入的 Consumer。当生产者试图发送消息的速度快于 Broker 可以处理的速度时，通常会发生QueueFullException首先先进行判断生产者是否能够降低生产速率，如果生产者不能阻止这种情况，为了处理增加的负载，用户需要添加足够的 Broker。或者选择生产阻塞，设置Queue.enQueueTimeout.ms 为 -1，通过这样处理，如果队列已满的情况，生产者将组织而不是删除消息。或者容忍这种异常，进行消息丢弃。Consumer 如何
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 消费指定分区消息Cosumer 消费消息时，想 Broker 发出 fetch 请求去消费特定分区的消息，Consumer 可以通过指定消息在日志中的偏移量 offset，就可以从这个位置开始消息消息，Consumer 拥有了 offset 的控制权，也可以向后回滚去重新消费之前的消息。也可以使用 seek(Long topicPartition) 来指定消费的位置。Replica、Leader 和 Follower 三者的概念:Kafka 中的 Partition 是有序消息日志，为了实现高可用性，需要采用备份机制，将相同的数据复制到多个 Broker 上，而这些备份日志就是 Replica，目的是为了防止数据丢失。所有 Partition 的副本默认情况下都会均匀地分布到所有 Broker 上,一旦领导者副本所在的 Broker 宕机，Kafka 会从追随者副本中选举出新的领导者继续提供服务。Leader： 副本中的领导者。负责对外提供服务，与客户端进行交互。生产者总是向 Leader 副本些消息，消费者总是从 Leader 读消息Follower： 副本中的追随者。被动地追随 Leader，不能与外界进
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行交付。只是向 Leader 发送消息，请求 Leader把最新生产的消息发给它，进而保持同步。Kafka 中 AR、ISR、OSR 三者的概念AR：分区中所有副本称为 ARISR：所有与主副本保持一定程度同步的副本（包括主副本）称为 ISROSR：与主副本滞后过多的副本组成 OSR分区副本什么情况下会从 ISR 中剔出Leader 会维护一个与自己基本保持同步的 Replica列表，该列表称为 ISR，每个Partition都会有一个 ISR，而且是由 Leader 动态维护。所谓动态维护，就是说如果一个 Follower比一个 Leader 落后太多，或者超过一定时间未发起数据复制请求，则 Leader 将其从 ISR 中移除。当 ISR 中所有 Replica 都向 Leader 发送 ACK（Acknowledgement确认）时，Leader 才 commit分区副本中的 Leader 如果宕机但 ISR 却为空该如何处理可以通过配置 unclean.leader.election ：true：允许 OSR 成为 Leader，但是 OSR 的消息较为滞后，可能会出现消息不一致的问题false：会一
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直等待旧 leader 恢复正常，降低了可用性Kafka的 Producer 有三种 ack机制，参数值有 0、1 和 -10： 相当于异步操作，Producer 不需要 Leader 给予回复，发送完就认为成功，继续发送下一条（批）Message。此机制具有最低延迟，但是持久性可靠性也最差，当服务器发生故障时，很可能发生数据丢失。1： Kafka 默认的设置。表示 Producer 要 Leader 确认已成功接收数据才发送下一条（批）Message。不过 Leader 宕机，Follower 尚未复制的情况下，数据就会丢失。此机制提供了较好的持久性和较低的延迟性。-1： Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些Follower都确认消息已同步，Producer 才发送下一条（批）Message。此机制持久性可靠性最好，但延时性最差Kafka 的 consumer 如何消费数据在 Kafka中，Producers 将消息推送给 Broker 端，在 Consumer 和 Broker 建立连接之后，会主动去 Pull（或者说 Fetch）消息。这种模式有些优点
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，首先 Consumer端可以根据自己的消费能力适时的去 fetch消息并处理，且可以控制消息消费的进度（offset）；此外，消费者可以控制每次消费的数，实现批量消费。Kafka 的 Topic 中 Partition 数据是怎么存储到磁盘的用磁盘顺序写+内存页缓存+稀疏索引Topic 中的多个 Partition 以文件夹的形式保存到 Broker，每个分区序号从 0递增，且消息有序。Partition 文件下有多个 Segment（xxx.index，xxx.log），Segment文件里的大小和配置文件大小一致。默认为 1GB，但可以根据实际需要修改。如果大小大于 1GB时，会滚动一个新的 Segment并且以上一个 Segment 最后一条消息的偏移量命名。生产者发送消息│▼Leader Partition│▼ (追加写入)当前活跃 Segment: [00000000000000.log]│▼ (每隔 4KB 数据)更新 .index/.timeindex│▼ (Segment 满 1GB)创建新 Segment: [00000000000015.log]消费者请求 offset=520│▼查 .
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: index 文件 → 找到 offset=500 → position=10240│▼从 .log 文件 10240 位置顺序扫描│▼找到 offset=520 的消息返回Kafka 创建 Topic 后如何将分区放置到不同的 Broker 中Kafka创建 Topic 将分区放置到不同的 Broker 时遵循以下规则：副本因子不能大于 Broker 的个数。第一个分区（编号为 0）的第一个副本放置位置是随机从 Broker List 中选择的。其他分区的第一个副本放置位置相对于第 0个分区依次往后移。也就是如果有 3个 Broker，3 个分区，假设第一个分区放在第二个 Broker 上，那么第二个分区将会放在第三个 Broker 上；第三个分区将会放在第一个 Broker 上，更多 Broker 与更多分区依此类推。剩余的副本相对于第一个副本放置位置其实是由nextReplicaShift决定的，而这个数也是随机产生的。Kafka 中如何进行主从同步Kafka动态维护了一个同步状态的副本的集合（a set of In-SyncReplicas），简称 ISR，在这个集合中的结点都是和 Leader 保持高
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度一致的，任何一条消息只有被这个集合中的每个结点读取并追加到日志中，才会向外部通知“这个消息已经被提交”同步复制Producer 会先通过 Zookeeper识别到 Leader，然后向 Leader 发送消息，Leader收到消息后写入到本地 log文件。这个时候 Follower 再向 Leader Pull 消息，Pull回来的消息会写入的本地 log 中，写入完成后会向 Leader 发送 Ack 回执，等到 Leader 收到所有 Follower 的回执之后，才会向 Producer 回传 Ack。异步复制Kafka 中 Producer 异步发送消息是基于同步发送消息的接口来实现的，异步发送消息的实现很简单，客户端消息发送过来以后，会先放入一个 BlackingQueue队列中然后就返回了。Producer 再开启一个线程 ProducerSendTread 不断从队列中取出消息，然后调用同步发送消息的接口将消息发送给 Broker。Kafka 中什么情况下会出现消息丢失/不一致的问题消息发送时消息发送有两种方式：同步 - sync 和 异步 - async。默认是同步的方式，可以通过 prod
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ucer.type 属性进行配置，kafka 也可以通过配置 acks 属性来确认消息的生产0：表示不进行消息接收是否成功的确认1：表示当 leader 接收成功时的确认-1：表示 leader 和 follower 都接收成功的确认当 acks = 0 时，不和 Kafka 进行消息接收确认，可能会因为网络异常，缓冲区满的问题，导致消息丢失当 acks = 1 时，只有 leader 同步成功而 follower 尚未完成同步，如果 leader挂了，就会造成数据丢失消息消费时Kafka 有两个消息消费的 consumer 接口，分别是 low-level 和 hign-levellow-level：消费者自己维护 offset 等值，可以实现对 kafka 的完全控制high-level：封装了对 partition 和 offset，使用简单如果使用高级接口，可能存在一个消费者提取了一个消息后便提交了 offset，那么还没来得及消费就已经挂了，下次消费时的数据就是 offset + 1 的位置，那么原先 offset 的数据就丢失了Kafa 中如何保证顺序消费Kafka 的消费单元是 Partitio
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n，同一个 Partition 使用 offset 作为唯一标识保证顺序性，但这只是保证了在 Partition 内部的顺序性而不是 Topic 中的顺序，因此我们需要将所有消息发往统一 Partition 才能保证消息顺序消费，那么可以在发送的时候指定 MessageKey，同一个 key 的消息会发到同一个 Partition 中。Java介绍一下 javajava 是一门开源的跨平台的面向对象的计算机语言.跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而虚拟机是不同平台有不同版本,所以说 java 是跨平台的.面向对象有几个特点:1.封装两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者'封装'在一个不可分割的独立单元(即对象)中另一层含义指'信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能隐藏对象的功能实现细节。2.继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有父类相同的行为。3.多态多态是同一个行为具有多个不同表现形式或形态的能力。Java 语言中含有方法重载与对象多态两种形式的多态：1.方法重载：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。2.对象多态：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。Java有哪些数据类型？java 主要有两种数据类型1.基本数据类型基本数据有八个,byte,short,int,long 属于数值型中的整数型float,double属于数值型中的浮点型char属于字符型boolean属于布尔型2.引用数据类型引用数据类型有三个,分别是类,接口和数组接口和抽象类有什么区别？1.接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类是声明方法的存在而不去实现它的类。2.接口可以多继承，抽象类不行。3.接口定义方法，不能实现，默认是 public abstract，而抽象类可以实现部分方法。4.接口中基本数据类型为 public static final 并且需要给出初始值，而抽类象不是的。重载和重写什么区别？重写：1.参数列表必须完全与被重写的方法相同，
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否则不能称其为重写而是重载.2.返回的类型必须一直与被重写的方法的返回类型相同，否则不能称其为重写而是重载。3.访问修饰符的限制一定要大于被重写方法的访问修饰符4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。重载：1.必须具有不同的参数列表；2.可以有不同的返回类型，只要参数列表不同就可以了；3.可以有不同的访问修饰符；4.可以抛出不同的异常；常见的异常有哪些？NullPointerException 空指针异常ArrayIndexOutOfBoundsException 索引越界异常InputFormatException 输入类型不匹配SQLException SQL 异常IllegalArgumentException 非法参数NumberFormatException 类型转换异常 等等....异常要怎么解决？Java标准库内建了一些通用的异常，这些类以 Throwable 为顶层父类。Throwable又派生出 Error 类和 Exception类。错误：Error类以及他的子类的实例，代表了 JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，程序员应该关注 Exception 为父类的分支下的各种异常类。异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被 Java异常处理机制使用，是异常处理的核心。hashMap 线程不安全体现在哪里？在 hashMap1.7 中扩容的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法在任意版本的 hashMap 中，如果在插入数据时多个线程命中了同一个槽，可能会有数据覆盖的情况发生，导致线程不安全。说说进程和线程的区别？进程是系统资源分配和调度的基本单位，它能并发执行较高系统资源的利用率.线程是比进程更小的能独立运行的基本单位,创建、销毁、切换成本要小于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为 1，结果又是什么？Integer a = 1000，Integer b = 1000，a==b 结果为 falseInteger a = 1，Integer b = 1，a==b 结
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果为 true这道题主要考察 Integer 包装类缓存的范围,在-128~127 之间会缓存起来,比较的是直接缓存的数据,在此之外比较的是对象JMM 就是 Java 内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以 java 内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 java 程序在各种平台下都能达到一致的并发效果。Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。线程不能直接读写主内存中的变量。每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式cas 是什么？cas 叫做 CompareAndSwap，比较并交换，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是通过处理器的指令来保证操作的原子性，它主要包含三
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个变量：当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，如果相同就认为其他线程没有进行修改，而将新值写入内存聊聊 ReentrantLock 吧ReentrantLock 意为可重入锁，说起 ReentrantLock 就不得不说 AQS ，因为其底层就是使用 AQS 去实现的。ReentrantLock有两种模式，一种是公平锁，一种是非公平锁。公平模式下等待线程入队列后会严格按照队列顺序去执行非公平模式下等待线程入队列后有可能会出现插队情况公平锁第一步：获取状态的 state 的值如果 state=0 即代表锁没有被其它线程占用，执行第二步。如果 state!=0 则代表锁正在被其它线程占用，执行第三步。第二步：判断队列中是否有线程在排队等待如果不存在则直接将锁的所有者设置成当前线程，且更新状态 state 。如果存在就入队。第三步：判断锁的所有者是不是当前线程如果是则更新状态 state 的值。如果不是，线程进入队列排队等待。非公平锁获取状态的 state 的值如果 state=0 即代表锁
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 没有被其它线程占用，则设置当前锁的持有者为当前线程，该操作用 CAS 完成。如果不为 0或者设置失败，代表锁被占用进行下一步。此时获取 state 的值如果是，则给 state+1，获取锁如果不是，则进入队列等待如果是 0，代表刚好线程释放了锁，此时将锁的持有者设为自己如果不是 0，则查看线程持有者是不是自己多线程的创建方式有哪些？继承 Thread类，重写 run()方法public class Demo extends Thread{//重写父类 Thread的 run()public void run() {}public static void main(String[] args) {Demo d1 = new Demo();Demo d2 = new Demo();d1.start();d2.start();}}实现 Runnable接口，重写 run()public class Demo2 implements Runnable{//重写 Runnable接口的 run()public void run() {}public static void main(String[] args) {Th
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: read t1 = new Thread(new Demo2());Thread t2 = new Thread(new Demo2());t1.start();t2.start();}}实现 Callable 接口public class Demo implements Callable<String>{public String call() throws Exception {System.out.println("正在执行新建线程任务");Thread.sleep(2000);return "结果";}public static void main(String[] args) throws InterruptedException,ExecutionException {Demo d = new Demo();FutureTask<String> task = new FutureTask<>(d);Thread t = new Thread(task);t.start();//获取任务执行后返回的结果String result = task.get();}}使用线程池创建public class 
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Demo {public static void main(String[] args) {Executor threadPool = Executors.newFixedThreadPool(5);for(int i = 0 ;i < 10 ; i++) {threadPool.execute(new Runnable() {public void run() {//todo}});}}}线程池有哪些参数？corePoolSize：核心线程数，线程池中始终存活的线程数。2.maximumPoolSize: 最大线程数，线程池中允许的最大线程数。3.keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。4.unit: 单位，参数 keepAliveTime 的时间单位，7种可选。5.workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种可选。6.threadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。7.handler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicy。线程池的执行流程？判断线程池中的
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程数是否大于设置的核心线程数如果小于，就创建一个核心线程来执行任务如果大于，就会判断缓冲队列是否满了如果没有满，则放入队列，等待线程空闲时执行任务如果队列已经满了，则判断是否达到了线程池设置的最大线程数如果没有达到，就创建新线程来执行任务如果已经达到了最大线程数，则执行指定的拒绝策略深拷贝、浅拷贝是什么？浅拷贝并不是真的拷贝，只是复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象聊聊 ThreadLocal 吧ThreadLocal其实就是线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。一个对象的内存布局是怎么样的?1.对象头: 对象头又分为 MarkWord 和 Class Pointer 两部分。MarkWord:包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位,gc 记录信息等等。ClassPointer:用来指向对象对应的 Class 对象（其对应的元数据对象）的内存地址。在 32 位系统占 4 字节，在 64 位系统中占 8 字节
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 。2.Length:只在数组对象中存在，用来记录数组的长度，占用 4 字节3.Instance data: 对象实际数据，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括静态成员变量，因为其是在方法区维护的)4.Padding:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，padding 的作用就是补充字节，保证对象是 8 字节的整数倍。HashMapHashMap的底层数据结构是什么？JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。链表过长，会严重影响HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是糟糕的 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：当链
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 表超过 8 且数据总量超过 64 时会转红黑树。将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。解决 hash冲突的办法有哪些？HashMap用的哪种？解决 Hash 冲突方法有：开放定址法：也称为再散列法，基本思想就是，如果 p=H(key)出现冲突时，则以p为基础，再次 hash，p1=H(p),如果 p1再次出现冲突，则以 p1为基础，以此类推，直到找到一个不冲突的哈希地址 pi。因此开放定址法所需要的 hash表的长度要大于等于所需要存放的元素，而且因为存在再次 hash，所以只能在删除的节点上做标记，而不能真正删除节点。再哈希法：双重散列，多重散列，提供多个不同的 hash函数，当 R1=H1(key1)发生冲突时，再计算 R2=H2(key1)，直到没有冲突为止。这样做虽然不易产生堆集，但增加了计算的时间。链地址法：拉链法，将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第 i 个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。建立公共溢出区：将哈希表分为公共表和
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。HashMap中采用的是链地址法为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。为什么 hash 值要与 length-1 相与？把 hash 值对数组长度取模运算，模运算的消耗很大，没有位运算快。当 length 总是 2 的 n次方时，h& (length-1) 运算等价于对 length取模，也就是 h%length，但是 & 比 % 具有更高的效率HashMap数组的长度为什么是 2 的幂次方？2 的 N 次幂有助于减少碰撞的几率。如果 length 为 2的幂次方，则 length-1 转化为二进制必定是 11111……的形式，在与 h 的二进制与操作效率会非
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 常的快，而且空间不浪费。HashMap 的 put方法流程？首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；2、如果数组是空的，则调用 resize 进行初始化；3、如果没有哈希冲突直接放在对应的数组下标里；4、如果冲突了，且 key 已经存在，就覆盖掉 value；5、如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；6、如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。一般用什么作为 HashMap的 key?一般用 Integer、String 这种不可变类当作 HashMap 的 key，String 最为常见。因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的。Integer、String 这些类已经很规范的重写了hashCode() 以及 
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: equals() 方法。MySQL关系型和非关系型数据库的区别？关系型数据库的优点容易理解，因为它采用了关系模型来组织数据。可以保持数据的一致性。数据更新的开销比较小。支持复杂查询（带 where 子句的查询）非关系型数据库（NOSQL）的优点无需经过 SQL 层的解析，读写效率高。基于键值对，读写性能很高，易于扩展可以支持多种类型数据的存储，如图片，文档等等。扩展（可分为内存性数据库以及文档型数据库，比如 Redis，MongoDB，HBase 等，适合场景：数据量大高可用的日志系统/地理位置存储系统）。详细说一下一条 MySQL 语句执行的步骤Server 层按顺序执行 SQL 的步骤为：客户端请求 -> 连接器（验证用户身份，给予权限）查询缓存（存在缓存则直接返回，不存在则执行后续操作）分析器（对 SQL 进行词法分析和语法分析操作）优化器（主要对执行的 SQL 优化选择最优的执行方案方法）执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-> 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）MySQL 使用索引的原因？根本原因索引的出现，就是为了提高数据查询的效率，就像书的
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 目录一样。对于数据库的表而言，索引其实就是它的“目录”。扩展创建唯一性索引，可以保证数据库表中每一行数据的唯一性。帮助引擎层避免排序和临时表将随机 IO 变为顺序 IO，加速表和表之间的连接索引的三种常见底层数据结构以及优缺点三种常见的索引底层数据结构：分别是哈希表、有序数组和搜索树。哈希表这种适用于等值查询的场景，比如 memcached 以及其它一些 NoSQL 引擎，不适合范围查询，哈希表的数据是完全无序存储的。它只能回答“某个键值等于多少”的记录在哪，无法高效地查询“键值在某个范围之间”的所有记录（如WHERE id BETWEEN 10 AND 20）。需要扫描全表或遍历所有桶，效率极低 (O(n))。有序数组索引只适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高。N 叉树由于读写上的性能优点以及适配磁盘访问模式以及广泛应用在数据库引擎中。扩展（以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。）索引的常见类型以及它是如何发挥作用的？根据叶子节点的内容，索引类型分为主键索引和非主键索引。主键索引的叶子节点存的整行数据，在 InnoDB 里也被称为聚簇索引。非主键索引叶子节点存的主键的值，在 InnoDB 里也被称为二级索引MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件。MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data域存放的是数据记录的地址，索引文件和数据文件是分离的InnoDB 为什么设计 B+ 树索引？两个考虑因素：InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。CPU 将磁盘上的数据加载到内存中需要花费大量时间。为什么选择 B+ 树：哈希索引虽然能提供 O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机IO。而 B+ 树的所有叶节点可以通过指针来相互连接
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，减少顺序遍历带来的随机 IO。普通索引还是唯一索引？由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。什么是覆盖索引和索引下推？覆盖索引：在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。索引下推：MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。MySQL 的 change buffer 是什么？当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。注意唯一索引的更新就不
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 能使用 change buffer，实际上也只有普通索引可以使用。适用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 changebuffer 的维护代价。MySQL 是如何判断一行扫描数的？MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。redo log 和 binlog 的区别？为什么需要 redo log？redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash 后 内存
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据丢失的隐患，而 redo log 具备 crash safe 崩溃恢复 的能力。为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让innoDB 判断哪些数据已经刷盘，哪些数据还没有。但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交redo log 每次更新操作
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。redo log 内部结构是基于页的，记录了这个页的字段值变化，只要 crash 后读取redo log 进行重放，就可以恢复数据。这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备当数据库 crash 后，如何恢复未刷盘的数据到内存中？根据 redo log 和 binlog 的两阶段提交，未持久化的数据分为几种情况：change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从binlog 恢复 redo log，再从 redo log 恢复 change buffer。change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。redo log 写入方式？redo log 包括两部分内容，分别是内存中的日志缓冲(re
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: do log buffer)和磁盘上的日志文件(redo log file)。MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是 WAL。可以发现，redo log buffer 写入到 redo log file，是经过 OS buffer 中转的。其实可以通过参数 innodb_flush_log_at_trx_commit 进行配置，参数值含义如下：0：称为延迟写，事务提交时不会将 redo log buffer 中日志写入到 OS buffer，而是每秒写入 OS buffer 并调用写入到 redo log file 中。1：称为实时写，实时刷”，事务每次提交都会将 redo log buffer 中的日志写入 OS buffer 并保存到 redo log file 中。2： 称为实时写，延迟刷。每次事务提交写入到 OS buffer，然后是每秒将日志写入到 redo log file。
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: redo log 的执行流程?MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server层。MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。InnoDB 存储引擎层将 a修改为 1的这个操作记录到内存中。记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。binlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗?binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。但是
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。什么是两阶段提交？MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog，这就是"两阶段提交"。而两阶段提交就是让这两个状态保持逻辑上的一致。redolog 用于恢复主机故障时的未更新的物理数据，binlog 用于备份操作。两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。为什么需要两阶段提交呢?如果不用两阶段提交的话，可能会出现这样情况先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log备份恢复时，数据不一致。两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。在恢复数据时，redolog 状态为 com
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: mit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。MySQL 怎么知道 binlog 是完整的?一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event什么是 WAL 技术，有什么优点？WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过 redo log恢复，所以能够实现快速响应 SQL 语句redo log 日志格式redo log buffer (内存中)是由首尾相连的四个文件组成的，它们分别是：ib_logfile_1、ib_logfile_2、ib_logfile_3、ib_logfile_4。write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。chec
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: kpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。有了 redo log，当数据库发生宕机重启后，可通过 redo log 将未落盘的数据（check point 之后的数据）恢复，保证已经提交的事务记录不会丢失，这种能力称为 crash-safe。InnoDB 数据页结构一个数据页大致划分七个部分File Header：表示页的一些通用信息，占固定的 38 字节。page Header：表示数据页专有信息，占固定的 56 字节。inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的 26 字节。User Records：真正存储我们插入的数据，大小不固定。Free Space：页中尚未使用的部分，大小不固定。Page Directory：页中某些记录的相对位置，也就是各个槽对
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 应的记录在页面中的地址偏移量。File Trailer：用于检验页是否完整，占固定大小 8 字节。MySQL 是如何保证数据不丢失的？只要 redolog 和 binlog 保证持久化磁盘就能确保 MySQL 异常重启后回复数据在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。28、误删数据怎么办？DBA 的最核心的工作就是保证数据的完整性，先要做好预防，预防的话大概是通过这几个点：权限控制与分配(数据库和服务器权限)制作操作规范定期给开发进行培训搭建延迟备库做好 SQL 审计，只要是对线上数据有更改操作的语句(DML 和 DDL)都需要进行审核做好备份。备份的话又分为两个点 (1)如果数据量比较大，用物理备份xtrabackup。定期对数据库进行全量备份，也可以做增量备份。 (2)如果数据量较少，用 mysqldump 或者 mysqldumper。再利用 binlog 来恢复或者搭建主从的方式来恢复数据。 定期备份 binlog 文件也是很有必要的如果发
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 生了数据删除的操作，又可以从以下几个点来恢复:DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的myflash，也是一个不错的工具，本质都差不多，都是先解析 binlog event，然后在进行反转。把 delete 反转为 insert，insert 反转为 delete，update 前后 image 对调。所以必须设置 binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上。DDL 语句误操作(truncate 和 drop)，由于 DDL 语句不管 binlog_format 是 row还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长rm 删除：使用备份跨机房，或者最好是跨城市保存。DDL（数据定义语言） DML（数据操纵语言） DQL（数据查询语言）drop、truncate 和 delete 的区别DELETE 语句执行删除的
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。drop 语句将表所占用的空间全释放掉。在速度上，一般来说，drop> truncate > delete。如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大；如果想删除表，当然用 drop； 如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可；如果和事务有关，或者想触发 trigger，还是用 delete； 如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 W
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: eb、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。都说 InnoDB 好，那还要不要使用 MEMORY 引擎？内存表就是使用 memory 引擎创建的表为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉MySQL 是如何保证主备同步？主备关系的建立：一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从 binlog 文件 A的位置 P”开始同步，主库就从这个指定的位置开始往后发。而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。MySQL 主备切换流程：客户端读写都是直接访问 A，而节点 B是备库，只要将 A的更新都同步过来，到本地执行就可以保证数据是相同的。当需要切换的时候就
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 把节点换一下，A的节点 B的备库一个事务完整的同步过程：备库 B和主库 A建立来了长链接，主库 A内部专门线程用于维护了这个长链接。在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求 binlog 包括文件名和日志偏移量在备库 B上执行 start-slave 命令备库会启动两个线程：io_thread 和sql_thread 分别负责建立连接和读取中转日志进行解析执行备库读取主库传过来的 binlog 文件备库收到文件写到本地成为中转日志后来由于多线程复制方案的引入，sql_thread 演化成了多个线程什么是主备延迟主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有：有些部署条件下，备库所在机器的性能要比主库性能差。备库的压力较大。大事务，一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10分钟MySQL 的一主一备和一主多从有什么区别？在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上短时间提高 MySQL 性能的方法第一种方法：先处理掉那
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。 kill connection + id第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL选错了索引（force index）。InnoDB 为什么要用自增 ID 作为主键？自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。说一下 MySQL 的锁MySQL 在 server 层 和 存储引擎层 都运用了大量的锁MySQL server 层需要讲两种锁，第一种是 MDL(metadata lock) 元数据锁，第二种则 Table Lock 表锁。MDL 又名元数据锁，那么什么是元数据呢，任何描述数据库的
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内容就是元数据，比如我们的表结构、库结构等都是元数据。那为什么需要 MDL 呢？主要解决两个问题：事务隔离问题；数据复制问题InnoDB 有五种表级锁：IS（意向读锁）；IX（意向写锁）；S（读）；X（写）；AUTO-INC在对表进行 select/insert/delete/update 语句时候不会加表级锁IS 和 IX 的作用是为了判断表中是否有已经被加锁的记录自增主键的保障就是有 AUTO-INC 锁，是语句级别的：为表的某个列添加AUTO_INCREMENT 属性，之后在插⼊记录时，可以不指定该列的值，系统会⾃动为它赋上单调递增的值。InnoDB 4 种行级锁RecordLock：记录锁GapLock：间隙锁解决幻读；前一次查询不存在的东西在下一次查询出现了，其实就是事务 A中的两次查询之间事务 B执行插入操作被事务 A感知了Next-KeyLock：锁住某条记录又想阻止其它事务在改记录前面的间隙插入新纪录InsertIntentionLock：插入意向锁;如果插入到同一行间隙中的多个事务未插入到间隙内的同一位置则无须等待行锁和表锁的抉择全表扫描用行级锁索引是一种能提高数据库查询效率的数据结构。它可
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 以比作一本字典的目录，可以帮你快速找到对应的记录。索引一般存储在磁盘的文件中，它是占用物理空间的。正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。数据结构维度B+树索引：所有数据存储在叶子节点，复杂度为 O(logn)，适合范围查询。哈希索引: 适合等值查询，检索效率高，一次到位。全文索引：MyISAM 和 InnoDB 中都支持使用全文索引，一般在文本类型char,text,varchar 类型上创建。R-Tree 索引: 用来对 GIS 数据类型创建 SPATIAL 索引物理存储维度聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。（Innodb 存储引擎）非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。（Innodb 存储引擎）逻辑维度主键索引：一种特殊的唯一索引，不允许有空值。普通索引：MySQL 中基本索引类型，允许空值和重复值。联合索引：多个字段创建的索引，使用时遵循最左前缀原则。唯一索引：索引列中的值必须是唯一的，但是允许为空值。空间索引：MySQL5.7 之后支持空间索引，在空间索引这方面遵循 Op
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: enGIS 几何数据模型规则索引什么时候会失效？查询条件包含 or，可能导致索引失效如果字段类型是字符串，where 时一定用引号括起来，否则索引失效like 通配符可能导致索引失效。联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。在索引列上使用 mysql 的内置函数，索引失效。对索引列运算（如，+、-、*、/），索引失效。索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。索引字段上使用 is null， is not null，可能导致索引失效。左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。mysql 估计使用全表扫描要比使用索引快,则不使用索引哪些场景不适合建立索引？数据量少的表，不适合加索引更新比较频繁的也不适合加索引区分度低的字段不适合加索引（如性别）where、group by、order by 等后面没有使用到的字段，不需要建立索引已经有冗余的索引的情况（比如已经有 a,b 的联合索引，不需要再单独建立 a索引）为什么不是一般二叉树？如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找 树来说，查找效率更稳定，总体的查
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 找速度也更快。为什么不是平衡二叉树呢？我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作 为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说 的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果 是 B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数 就降下来啦，查询效率就快啦。那为什么不是 B 树而是 B+树呢？B+树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储 键值，也会存储数据。innodb 中页的默认大小是 16KB，如果不存储数据，那 么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就 会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数有会再次减少，数据查 询的效率也会更快。B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链 表连着的。那么 B+树使得范围查找，排序查找，分组查找以及去重查找变得 异常简单。一次 B+树索引树查找过程：select * from Temployee where age=32;这条 SQL 查询语句执行大概流程是这样的：搜索 idx_age 索引
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 树，将磁盘块 1加载到内存，由于 32<43,搜索左路分支，到磁盘寻址磁盘块 2。将磁盘块 2加载到内存中，由于 32<36,搜索左路分支，到磁盘寻址磁盘块 4。将磁盘块 4加载到内存中，在内存继续遍历，找到 age=32 的记录，取得 id = 400.拿到 id=400 后，回到 id 主键索引树。搜索 id 主键索引树，将磁盘块 1加载到内存，因为 300<400<500,所以在选择中间分支，到磁盘寻址磁盘块 3。虽然在磁盘块 3，找到了 id=400，但是它不是叶子节点，所以会继续往下找。到磁盘寻址磁盘块 8。将磁盘块 8加载内存，在内存遍历，找到 id=400 的记录，拿到 R4 这一行的数据，好的，大功告成。什么是回表？如何减少回表？当查询的数据在索引树中，找不到的时候，需要回到主键索引树中去获取，这个过程叫做回表。比如在第 6小节中，使用的查询 SQLselect * from Temployee where age=32;需要查询所有列的数据，idx_age 普通索引不能满足，需要拿到主键 id 的值后，再回到 id 主键索引查找获取，这个过程就是回表。什么是覆盖索引？如果我们查询 SQL 的
2025-08-11 10:56:00.285 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  select * 修改为 select id, age 的话，其实是不需要回表的。因为 id 和 age 的值，都在 idx_age 索引树的叶子节点上，这就涉及到覆盖索引的知识点了。覆盖索引是 select 的数据列只用从索引中就能够取得，不必回表，换句话说，查询列要被所建的索引覆盖聊聊索引的最左前缀原则、索引的最左前缀原则，可以是联合索引的最左 N个字段。比如你建立一个组合索引（a,b,c），其实可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。索引下推了解过吗？什么是索引下推select * from employee where name like '小%' and age=28 and sex='0';其中，name 和 age 为联合索引（idx_name_age）。如果是Mysql5.6之前，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键 id，然后回表找出数据行，再去对比年龄和性别等其他字段。有些朋友可能觉得奇怪，idx_name_age（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄 age 
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。因此，MySQL5.6 版本之后，选出包含“小”字后，顺表过滤 age=28大表如何添加索引？如果一张表数据量级是千万级别以上的，那么，如何给这张表添加索引？我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：先创建一张跟原表 A数据结构相同的新表 B。在新表 B添加需要加上的新索引。把原表 A数据导到新表 Brename 新表 B为原表的表名 A，原表 A换别的表名Hash 索引和 B+树区别是什么？你在设计索引是怎么抉择的？B+树可以进行范围查询，Hash 索引不能。B+树支持联合索引的最左侧原则，Hash 索引不支持。B+树支持 order by 排序，Hash 索引不支持。Hash 索引在等值查询上比 B+树效率更高。（但是索引列的重复值很多的话，Hash冲突，效率降低）。B+树使用 like 进行模糊查询的时候，like 后面（比如%开头）的话可以起到优化的作用，Hash 索引根
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 本无法进行模糊查询。索引有哪些优缺点？优点：索引可以加快数据查询速度，减少查询时间唯一索引可以保证数据库表中每一行的数据的唯一性缺点：创建索引和维护索引要耗费时间索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间以表中的数据进行增、删、改的时候，索引也要动态的维护。聚簇索引与非聚簇索引的区别聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它表示索引结构和数据一起存放的索引。非聚集索引是索引结构和数据分开存放的索引。接下来，我们分不同存存储引擎去聊哈~在 MySQL 的 InnoDB 存储引擎中， 聚簇索引与非聚簇索引最大的区别，在于叶节点是否存放一整行记录。聚簇索引叶子节点存储了一整行记录，而非聚簇索引叶子节点存储的是主键信息，因此，一般非聚簇索引还需要回表查询。一个表中只能拥有一个聚集索引（因为一般聚簇索引就是主键索引），而非聚集索引一个表则可以存在多个。一般来说，相对于非聚簇索引，聚簇索引查询效率更高，因为不用回表。而在 MyISM 存储引擎中，它的主键索引，普通索引都是非聚簇索引，因为数据和索引是分开的，叶子节点都使用一个地址指向真正的表数据。Nginx什么是 Ng
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: inx？Nginx 是一个 轻量级/高性能的反向代理 Web 服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。他实现非常高效的反向代理、负载平衡，他可以处理 2-3万并发连接数，官方监测能支持 5万并发，现在中国使用 nginx 网站用户有很多，例如：新浪、网易、 腾讯等。Nginx 怎么处理请求的？首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的 Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与 nginx 进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建nginx 对连接的封装，即 ngx
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: _connection_t 结构体。接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。Nginx 是如何实现高并发的？如果一个 server 采用一个进程(或者线程)负责一个 request 的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。而 Nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。每进来一个 request ，会有一个 worker 进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发 request ，并等待请求返回。那么，这个处理的 worker 不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有 request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker 才会来接手
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，这个 request 才会接着往下走。这就是为什么说，Nginx 基于事件模型。由于 web server 的工作性质决定了每个 request 的大部份生命都是在网络传输中，实际上花费在 server 机器上的时间片不多。这是几个进程就解决高并发的秘密所在。即：webserver 刚好属于网络 IO 密集型应用，不算是计算密集型。异步，非阻塞，使用 epoll ，和大量细节处的优化。也正是 Nginx 之所以然的技术基石。什么是正向代理？一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的 OpenVPN 等等。什么是反向代理？反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet 上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 反向代理总结就一句话：代理端代理的是服务端。反向代理服务器的优点是什么?反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。cookie 和 session 区别？共同：存放用户信息。存放的形式：key-value 格式 变量和变量内容键值对。区别：cookie存放在客户端浏览器每个域名对应一个 cookie，不能跨跃域名访问其他 cookie用户可以查看或修改 cookiehttp 响应报文里面给你浏览器设置钥匙（用于打开浏览器上锁头）session:存放在服务器（文件，数据库，redis）存放敏感信息锁头为什么 Nginx 不使用多线程？Apache: 创建多个进程或线程，而每个进程或线程都会为其分配 cpu 和内存（线程要比进程小的多，所以 worker 支持比 perfork 高的并发），并发过大会榨干服务器资源。Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置 Nginx 主进程的工作进程的数量）(epoll)，不会为每个请求分配 cpu 和内存资源，节省了大量资源，同时也减少了大量的 
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: CPU 的上下文切换。所以才使得 Nginx 支持更高的并发。nginx 和 apache 的区别轻量级，同样起 web 服务，比 apache 占用更少的内存和资源。抗并发，nginx 处理请求是异步非阻塞的，而 apache 则是阻塞性的，在高并发下 nginx 能保持低资源，低消耗高性能。高度模块化的设计，编写模块相对简单。最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，nginx是异步的，多个连接可以对应一个进程什么是动态资源、静态资源分离？动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。为什么要做动、静分离？在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。什么叫 CDN 服务？CDN ，即内容分发网络。其目的是，通过在现有的 Internet 中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务Nginx 怎么做的动静分离？只需要指定路径对应的目录。locatio
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n/可以使用正则表达式匹配。并指定对应的硬盘中的目录Nginx 负载均衡的算法怎么实现的?策略有哪些?为了避免服务器崩溃，大家会通过负载均衡的方式来分担服务器压力。将对台服务器组成一个集群，当用户访问时，先访问到一个转发服务器，再由转发服务器将访问分发到压力更小的服务器。Nginx 负载均衡实现的策略有以下种：1 .轮询(默认)每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。upstream backserver {server 192.168.0.12;server 192.168.0.13;}2. 权重 weightweight 的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。# 权重越高，在被访问的概率越大，如上例，分别是 20%，80%。upstream backserver {server 192.168.0.12 weight=2;server 192.168.0.13 weight=8;}3. ip_hash( IP 绑定)每个请求按访问 IP 的哈希结果分配，
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 使来自同一个 IP 的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的 session 共享问题upstream backserver {ip_hash;server 192.168.0.12:88;server 192.168.0.13:80;}Nginx 虚拟主机怎么配置?1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台3、基于 ip 的虚拟主机。location 的作用是什么？location 指令的作用是根据用户请求的 URI 来执行不同的应用，也就是根据用户请求的网站 URL 进行匹配，匹配成功即进行相关的操作限流怎么做的？Nginx 限流就是限制用户请求速度，防止服务器受不了限流有 3种正常限制访问频率（正常流量）突发限制访问频率（突发流量）限制并发连接数Nginx 的限流都是基于漏桶流算法漏桶流算法和令牌桶算法知道？漏桶算法漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 续进入的水直接溢出（拒绝请求），以此实现限流。令牌桶算法令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。Nginx 配置高可用性怎么配置？当上游服务器(真实访问服务器)，一旦出现故障或者是没有及时相应的话，应该直接轮训到下一台服务器，保证服务器的高可用生产中如何设置 worker 进程的数量呢？在有多个 cpu 的情况下，可以设置多个 worker，worker 进程的数量可以设置到和 cpu 的核心数一样多，如果在单个 cpu 上起多个 worker 进程，那么操作系统会在多个 worker 之间进行调度，这种情况会降低系统性能，如果只有一个 cpu，那么只启动一个 worker 进程就可以了。Java 基础八股文Java 语言具有哪些特点？Java 为纯面向对象的语言。它能够直接反应现实生活中的
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象。具有平台无关性。Java 利用 Java 虚拟机运行字节码，无论是在 Windows、Linux还是 MacOS 等其它平台对 Java 程序进行编译，编译后的程序可在其它平台运行。Java 为解释型语言，编译器把 Java 代码编译成平台无关的中间代码，然后在JVM 上解释运行，具有很好的可移植性。Java 提供了很多内置类库。如对多线程支持，对网络通信支持，最重要的一点是提供了垃圾回收器。Java 具有较好的安全性和健壮性。Java 提供了异常处理和垃圾回收机制，去除了 C++中难以理解的指针特性JDK 与 JRE 有什么区别？JDK：Java 开发工具包（Java Development Kit），提供了 Java 的开发环境和运行环境。JRE：Java 运行环境(Java Runtime Environment)，提供了 Java 运行所需的环境。JDK 包含了 JRE。如果只运行 Java 程序，安装 JRE 即可。要编写 Java 程序需安装 JDK简述 Java 基本数据类型byte: 占用 1 个字节，取值范围-128 ~ 127short: 占用 2 个字节，取值范围-215 ~ 21
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 5-1int：占用 4 个字节，取值范围-231 ~ 231-1long：占用 8 个字节float：占用 4 个字节double：占用 8 个字节char: 占用 2 个字节boolean：占用大小根据实现虚拟机不同有所差异简述自动装箱拆箱对于 Java 基本数据类型，均对应一个包装类。装箱就是自动将基本数据类型转换为包装器类型，如 int->Integer拆箱就是自动将包装器类型转换为基本数据类型，如 Integer->int简述 Java 访问修饰符default: 默认访问修饰符，在同一包内可见private: 在同一类内可见，不能修饰类protected : 对同一包内的类和所有子类可见，不能修饰类public: 对所有类可见构造方法、成员变量初始化以及静态成员变量三者的初始化顺序？先后顺序：静态成员变量、成员变量、构造方法。详细的先后顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。面向对象的三大特性？继承：对象的一个新类可以从现有的类中派生，派生类可以从它的基类那继承方法和实例变量，且
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 派生类可以修改或新增新的方法使之更适合特殊的需求。封装：将客观事物抽象成类，每个类可以把自身数据和方法只让可信的类或对象操作，对不可信的进行信息隐藏。多态：允许不同类的对象对同一消息作出响应。不同对象调用相同方法即使参数也相同，最终表现行为是不一样的为什么 Java 语言不支持多重继承？为了程序的结构能够更加清晰从而便于维护。假设 Java 语言支持多重继承，类C 继承自类 A 和类 B，如果类 A 和 B 都有自定义的成员方法 f()，那么当代码中调用类 C 的 f() 会产生二义性。Java 语言通过实现多个接口间接支持多重继承，接口由于只包含方法定义，不能有方法的实现，类 C 继承接口 A 与接口 B 时即使它们都有方法 f()，也不能直接调用方法，需实现具体的 f()方法才能调用，不会产生二义性。多重继承会使类型转换、构造方法的调用顺序变得复杂，会影响到性能Java 提供的多态机制？Java 提供了两种用于多态的机制，分别是重载与覆盖(重写)。重载：重载是指同一个类中有多个同名的方法，但这些方法有不同的参数，在编译期间就可以确定调用哪个方法。覆盖：覆盖是指派生类重写基类的方法，使用基类指向其子类的实例
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象，或接口的引用变量指向其实现类的实例对象，在程序调用的运行期根据引用变量所指的具体实例对象调用正在运行的那个对象的方法，即需要到运行期才能确定调用哪个方法重载与覆盖的区别？覆盖是父类与子类之间的关系，是垂直关系；重载是同一类中方法之间的关系，是水平关系。覆盖只能由一个方法或一对方法产生关系；重载是多个方法之间的关系。覆盖要求参数列表相同；重载要求参数列表不同。覆盖中，调用方法体是根据对象的类型来决定的，而重载是根据调用时实参表与形参表来对应选择方法体。重载方法可以改变返回值的类型，覆盖方法不能改变返回值的类型。接口和抽象类的相同点和不同点？相同点:都不能被实例化。接口的实现类或抽象类的子类需实现接口或抽象类中相应的方法才能被实例化。不同点：接口只能有方法定义，不能有方法的实现，而抽象类可以有方法的定义与实现。实现接口的关键字为 implements，继承抽象类的关键字为 extends。一个类可以实现多个接口，只能继承一个抽象类。当子类和父类之间存在逻辑上的层次结构，推荐使用抽象类，有利于功能的累积。当功能不需要，希望支持差别较大的两个或更多对象间的特定交互行为，推荐使用接口。使用接口能降低软件系统的耦合
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度，便于日后维护或添加删除方法Java 语言中关键字 static 的作用是什么？static 的主要作用有两个：为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。具体而言 static 又可分为 4 种使用方式：修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用。修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法。修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次。修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方法为什么要把 String 设计为不可变？节省空间：字符串常量存储在 JVM 的字符串池中可以被用户共享。提高效率：String 可以被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。安全：String 常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改简述 String/StringBuffer 与 StringBuilderString 类采用利用 final 修饰的字符数组进行字符串保存，因此不可变。如果对 String 类型对象修改，需要新建对象，将老字符和新增加的字符一并存进去。StringBuilder，采用无 final 修饰的字符数组进行保存，因此可变。但线程不安全。StringBuffer，采用无 final 修饰的字符数组进行保存，可理解为实现线程安全的 StringBuilder。判等运算符==与 equals 的区别？== 比较的是引用，equals 比较的是内容。如果变量是基础数据类型，== 用于比较其对应值是否相等。如果变量指向的是对象，== 用于比较两个对象是否指向同一块存储空间。equals 是 Object 类提供的方法之一，每个 J
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 类都继承自 Object 类，所以每个对象都具有 equals 这个方法。Object 类中定义的 equals 方法内部是直接调用 == 比较对象的。但通过覆盖的方法可以让它不是比较引用而是比较数据内容简述 Java 异常的分类Java 异常分为 Error（程序无法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。Error 常见的有 StackOverFlowError、OutOfMemoryError 等等。Exception 可分为运行时异常和非运行时异常。对于运行时异常，可以利用 trycatch 的方式进行处理，也可以不处理。对于非运行时异常，必须处理，不处理的话程序无法通过编译final、finally 和 finalize 的区别是什么？final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承。finally 作为异常处理的一部分，只能在 try/catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下。finalize 是 Object 类的一个方法
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存简述 Java 中 Class 对象java 中对象可以分为实例对象和 Class 对象，每一个类都有一个 Class 对象，其包含了与该类有关的信息。获取 Class 对象的方法：Class.forName(“类的全限定名”)实例对象.getClass()类名.classJava 反射机制是什么？Java 反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得 Java 具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射 API。简述 Java 序列化与反序列化的实现序列化：将 java 对象转化为字节序列，由此可以通过网络对象进行传输。反序列化：将字节序列转化为 java 对象。具体实现：实现 Serializable 接口，或实现 Externalizable 接口中的writeExte
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: rnal()与 readExternal()方法。简述 Java 的 ListList 是一个有序队列，在 Java 中有两种实现方式:ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。Java 中线程安全的基本数据结构有哪些HashTable: 哈希表的线程安全版，效率低ConcurrentHashMap：哈希表的线程安全版，效率高，用于替代 HashTableVector：线程安全版 ArraylistStack：线程安全版栈BlockingQueue 及其子类：线程安全版队列简述 Java 的 SetSet 即集合，该数据结构不允许元素重复且无序。Java 对 Set 有三种实现方式：HashSet 通过 HashMap 实现，HashMap 的 Key 即 HashSet 存储的元素，Value系统自定义一个名为 PRESENT 的 Object 类型常量。判断元素是否相同时，先比较 hashCode，相同后再利用 equ
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: als 比较，查询 O(1)LinkedHashSet 继承自 HashSet，通过 LinkedHashMap 实现，使用双向链表维护元素插入顺序。TreeSet 通过 TreeMap 实现的，底层数据结构是红黑树，添加元素到集合时按照比较规则将其插入合适的位置，保证插入后的集合仍然有序。查询 O(logn)简述 Java 的 HashMapJDK8 之前底层实现是数组 + 链表，JDK8 改为数组 + 链表/红黑树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因子 loadFactor。HashMap 中数据以键值对的形式存在，键对应的 hash 值用来计算数组下标，如果两个元素 key 的 hash 值一样，就会发生哈希冲突，被放到同一个链表上。table 数组记录 HashMap 的数据，每个下标对应一条链表，所有哈希冲突的数据都会被存放到同一条链表，Node/Entry 节点包含四个成员变量：key、value、next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红黑树为何 HashMap 线程不安全在 JDK1.7 中，HashMap 采用头插法插入元素
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，因此并发情况下会导致环形链表，产生死循环。虽然 JDK1.8 采用了尾插法解决了这个问题，但是并发下的 put 操作也会使前一个 key 被后一个 key 覆盖。由于 HashMap 有扩容机制存在，也存在 A 线程进行扩容后，B 线程执行 get 方法出现失误的情况。简述 Java 的 TreeMapTreeMap 是底层利用红黑树实现的 Map 结构，底层实现是一棵平衡的排序二叉树，由于红黑树的插入、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表无法提供键值对的有序输出，红黑树可以按照键的值的大小有序输出ArrayList、Vector 和 LinkedList 有什么共同点与区别？ArrayList、Vector 和 LinkedList 都是可伸缩的数组，即可以动态改变长度的数组。ArrayList 和 Vector 都是基于存储元素的 Object[] array 来实现的，它们会在内存中开辟一块连续的空间来存储，支持下标、索引访问。但在涉及插入元素时可能需要移动容器中的元素，插入效率较低。当存储元素超过容器的初始化容量大小，ArrayList 与 Vector 均会进
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行扩容。Vector 是线程安全的，其大部分方法是直接或间接同步的。ArrayList 不是线程安全的，其方法不具有同步性质。LinkedList 也不是线程安全的。LinkedList 采用双向列表实现，对数据索引需要从头开始遍历，因此随机访问效率较低，但在插入元素的时候不需要对数据进行移动，插入效率较高HashMap 和 Hashtable 有什么区别？HashMap 是 Hashtable 的轻量级实现，HashMap 允许 key 和 value 为 null，但最多允许一条记录的 key 为 null.而 HashTable 不允许。HashTable 中的方法是线程安全的，而 HashMap 不是。在多线程访问 HashMap需要提供额外的同步机制。Hashtable 使用 Enumeration 进行遍历，HashMap 使用 Iterator 进行遍历。如何决定使用 HashMap 还是 TreeMap?如果对 Map 进行插入、删除或定位一个元素的操作更频繁，HashMap 是更好的选择。如果需要对 key 集合进行有序的遍历，TreeMap 是更好的选择Collection 和 Colle
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ctions 有什么区别？Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。Collections 是一个包装类，包含了很多静态方法、不能被实例化，而是作为工具类使用，比如提供的排序方法：Collections.sort(list);提供的反转方法：Collections.reverse(list)。Java 并发编程1.并行跟并发有什么区别？并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。你是如何理解线程安全的？如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。可以从三个要素来确保线程安全：1 、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态2 、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行2.说说进程和线程的区别？进
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤：线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中3. 说说线程有几种创建方式？分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象。还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。调用 start 方法时会执行 run 方法，那怎么不直接调用 run 方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。说说 wait 方法和 notify 方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到：线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法、当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。如果调用的是 notifyAll 方法，会唤醒所
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有在这个共享变量上调用 wait 系列方法而被挂起的线程。说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。6.线程有几种状态？6 种。new 代表线程被创建但未启动；runnable 代表线程处于就绪或正在运行状态，由操作系统调度；blocked 代表线程被阻塞，等待获取锁；waiting 代表线程等待其他线程的通知或中断；timed_waiting 代表线程会等待一段时间，超时后自动恢复；terminated 代表线程执行完毕，生命周期结束。什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程使用完时间片后，就会让出 CPU 让其他线程占用。守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM内部，同时还启动了很多守护线程，比如垃圾回收线程。守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。sleep() 方法专属于 Thread 类。wait() 方法专属于 Object 类。waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。而当线程执行 wait 方法时，它会释放持有的对象锁，
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 因此其他线程也有机会获取该对象的锁。有个 int 的变量为 0，十个线程轮流对其进行++操作（循环 10000 次），结果大于 10 万还是小于等于 10 万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。读取变量的值。将读取到的值加 1。将结果写回变量。这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。详细解释下。多个线程在并发执行 ++ 操作时，可能出现以下竞态条件：线程 1 读取变量值为 0。线程 2 也读取变量值为 0。线程 1 进行加法运算并将结果 1 写回变量。线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。.ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程隔离。在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry数组，key 是 Thread
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Local 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value设置为 null，这样可以在很大程度上避免内存泄漏。ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。3、Map 的大小由 ThreadLocal 对象的多少决定。15.ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove()
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法释放内存空间。那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。ThreadLocalMap 的源码看过吗？有研究过。ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位：如果 i 位置被占用，尝试 i+1。如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。如果到达数组末尾，则回到数组头部，继续寻找空位。为什么要用线性探测法而不是 HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。拉链法还需要单独维护一个链表，甚至红黑树，
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不适合 ThreadLocal 这种场景ThreadLocalMap 扩容机制了解吗？了解。与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。父线程能用 ThreadLocal 给子线程传值吗？不能。因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。可以使用 InheritableThreadLocal 来解决这个问题。InheritableThreadLocal 的原理了解吗？了解。在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap：普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 newThread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有inheritableThreadLocals，
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 如果有，就会拷贝 InheritableThreadLocal 变量到子线程：说一下你对 Java 内存模型的理解？Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突volatile 了解吗？了解。第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为：StoreStore 屏障可以禁止普通写操作与 volatile 写操作的重排StoreLoad 屏障会禁止 volatile 写与 volatile 读重排LoadLoad 屏障会禁止 volatile 读与后续普通读操作重排LoadStore 屏障会禁止 volatile 读与后续普通写操作重排volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。锁synchronized 用过吗？用过，频率还
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 很高。synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象this。synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步：前者表示线程正在尝试获取 lock 对象的 Monitor；后者表示线程执行完了同步代码块，正在释放锁。你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——MarkWord，用于存储锁的状态，以及 Monito
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: r 对象的指针。synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。synchronized 怎么保证可见性？通过两步操作：加锁时，线程必须从主内存读取最新数据。释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 标记为未持有状态，并清除线程 ID 信息。synchronized 锁升级了解吗？JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。了解 synchronized 四种锁状态吗？了解。①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。未持有锁的线程通过 CAS 自旋等待锁释放④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，轻量级锁就会升级为重量级锁。此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。随着 JDK 版本的更新，synchronized 的性能得到了极大的优化：①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销详细解释一下：①、从无锁到偏向锁：当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。②、偏向锁的轻量级锁：进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。③、轻量级锁到重量级锁：轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。30.synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。synchronized 可以自动加锁和解锁，ReentrantLoc
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: k 需要手动 lock() 和 unlock()。并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为：ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制AQS 了解多少？AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为ReentrantLock 等类提供底层支持。AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。内部通过一个计数器 state 来跟踪锁的状态和
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。非公平锁和公平锁有什么不同？两句话回答：公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序CAS 了解多少？CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。在 CAS 中，有三个值：V：要更新的变量(var)E：预期值(expected)N：新值(new)先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 当前线程就放弃更新。这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe类实现的。怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS会误认为这个值没有发生变化。可以使用版本号/时间戳的方式来解决 ABA 问题。比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新Java 有哪些保证原子性的方法？比如说以 Atomic 开头的原子类，synchroni
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: zed 关键字，ReentrantLock 锁等原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有AtomicInteger、AtomicLong、AtomicReference 等。线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁R2；线程 2 持有锁 R2，等待锁 R1。第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请聊聊线程同步和互斥？
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: （补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程A 先执行，线程 B 再执行。互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费CPU 资源。聊聊悲观锁和乐观锁？（补充）悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。乐观锁认为冲突不
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。CountDownLatch 了解吗？CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。场景题：假如要查 10万多条数据，用线程池分成 20 个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。第三步，主线程调用 await() 方法，等待所有线程执行完毕。CyclicBarrier 和 CountDownLatch 有什么
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。说一下 JDK 8 中 ConcurrentHashMap 的实现原理？JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS的方式初始化，以确保只有一个线程在初始化数组。第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为synchronized 代码块来插入节点。插入的过程中会判断桶的哈希是否小于 0（f.hash >= 0），小于 0 说明是红黑树，大于等于 0 说明是链表。这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash值固定为 -2。第三步，如果链表长度超过 8，转换为红黑树。第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了ReentrantL
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ock，这样可以保证每个 Segment 都可以独立地加锁。而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized关键字。而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。比如说 put 的时候优先使用 CAS 尝试
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 插入，如果失败再使用 synchronized 代码块加锁。get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。说一下线程池的工作流程？可以简单总结为：任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。第一步，线程池通过 submit() 提交任务。第二步，线程池会先创建核心线程来执行任务。第三步，如果核心线程都在忙，任务会被放入任务队列中。第四步，如果任务
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。另外一版回答。第一步，创建线程池。第二步，调用线程池的 execute()方法，准备执行任务。如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务；如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列；如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务；如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。①、corePoolSize：核心线程数，长期存活，执行任务的主力。②、maxim
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: umPoolSize：线程池允许的最大线程数。③、workQueue：任务队列，存储等待执行的任务。④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。线程池的拒绝策略有哪些？AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute方法的线程。DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedB
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: lockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列SynchronousQueue。1 、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。2 、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是Integer.MAX_VALUE，几乎相当于一个无界队列。3 、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务4 、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。5 、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。submit 有返回值，适用于需要获取结果或
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 处理异常的场景。线程池怎么关闭知道吗？可以调用线程池的 shutdown 或 shutdownNow方法来关闭线程池。shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？1 、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。2 、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。有哪几种常见的线程池？主要有四种：固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等缓存线程池 Executors.newCach
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: edThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。能说一下四种常见线程池的原理吗？说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 & 周期性任务执行，使用DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate()方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 延迟执行。缺点是，如果任务执行时间 > 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。状态由 RUNNING→ SHUTDOWN→ STOP → TIDYING → TERMINATED 依次流转。RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，比如说setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点：第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中JVMJVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件；程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高说说 JVM 的组织架构（补充）JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器能说一下 JVM 的内存区域吗？按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 C/C++编写。在本地方法栈中，主要存放了 native
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。详细说明一下：比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。介绍一下方法区？方法区并不真实存在，属于 J
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 虚拟机规范中的一个逻辑概念，用于存储已被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的GC 分代年龄等信息。最后，JVM 会执行构造方法 <init> 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。new 对象时，堆会发生抢占吗？new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存，两个线程就发生了抢占JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。能说一下对象的内存布局吗？对象在内存中包括三部分：对象头、实例数据和对齐填充说说对象头的作用？对象头是对象存储在内存中的元信息，包含了 Mark Word、类型指针等信息。对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。比如说，如果对象头 + 实例数据 = 14 字节，则需要填充 2 个字节，使总长度变为 16 字节为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 就会降低。new Object() 对象的内存大小是多少？一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16字节；如果开启了压缩指针，就是 12 字节。实例数据的大小取决于对象的成员变量和它们的类型。对于 new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。对象的引用大小了解吗？在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。、在直接指针访问中，引用直接存储对象的内存地址；对象的实
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例数据和类型信息都存储在堆中固定的内存区域。说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。对
2025-08-11 10:56:00.286 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。STW 了解吗？了解。JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为 Stop TheWorld。简称 STW。如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：JVM 发出暂停信号；线程执行到安全点后，挂起自身并等待垃圾收集完成；垃圾回收器完成 GC 操作；线程恢复执行对象一定分配在堆中吗？不一定。默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程逃逸分析会带来什么好处？主要有三个。第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 无法被回收而导致的内存泄漏可能由哪些原因导致呢？静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。数据库、IO、Socket 等连接资源没有及时关闭；ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。有没有处理过内存泄漏问题？当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出StackOverflowError。讲讲 JVM 的垃圾回收机制（补充）垃圾回收就
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩/整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。这也是 G1、CMS 等主流垃圾收集器使用的主要算法。什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失Java 中可作为 GC Roots 的引用有哪几种？所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种虚拟机栈中的引用（方法的参数、局部变量等）本地方法栈中 JNI 的引用类静态变量运行时常量池中的常量（String 或 Class 类型）finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。筛选的条件是对象是否有必要执行 finalize()方法。如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法说说标记-清除算法？标记-清除算法分为两个阶段：标记：标记所有需要回收的对
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象清除：回收所有被标记的对象优点是实现简单，缺点是回收过程中会产生内存碎片说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。缺点是浪费了一半的内存空间。说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。缺点是移动对象的成本比较高。说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 集。年轻代包含 Eden 区以及两个 Survivor 区Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生FULL gc 怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 < 新生代历次Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一次Full GC。知道哪些垃圾收集器？JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。说说 CMS 收集器？CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC能详细说一下 CMS 的垃圾收集过程吗？CMS 使用标记-清除算法进行垃圾收集，分 4 大步：初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。并发清除：清除未被标记的对象，回收它们占用的内存空间。三色标记法的工作流程：①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。②、并发标记（Concurrent Marking）：在此
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。说说 G1 收集器？G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。G1 收集器的运行过程大致可划分为这几个步骤：①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区域和老年代区域。选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。3 、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1会尽可能地在这个时间内完成垃圾回收。CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 序使用的，并释放这些内存以便重新使用。这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。G1 非常适合大内存、多核处理器的环境。工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间JVM 调优37. 用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。你一般都怎么用 jmap？我一般会使用 jmap -heap <pid> 查看堆内存摘要，包括新生代、老年代、元空间等。
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 或者使用 jmap -histo <pid> 查看对象分布了解哪些可视化的性能监控工具？JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？-Xms：初始堆大小-Xmx：最大堆大小-XX:NewSize=n：设置年轻代大小-XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 1/4-XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n=3表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 1/5做过 JVM 调优吗？JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。在项目运行期间，我会使用 JVisualVM
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等CPU 占用过高怎么排查？首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。接着，使用 jstack 命令查看对应进程的线程堆栈信息。然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程ID接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收排查的方法主要分为以下几步：第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。或者使用 jmap 
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。第二步，通过 jmap 命令 dump 出堆内存信息第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过-XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。调整为 6 的话，会减少
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象该怎么排查 Full GC 频繁问题？通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过-XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。或者将大对象拆分成小对象，减少大对象的创建。比如说分页。假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说ThreadLocal、数据库连接、IO 资源等。了解类的加载机制吗？（补充）JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。类加载过程：包括加载、验证、准备、解析和初始化等步骤。双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。以下是整理自网络的一些 JVM 调优实例：网站流量浏览量暴增后，网站反应页面响很慢问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。定位：为了确认推测的正确性，在线上通过 jstat -gc 指令 看到 JVM 进行 GC 次数频率非常高，GC 所占用的时间非常长，所以基本推断就是因为 GC 频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。解决
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁 GC，所以这里问题在于新生代内存太小，所以这里可以增加 JVM 内存就行了，所以初步从原来的 2G 内存增加到 16G 内存。第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次 GC 的时间变长从而导致间接性的卡顿。定位：还是通过 jstat -gc 指令 查看到 的确 FGC 次数并不是很高，但是花费在 FGC 上的时间是非常高的,根据 GC 日志 查看到单次 FGC 的时间有达到几十秒的。解决方案： 因为 JVM 默认使用的是 PS+PO 的组合，PS+PO 垃圾标记和收集阶段都是 STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次 GC 时间过长，所以需要更换并发类的收集器，因为当前的 JDK 版本为 1.7，所以最后选择 CMS 垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。公司的后台系统
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，偶发性的引发 OOM 异常，堆内存溢出。因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从 4G 调整到 8G。但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError 参数 获得堆内存的 dump 文件。VisualVM 对 堆 dump 文件进行分析，通过 VisualVM 查看到占用内存最大的对象是 String 对象，本来想跟踪着 String 对象找到其引用的地方，但 dump 文件太大，跟踪进去的时候总是卡死，而 String 对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成 excel，这个过程会产生大量的 String 对象为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和 EXCEL 对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出jvm.gc.time：每分钟的 GC 耗时在 1s 以内，500ms 以内尤佳jvm.gc.meantime：每次 YGC 耗时在 100ms 以内，50ms 以内尤佳jvm.fullgc.count：FGC 最多几小时 1次，1天不到 1次尤佳jvm.fullgc.time：每次 FGC 耗时在 1s 以内，500ms 以内尤佳// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pidSpringSpring 是什么？Spring 是一个 Java 后端开发框架，其最核心的作用是帮我们管理 Java 对象其最重要的特性就是 IoC，也就是控制反转。以前我们要使用一个对象时，都要自己先 new 出来。但有了 Spring 之后，我们只需要告诉 Spr
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ing 我们需要什么对象，它就会自动帮我们创建好并注入到 Spring 容器当中。比如我在一个Service 类里需要用到 Dao 对象，只需要加个 @Autowired 注解，Spring 就会自动把 Dao 对象注入到 Spring 容器当中，这样就不需要我们手动去管理这些对象之间的依赖关系了。另外，Spring 还提供了 AOP，也就是面向切面编程，在我们需要做一些通用功能的时候特别有用，比如说日志记录、权限校验、事务管理这些，我们不用在每个方法里都写重复的代码，直接用 AOP 就能统一处理。Spring 的生态也特别丰富，像 Spring Boot 能让我们快速搭建项目，Spring MVC能帮我们处理 web 请求，Spring Data 能帮我们简化数据库操作，Spring Cloud能帮我们做微服务架构等等Spring 有哪些特性？首先最核心的就是 IoC 控制反转和 DI 依赖注入。这个我前面也提到了，就是Spring 能帮我们管理对象的创建和依赖关系。第二个就是 AOP 面向切面编程。这个在我们处理一些横切关注点的时候特别有用，比如说我们要给某些 Controller 方法都加上权限控制，如
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果没有 AOP 的话，每个方法都要写一遍加权代码，维护起来很麻烦。用了 AOP 之后，我们只需要写一个切面类，定义好切点和通知，就能统一处理了。事务管理也是同样的道理，加个 @Transactional 注解就搞定了。简单说一下什么是 AOP 和 IoC？AOP 面向切面编程，简单点说就是把一些通用的功能从业务代码里抽取出来，统一处理。比如说技术派中的 @MdcDot 注解的作用是配合 AOP 在日志中加入 MDC信息，方便进行日志追踪。IoC 控制反转是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。Spring 有哪些模块呢？首先是 Spring Core 模块，这是整个 Spring 框架的基础，包含了 IoC 容器和依赖注入等核心功能。还有 Spring Beans 模块，负责 Bean 的配置和管理。这两个模块基本上是其他所有模块的基础，不管用 Spring 的哪个功能都会用到。然后是 Spring Context 上下文模块，它在 Core 的基础上提供了更多企业级的功能，比如国际化、事件传播、资源加载这些。ApplicationContext 就是在这
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个模块里面的。Spring AOP 模块提供了面向切面编程的支持，我们用的@Transactional、自定义切面这些都是基于这个模块。Web 开发方面，Spring Web 模块提供了基础的 Web 功能，Spring WebMVC 就是我们常用的 MVC 框架，用来处理 HTTP 请求和响应。现在还有 Spring WebFlux，支持响应式编程。还有一些其他的模块，比如 Spring Security 负责安全认证，Spring Batch 处理批处理任务等等。现在我们基本都是用 Spring Boot 来开发，它把这些模块都整合好了，用起来更方便。Spring 有哪些常用注解呢？Spring 的注解挺多的，我按照不同的功能分类来说一下平时用得最多的那些。首先是 Bean 管理相关的注解。@Component 是最基础的，用来标识一个类是Spring 组件。像 @Service、@Repository、@Controller 这些都是 @Component的特化版本，分别用在服务层、数据访问层和控制器层。依赖注入方面，@Autowired 是用得最多的，可以标注在字段、setter 方法或者构造方法上。
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: @Qualifier 在有多个同类型 Bean 的时候用来指定具体注入哪一个。@Resource 和 @Autowired 功能差不多，不过它是按名称注入的。配置相关的注解也很常用。@Configuration 标识配置类，@Bean 用来定义 Bean，@Value 用来注入配置文件中的属性值。我们项目里的数据库连接信息、Redis 配置这些都是用 @Value 来注入的。@PropertySource 用来指定配置文件的位置。@RequestMapping 及其变体@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 用来映射 HTTP 请求。@PathVariable 获取路径参数，@RequestParam 获取请求参数，@RequestBody 接收 JSON 数据。、AOP 相关的注解，@Aspect 定义切面，@Pointcut 定义切点，@Before、@After、@Around 这些定义通知类型不过我们用得最多的还是@Transactional，基本上 Service 层需要保证事务原子性的方法都会加上这个注解。Spring 用了哪些设计模
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 式？Spring 框架里面确实用了很多设计模式，我从平时工作中能观察到的几个来说说。首先是工厂模式，这个在 Spring 里用得非常多。BeanFactory 就是一个典型的工厂，它负责创建和管理所有的 Bean 对象。我们平时用的 ApplicationContext其实也是 BeanFactory 的一个实现。当我们通过 @Autowired 获取一个 Bean的时候，底层就是通过工厂模式来创建和获取对象的。单例模式也是 Spring 的默认行为。默认情况下，Spring 容器中的 Bean 都是单例的，整个应用中只会有一个实例。这样可以节省内存，提高性能。当然我们也可以通过 @Scope 注解来改变 Bean 的作用域，比如设置为 prototype 就是每次获取都创建新实例。代理模式在 AOP 中用得特别多。Spring AOP 的底层实现就是基于动态代理的，对于实现了接口的类用 JDK 动态代理，没有实现接口的类用 CGLIB 代理。比如我们用 @Transactional 注解的时候，Spring 会为我们的类创建一个代理对象，在方法执行前后添加事务处理逻辑。Spring 如何实现单例模式？传统的
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 单例模式是在类的内部控制只能创建一个实例，比如用 private 构造方法加 static getInstance() 这种方式。但是 Spring 的单例是容器级别的，同一个 Bean 在整个 Spring 容器中只会有一个实例。具体的实现机制是这样的：Spring 在启动的时候会把所有的 Bean 定义信息加载进来，然后在 DefaultSingletonBeanRegistry 这个类里面维护了一个叫singletonObjects 的 ConcurrentHashMap，这个 Map 就是用来存储单例 Bean的。key 是 Bean 的名称，value 就是 Bean 的实例对象。当我们第一次获取某个 Bean 的时候，Spring 会先检查 singletonObjects 这个 Map 里面有没有这个 Bean，如果没有就会创建一个新的实例，然后放到 Map 里面。后面再获取同一个 Bean 的时候，直接从 Map 里面取就行了，这样就保证了单例。还有一个细节就是 Spring 为了解决循环依赖的问题，还用了三级缓存。除了singletonObjects 这个一级缓存，还有 earlySingl
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: etonObjects 二级缓存和singletonFactories 三级缓存。这样即使有循环依赖，Spring 也能正确处理Spring 容器和 Web 容器之间的区别知道吗？（补充）首先从概念上来说，Spring 容器是一个 IoC 容器，主要负责管理 Java 对象的生命周期和依赖关系。而 Web 容器，比如 Tomcat、Jetty 这些，是用来运行 Web应用的容器，负责处理 HTTP 请求和响应，管理 Servlet 的生命周期。从功能上看，Spring 容器专注于业务逻辑层面的对象管理，比如我们的 Service、Dao、Controller 这些 Bean 都是由 Spring 容器来创建和管理的。而 Web 容器主要处理网络通信，比如接收 HTTP 请求、解析请求参数、调用相应的 Servlet，然后把响应返回给客户端在实际项目中，这两个容器是相辅相成的。我们的 Web 项目部署在 Tomcat 上的时候，Tomcat 会负责接收 HTTP 请求，然后把请求交给 DispatcherServlet处理，而 DispatcherServlet 又会去 Spring 容器中查找相应的 Cont
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: roller来处理业务逻辑。现在我们都用 Spring Boot 了，Spring Boot 内置了 Tomcat，把 Web 容器和Spring 容器都整合在一起了，我们只需要运行一个 jar 包就可以了说一说什么是 IoC？IoC 的全称是 Inversion of Control，也就是控制反转。这里的“控制”指的是对象创建和依赖关系管理的控制权。以前我们写代码的时候，如果 A 类需要用到 B 类，我们就在 A 类里面直接 new一个 B 对象出来，这样 A 类就控制了 B 类对象的创建。有了 IoC 之后，这个控制权就“反转”了，不再由 A 类来控制 B 对象的创建，而是交给外部的容器来管理。DI 和 IoC 的区别了解吗？IoC 的思想是把对象创建和依赖关系的控制权由业务代码转移给 Spring 容器。这是一个比较抽象的概念，告诉我们应该怎么去设计系统架构。而 DI，也就是依赖注入，它是实现 IoC 这种思想的具体技术手段。在 Spring 里，我们用 @Autowired 注解就是在使用 DI 的字段注入方式。为什么要使用 IoC 呢？在日常开发中，如果我们需要实现某一个功能，可能至少需要两个以上
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系能说一下 IoC 的实现机制吗？好的，Spring IoC 的实现机制还是比较复杂的，我尽量用比较通俗的方式来解释一下整个流程。第一步是加载 Bean 的定义信息。Spring 会扫描我们配置的包路径，找到所有标注了 @Component、@Service、@Repository 这些注解的类，然后把这些类的元信息封装成 BeanDefinition 对象。第二步是 Bean 工厂的准备。Spring 会创建一个 DefaultListableBeanFactory作为 Bean 工厂来负责 Bean 的创建和管理。第三步是 Bean 的实例化和初始化。这个过程比较复杂，Spring 会根据BeanDefinition 来创建 Bean 实例。对于单例 Bean，Spring 会先检查缓存中是否已经存在，如果不存在就创建新实例。创建实例的时候会通过反射调用构造方法，然后进行属性注入，最后执行初始化回调方法。依赖注入的实现主
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 要是通过反射来完成的。比如我们用 @Autowired 标注了一个字段，Spring 在创建 Bean 的时候会扫描这个字段，然后从容器中找到对应类型的 Bean，通过反射的方式设置到这个字段上。你是怎么理解 Spring IoC 的？IoC 本质上一个超级工厂，这个工厂的产品就是各种 Bean 对象。我们通过 @Component、@Service 这些注解告诉工厂：“我要生产什么样的产品，这个产品有什么特性，需要什么原材料”。然后工厂里各种生产线，在 Spring 中就是各种 BeanPostProcessor。比如AutowiredAnnotationBeanPostProcessor 专门负责处理 @Autowired 注解。工厂里还有各种缓存机制用来存放产品，比如说 singletonObjects 是成品仓库，存放完工的单例 Bean；earlySingletonObjects 是半成品仓库，用来解决循环依赖问题。说说 BeanFactory 和 ApplicantContext 的区别?BeanFactory 算是 Spring 的“心脏”，而 ApplicantContext 可以说是Spri
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ng 的完整“身躯”。BeanFactory 提供了最基本的 IoC 能力。它就像是一个 Bean 工厂，负责 Bean的创建和管理。他采用的是懒加载的方式，也就是说只有当我们真正去获取某个Bean 的时候，它才会去创建这个 Bean。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上扩展了很多企业级的功能。它不仅包含了 BeanFactory 的所有功能，还提供了国际化支持、事件发布机制、AOP、JDBC、ORM 框架集成等等。ApplicationContext 采用的是饿加载的方式，容器启动的时候就会把所有的单例 Bean 都创建好，虽然这样会导致启动时间长一点，但运行时性能更好。另外一个重要的区别是生命周期管理。ApplicationContext 会自动调用 Bean的初始化和销毁方法，而 BeanFactory 需要我们手动管理。在 Spring Boot 项目中，我们可以通过 @Autowired 注入 ApplicationContext，或者通过实现 ApplicationContextAware 接口来获取 Applicatio
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: nContext。项目启动时 Spring 的 IoC 会做什么？第一件事是扫描和注册 Bean。IoC 容器会根据我们的配置，比如@ComponentScan 指定的包路径，去扫描所有标注了 @Component、@Service、@Controller 这些注解的类。然后把这些类的元信息包装成 BeanDefinition 对象，注册到容器的 BeanDefinitionRegistry 中。这个阶段只是收集信息，还没有真正创建对象。第二件事是 Bean 的实例化和注入。这是最核心的过程，IoC 容器会按照依赖关系的顺序开始创建 Bean 实例。对于单例 Bean，容器会通过反射调用构造方法创建实例，然后进行属性注入，最后执行初始化回调方法在依赖注入时，容器会根据 @Autowired、@Resource 这些注解，把相应的依赖对象注入到目标 Bean 中。比如 UserService 需要 UserDao，容器就会把UserDao 的实例注入到 UserService 中。说说 Spring 的 Bean 实例化方式？Spring 提供了 4 种方式来实例化 Bean，以满足不同场景下的需求第一种是通过
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法实例化，这是最常用的方式。当我们用 @Component、@Service 这些注解标注类的时候，Spring 默认通过无参构造器来创建实例的。如果类只有一个有参构造方法，Spring 会自动进行构造方法注入。第二种是通过静态工厂方法实例化。有时候对象的创建比较复杂，我们会写一个静态工厂方法来创建，然后用 @Bean 注解来标注这个方法。Spring 会调用这个静态方法来获取 Bean 实例。第三种是通过实例工厂方法实例化。这种方式是先创建工厂对象，然后通过工厂对象的方法来创建 Bean：第四种是通过 FactoryBean 接口实例化。这是 Spring 提供的一个特殊接口，当我们需要创建复杂对象的时候特别有用：你是怎么理解 Bean 的？在我看来，Bean 本质上就是由 Spring 容器管理的 Java 对象，但它和普通的Java 对象有很大区别。普通的 Java 对象我们是通过 new 关键字创建的。而Bean 是交给 Spring 容器来管理的，从创建到销毁都由容器负责。从实际使用的角度来说，我们项目里的 Service、Dao、Controller 这些都是Bean。比如 UserServ
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ice 被标注了 @Service 注解，它就成了一个 Bean，Spring会自动创建它的实例，管理它的依赖关系，当其他地方需要用到 UserService 的时候，Spring 就会把这个实例注入进去。这种依赖注入的方式让对象之间的关系变得松耦合。Spring 提供了多种 Bean 的配置方式，基于注解的方式是最常用的。@Component 和 @Bean 有什么区别？首先从使用上来说，@Component 是标注在类上的，而 @Bean 是标注在方法上的。@Component 告诉 Spring 这个类是一个组件，请把它注册为 Bean，而 @Bean 则告诉 Spring 请将这个方法返回的对象注册为 Bean。从控制权的角度来说，@Component 是由 Spring 自动创建和管理的。而 @Bean 则是由我们手动创建的，然后再交给 Spring 管理，我们对对象的创建过程有完全的控制权。能说一下 Bean 的生命周期吗？Bean 的生命周期可以分为 5 个主要阶段，我按照实际的执行顺序来说一下。第一个阶段是实例化。Spring 容器会根据 BeanDefinition，通过反射调用 Bean的
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法创建对象实例。如果有多个构造方法，Spring 会根据依赖注入的规则选择合适的构造方法。第二阶段是属性赋值。这个阶段 Spring 会给 Bean 的属性赋值，包括通过@Autowired、@Resource 这些注解注入的依赖对象，以及通过 @Value 注入的配置值第三阶段是初始化。这个阶段会依次执行：@PostConstruct 标注的方法InitializingBean 接口的 afterPropertiesSet 方法通过 @Bean 的 initMethod 指定的初始化方法初始化后，Spring 还会调用所有注册的 BeanPostProcessor 后置处理方法。这个阶段经常用来创建代理对象，比如 AOP 代理。第五阶段是使用 Bean。比如我们的 Controller 调用 Service，Service 调用DAO。最后是销毁阶段。当容器关闭或者 Bean 被移除的时候，会依次执行：@PreDestroy 标注的方法DisposableBean 接口的 destroy 方法通过 @Bean 的 destroyMethod 指定的销毁方法Aware 类型的接口有什么作用？Aware 
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 接口在 Spring 中是一个很有意思的设计，它们的作用是让 Bean 能够感知到 Spring 容器的一些内部组件。从设计理念来说，Aware 接口实现了一种“回调”机制。正常情况下，Bean 不应该直接依赖 Spring 容器，这样可以保持代码的独立性。但有些时候，Bean 确实需要获取容器的一些信息或者组件，Aware 接口就提供了这样一个能力。什么是自动装配？自动装配的本质就是让 Spring 容器自动帮我们完成 Bean 之间的依赖关系注入，而不需要我们手动去指定每个依赖。简单来说，就是“我们不用告诉 Spring具体怎么注入，Spring 自己会想办法找到合适的 Bean 注入进来”。自动装配的工作原理简单来说就是，Spring 容器在启动时自动扫描@ComponentScan 指定包路径下的所有类，然后根据类上的注解，比如@Autowired、@Resource 等，来判断哪些 Bean 需要被自动装配。之后分析每个 Bean 的依赖关系，在创建 Bean 的时候，根据装配规则自动找到合适的依赖 Bean，最后根据反射将这些依赖注入到目标 Bean 中Bean 的作用域有哪些Bean 的作用域决
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 定了 Bean 实例的生命周期和创建策略，singleton 是默认的作用域。整个 Spring 容器中只会有一个 Bean 实例。不管在多少个地方注入这个 Bean，拿到的都是同一个对象。生命周期和 Spring 容器相同，容器启动时创建，容器销毁时销毁。实际开发中，像 Service、Dao 这些业务组件基本都是单例的，因为单例既能节省内存，又能提高性能。当把 scope 设置为 prototype 时，每次从容器中获取 Bean 的时候都会创建一个新的实例。当需要处理一些有状态的 Bean 时会用到 prototype，比如每个订单处理器需要维护不同的状态信息如果作用于是 request，表示在 Web 应用中，每个 HTTP 请求都会创建一个新的 Bean 实例，请求结束后 Bean 就被销毁。如果作用于是 session，表示在 Web 应用中，每个 HTTP 会话都会创建一个新的 Bean 实例，会话结束后 Bean 被销毁。application 作用域表示在整个应用中只有一个 Bean 实例，类似于 singleton，但它的生命周期与 ServletContext 绑定。Spring 中的单
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例 Bean 会存在线程安全问题吗？首先要明确一点。Spring 容器本身保证了 Bean 创建过程的线程安全，也就是说不会出现多个线程同时创建同一个单例 Bean 的情况。但是 Bean 创建完成后的使用过程，Spring 就不管了换句话说，单例 Bean 在被创建后，如果它的内部状态是可变的，那么在多线程环境下就可能会出现线程安全问题单例 Bean 的线程安全问题怎么解决呢？第一种，使用局部变量，也就是使用无状态的单例 Bean，把所有状态都通过方法参数传递：第二种，当确实需要维护线程相关的状态时，可以使用 ThreadLocal 来保存状态。ThreadLocal 可以保证每个线程都有自己的变量副本，互不干扰。第三种，如果需要缓存数据或者计数，使用 JUC 包下的线程安全类，比如说AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。第四种，对于复杂的状态操作，可以使用 synchronized 或 Lock：第五种，如果 Bean 确实需要维护状态，可以考虑将其改为 prototype 作用域，这样每次注入都会创建一个新的实例，避免了多线程共享同
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个实例的问题。说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖：一级缓存：存放完全初始化好的单例 Bean。二级缓存：存放正在创建但未完全初始化的 Bean 实例。三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。三级缓存解决循环依赖的过程是什么样的？实例化 Bean 时，将其早期引用放入三级缓存。其他依赖该 Bean 的对象，可以从缓存中获取其引用。初始化完成后，将 Bean 移入一级缓存。假如 A、B 两个类发生循环依赖：A 实例的初始化过程：1 、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道2 、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。3 、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。4 、发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 已经实例化并且初始化完成了，把 B 放入⼀级缓存5 、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存6 、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor在 Bean 的初始化阶段，会通过 
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Bean 后置处理器来进行一些前置和后置的处理。实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是AutowiredAnnotationBeanPostProcessor。Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。AOP 有哪些核心概念？切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法切点（Pointcut）：对连接点进行拦截的定位通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强目标对象 （Target）：代理的目标对象引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。在 Spring 容器初始化 Bean的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 理对象中织入切面逻辑。这一过程发生在Spring 容器的后处理器（BeanPostProcessor）阶段。简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像@Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现AOP。③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。AOP 的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。第一步，自定义注解作为切点第二
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间第三步，在使用的地方加上自定义注解第四步，当接口被调用时，就可以看到对应的执行日志。说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。说说 Spring AOP 和 AspectJ AOP 区别?说
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。反射：运行时操作类的元信息的底层能力动态代理：基于反射实现方法拦截的设计模式Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚.我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部实现事务的控制。介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。说说两者的区别？编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务说说 Spring 的事务隔离级别？好，事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: TransactionDefinition 接口中。①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 ），MySQL默认的是可重复读，Oracle 默认的读已提交。②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 下文如何在方法调用链中传播。Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。事务传播机制是使用ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。第一步，在 Bean 初始化阶段创建代理对象：Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性切面，也就是@Transactional 注解及其属性值。然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。第二步，在执行目标方法时进行事务增强操作：当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为 MethodInterceptor，事务增强对该接口的实现为 TransactionInterceptor，@Transactional 应用在非 public 修饰的方法上如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。Spring MVC 的核心组件？DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping将不同
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的请求映射到不同的 Handler。HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。Spring MVC 的
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。然后 DispatcherServlet 通过 ViewResolver 解析视图。最后，DispatcherServlet 渲染视图并将响应返回给客户端①、发起请求：客户端通过 HTTP 协议向服务器发起请求。②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个Controller 应该处理这个请求。通常会根据请求的 URL 来确定。④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用HandlerAdapter 来调用 Controller 的处理方法。⑤、执行处理器：Controller 处理请求，处理完后
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用ViewResolver 来解析视图名称，找到具体的视图页面。⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。⑧、响结果：DispatcherServlet 将视图结果返回给客户端。Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller 注解的类，还可以是实现了特定接口的其他类（如
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  HttpRequestHandler 或SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。SpringMVC Restful 风格的接口的流程是什么样的呢？我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同：客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServletDispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个 ControllerController 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理HandlerAdapter 执行完对 Controller 的请求，会调用HandlerMethodReturnValueHandler 去处理返回值，主要的过程：5.1. 调用 RequestResponseBodyMethodProcessor，创建ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入ServletServerHttpResponse 的 OutputStream 输出流中5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化执行完请求后，返回的 ModealAndView 为 null，ServletServerHtt
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: pResponse里也已经写入了响应，所以不用关心 View 的处理介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架Spring Boot 的优点非常多，比如说：Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和Spring MVC。Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控Spring Boot 常用注解有哪些？@SpringBootApplication：Spring Boot 应用的入口，用在启动类上。还有一些 Spring 框架本身的注解，比如 @Component、@RestController、@Service、@ConfigurationProperties、@Transact
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ional 等。SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。SpringBoot 的自动装配机制主要通过 @EnableAutoConfiguration 注解实现，这个注解是 @SpringBootApplication 注解的一部分，后者是一个组合注解，包括 @SpringBootConfiguration、@ComponentScan 和@EnableAutoConfiguration。@EnableAutoConfiguration 注解通过AutoConfigurationImportSelector 类来加载自动装配类，这个类实现了ImportSelector 接口的 selectImports 方法，该方法负责获取所有符
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat和 Jackson 等依赖。自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。为什么使用 Spring Boot？Spring Boot 解决了传统 Spring 开发的三大痛点：简化配置：自动装配 + 起步依赖，告别 XML 配置地狱快速启动：内嵌 Tomcat/Jetty，一键启动独立运行应用生产就绪：Actua
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: tor 提供健康检查、监控等运维能力@Import 的作用实现模块化配置导入，三种用法导入普通配置类：@Import(MyConfig.class)导入 ImportSelector 实现（自动装配核心）：@Import(AutoConfigurationImportSelector.class)导入 ImportBeanDefinitionRegistrar 实现（动态注册 Bean）Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责：第一步，创建 SpringApplication 实例，负责应用的启动和初始化；第二步，从 application.yml 中加载配置文件和环境变量；第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入；第四步，启动内嵌的 Web 容器。第五步，发布启动完成事件 ApplicationReadyEvent，并调用ApplicationRunner 的 run 方法完成启动后的逻辑。了解@SpringBootApplication 注解吗？@SpringBootApplic
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ation 是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解：@SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。@EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。@ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication 注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部的一系列处理流程Spring Boot 利用 Java 反射机制来读取传递给 run 方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication@SpringBootApplication 是一个组合注解，它里面的@ComponentScan 注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。如果需要自定义包扫描路径，可以在@SpringBootApplication 注解上添加@ComponentScan 注解，指定要扫描的包路径。这种方式会覆盖默认的包扫描路径，只扫描 com.github.paicoding.forum 包及其子包下的所有组件。SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种Model-View-Controller（模型-视图-控制器）
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的开发模式。Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个SpringMVC 的 Web 项目。Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。微服务架构主要要解决哪些问题？服务很多，客户端怎么访问，如何提供对外网关?这么多服务，服务之间如何通信? HTTP 还是 RPC?这
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 么多服务，如何治理? 服务的注册和发现。服务挂了怎么办？熔断机制SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml：用 SpringTask资源占用太高，有什么其他的方式解决？（补充）第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。第二，使用数据库调度器（如 Quartz）Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。Spring Cache 和 Redis 有什么区别？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。Redis 是一个分布式的缓存中间件，支持多种数
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。说说什么是 MyBatis?Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL语句的功底有一定要求SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中JDBC 编程有哪些不足之处，MyBatis 是如何解决的？1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xm
2025-08-11 10:56:00.287 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: l文件中与 java 代码分离。3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。Hibernate 和 MyBatis 有什么区别？不同点1）映射关系MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂2）SQL 优化和移植性Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。MyBatis 需要
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步：1）创建 SqlSessionFactory2）通过 SqlSessionFactory 创建 SqlSessionSqlSession（会话）可以理解为程序和数据库之间的桥梁3）通过 sqlsession 执行数据库操作，可以通过 SqlSession 实例来直接执行已映射的 SQL 语句：4）调用 session.commit()提交事务5）调用 session.close()关闭会话说说 MyBatis 生命周期？SqlSessionFactoryBuilder一旦创建了 SqlSessionFactory，就不再需要它了。 因此SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。SqlSessionSqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。Mapper映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。在 mapper 中如何传递多个参数？#{}和${}的区别?①、当使用 #{} 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。由于 #{} 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。<select id="selectUser" resultType="User">SELECT * FROM users WHERE id = #{id}</select>②、当使用 ${} 时，参数的值会
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直接替换到 SQL 语句中去，而不会经过预处理。这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or1=1，这样就会导致查询出所有用户的结果。${} 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。<select id="selectUsersByOrder" resultType="User">SELECT * FROM users ORDER BY ${columnName} ASC</select>模糊查询 like 语句该怎么写?CONCAT('%',#{question},'%') 使用 CONCAT()函数，（推荐 ✨）说说 Mybatis 的一级、二级缓存？一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或close 之后，该 SqlSession 中的所有 Ca
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: che 就将清空，MyBatis 默认打开一级缓存。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。能说说 MyBatis 的工作原理吗？按工作原理，可以分为两大步：生成会话工厂、会话运行构造会话工厂也可以分为两步：获取配置获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括：读取配置文件，包括基础配置文件和映射文件初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等提供一个单例，作为会话工厂构建的重要参数它的构建过程也会初始化一些环境变量，比如数据源构建 SqlSessionFactorySqlSessionFactory 只是一
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合：Executor（执行器）Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法StatementHandler（数据库会话器）StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个handler 去执行 query。ParameterHandler （参数处理器）PreparedStatementHandler 里对 sql 进行了预编译处理ResultSetHandler（结果处理器）我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂SqlSessionFactory。创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。参数处理：对输入参数的类型进行处理，并预编译。结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。MyBatis 的功能架构是什么样的？我们一般把 Mybatis 的功能架构分为三层：API 接口层：提供给外部使用的接口 API，开发人员通
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑Mybatis 都有哪些 Executor 执行器？Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String, Statement>内，供下一次使用。简言之，就是重复使
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 用Statement 对象。BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。说说 JDBC 的执行步骤？Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤：在与数据库建立连接之前，首先需要通过 Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了 DriverManager 类中。Class.forName("com.mysql.cj.jdbc.Driver");第二步，建立数据库连接使用 DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数C
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: onnection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/databaseName", "username","password");第三步，创建 Statement 对象通过建立的数据库连接对象 Connection 创建 Statement、PreparedStatement或 CallableStatement 对象，用于执行 SQL 语句Statement stmt = conn.createStatement();第四步，执行 SQL 语句使用 Statement 或 PreparedStatement 对象执行 SQL 语句。执行查询（SELECT）语句时，使用 executeQuery()方法，它返回 ResultSet 对象；执行更新（INSERT、UPDATE、DELETE）语句时，使用 executeUpdate()方法，它返回一个整数表示受影响的行数。ResultSet rs = stmt.executeQuery("SELECT * FROM tableName");int affectedRow
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: s = stmt.executeUpdate("UPDATE tableName SET column ='value' WHERE condition");第五步，处理结果集如果执行的是查询操作，需要处理 ResultSet 对象来获取数据第六步，关闭资源最后，需要依次关闭 ResultSet、Statement 和 Connection 等资源，释放数据库连接等资源创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是 java.sql.Connection 对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。Connection 对象代表了应用程序和数据库的一个连接会话。通过调用 DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。一旦获得 Connection 对象，就可以使用它来创建执行 SQL 语句的 Statement、PreparedStatement 和 CallableStatement 对象，以及管理事务等。什么是 SQL 注入？如
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。为了防止 SQL 注入，可以采取以下措施：①、使用参数化查询使用参数化查询，即使用 PreparedStatement 对象，通过 setXxx 方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。②、限制用户输入对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或SQL 关键字。③、使用 ORM 框架比如，在 MyBatis 中，使用#{}占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。<select id="selectUser" resultType="User">SELECT * FROM users WHERE username = #{userName}</select>分布式说说 CAP 原则？、CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition toleran
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ce（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。假如现在有这样的场景：用户访问了 N1，修改了 D1 的数据。用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。接下来：保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。ASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性（Eventual consistency）的效果。BASE 的主要含义：Basically Available（基本可用）什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。Soft State（软状态）什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。Eventually Consistent（最终一致性）上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL 分布式锁、ZooKepper 分布式锁、Redis分布式锁。MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。加锁的时候，在锁表中增加一条记录
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 即可；释放锁的时候删除记录就行。如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。基于 Redis 的分布式锁核心思想： 利用 Redis 单线程执行命令的特性以及其丰富的数据结构和命令（尤其是 SETNX, SET with NX/PX/EX, Lua 脚本）来实现高性能锁。当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。什么是分布式事务
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性分布式事务有哪些常见的实现方案？二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。本地消息表：在数据库中存储事务事件，通过定时任务处理消息。基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。7.1 说说 2PC 两阶段提交？两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。优点：尽量保证了数据的强一致，实现成本
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 较低，在各大主流数据库都有自己实现，缺点:单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务：如果所有参与者都返回正确的 ACK 响应，则提交事务如果参与者有一个或多个参与者收到错误的 ACK 响应或者超时，则中断事务可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。Confirm：确
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。执行流程：订单服务，添加一条订单和一条消息，在一个事务里提交订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送库存服务，接收 MQ 消息，修改库存表，需要保证幂等
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 操作如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息如果修改失败，可以不做处理，等待重试MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。分布式算法 paxos 了解么 ？Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一在 Paxos 中有这么几个角色：Proposer（提议者） : 提议者提出提案，用于投票表决。Accecptor（接受者） : 对提案进行投票，并接受达成
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 共识的提案。Learner（学习者） : 被告知投票的结果，接受达成共识的提案Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受)Prepare(准备)阶段提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答：两个承诺：承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求承诺不会再接受提案号小于 Mn 的 Accept 请求一个应答：不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。Accept(接受)阶段如果提议者收到来自
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题笔试题1 为什么使用消息队列？消息队列（Message Queue，简称 MQ）是一种跨进程的通信机制，用于上下游传递消息。它在现代分布式系统中扮演着重要的角色，主要用于系统间的解耦、异步消息处理以及流量削峰。消息队列的使用场景解耦在没有消息队列的系统中，如果
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个系统需要与多个系统交互，它们之间的耦合度会非常高。例如，系统 A直接调用系统 B 和 C的接口，如果未来需要接入系统 D或取消 B 系统，系统 A 需要修改代码，这增加了系统的风险。使用消息队列后，系统 A 只需将消息推送到队列，其他系统根据需要从队列中订阅消息。这样，系统 A 不需要做任何修改，也不需要考虑下游消费失败的情况，从而实现了系统间的解耦。异步处理在同步操作中，一些非关键的业务逻辑可能会消耗大量时间，导致用户体验不佳。例如，系统 A 在处理一个请求时，需要在多个系统中进行操作，这可能导致总延迟增加。通过使用消息队列，系统 A 可以将消息写入队列，而其他业务逻辑可以异步执行，从而显著减少总耗时。流量削峰对于面临突发流量的系统，如果直接将所有请求发送到数据库，可能会导致数据库连接异常或系统崩溃。消息队列可以帮助系统按照下游系统的处理能力从队列中慢慢拉取消息，从而避免因突发流量导致的系统崩溃。消息队列的优缺点优点解耦：使得系统间的依赖关系最小化，降低系统间的耦合度。异步处理：提高系统的响应速度和吞吐量。流量削峰：使系统能够应对高流量压力，避免系统因突发流量而崩溃2.简述数据库的事务，说出事务的特点？
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 在数据库管理系统中，事务是一个非常重要的概念，它指的是一系列的数据库操作，这些操作要么全部成功，要么全部失败，确保数据的完整性和一致性。事务的四大特性通常被称为 ACID属性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。原子性（Atomicity）原子性确保事务中的所有操作要么全部成功，要么全部失败回滚，不会出现只执行了部分操作的情况。这意味着事务是一个不可分割的工作单位，例如，在银行转账的场景中，转账操作需要同时更新两个账户的余额，这两个操作必须要么都执行，要么都不执行一致性（Consistency）一致性意味着数据库在事务开始之前和结束之后，都必须保持一致状态。事务不会破坏数据的完整性和业务规则。例如，如果一个转账事务在执行过程中系统崩溃，事务没有提交，那么事务中所做的修改也不会保存到数据库中，保证了数据的一致性隔离性（Isolation）隔离性保证了当多个用户并发访问数据库时，数据库系统能够为每个用户的事务提供一个独立的运行环境，事务之间不会互相干扰。例如，当一个事务正在处理数据时，其他事务必须等待，直到该事务完成，
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 才能访问同样的数据持久性（Durability）持久性确保一旦事务提交，它对数据库的改变就是永久性的。即使发生系统故障，事务的结果也不会丢失。例如，一旦银行转账事务提交，转账的金额就会永久地反映在各个账户的余额中事务的四大特性是数据库管理系统设计的基础，它们确保了数据库操作的安全性和可靠性，使得用户可以信赖数据库处理复杂的业务逻辑。3. SOA 和微服务之间的区别？SOA（面向服务的架构）[&和微服务架构&]是两种常见的软件架构设计方法，它们在服务划分、通信方式和应用场景等方面存在显著差异。SOA 的特点 SOA 是一种高层次的架构设计理念，旨在通过服务接口实现系统间的松耦合和功能复用。服务通过企业服务总线（ESB）进行通信，ESB负责消息路由、协议转换和服务集成。SOA 的服务粒度较粗，适用于复杂的企业级系统，尤其是需要集成异构系统的场景。微服务的特点 微服务架构是对 SOA 的进一步演进，强调将单一业务系统拆分为多个独立的小型服务。每个服务独立开发、部署和运行，通常通过轻量级协议（如 HTTP/REST）进行通信。微服务更注重快速交付和自动化运维，适合快速变化的互联网系统。主要区别服务粒度：SOA 的服务
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 粒度较粗，通常是一个完整的业务模块；微服务的服务粒度较细，专注于单一功能。通信方式：SOA 依赖 ESB 进行服务间通信，支持多种协议；微服务使用轻量级协议（如 HTTP/REST），去掉了 ESB。部署方式：SOA 通常整体部署，微服务则支持独立部署，便于快速迭代。应用场景：SOA 适用于复杂的企业级系统，微服务更适合轻量级、基于 Web 的系统。总结 SOA 和微服务各有优劣，选择哪种架构取决于具体的业务需求和系统复杂性。SOA 更适合需要集成异构系统的大型企业应用，而微服务则适合快速变化的互联网应用。4.深拷贝和浅拷贝的区别？在编程中，深复制和浅复制是两种不同的对象复制方式。它们主要用于处理对象和数组等引用数据类型。浅复制浅复制只复制对象的引用，而不复制对象本身。也就是说，新旧对象共享同一块内存空间，对其中一个对象的修改会影响到另一个对象。浅复制适用于对象的属性是基本数据类型的情况，但如果属性是引用类型，则会出现共享内存的问题。深复制深复制会创建一个新的对象，并递归复制所有层级的属性和数组元素。新对象与原对象不共享内存，修改新对象不会影响到原对象。深复制适用于需要完全独立的对象副本的情况。5.有了关系型
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据库，为什么还需要 NOSQL 数据库？关系型数据库（RDBMS）在数据存储和管理方面表现出色，但在某些场景下存在局限性。随着互联网和大数据时代的到来，传统的关系型数据库在处理大量非结构化和半结构化数据时显得力不从心。为了解决这些问题，非关系型数据库（NoSQL）应运而生。关系型数据库的局限性扩展性：关系型数据库通常采用垂直扩展（Scale-Up）的方式，通过增加硬件资源来提升性能。然而，这种方式在高并发、大数据量的情况下成本高昂且效果有限。灵活性：关系型数据库要求预先定义数据模式（Schema），在需求频繁变化的应用场景中显得不够灵活。每次修改数据模式都需要停机或复杂的迁移操作。性能：在高并发读写、大规模数据处理的情况下，关系型数据库的性能可能无法满足需求，特别是在分布式环境下。成本：关系型数据库通常需要昂贵的硬件和专业的维护团队，对于中小型企业和初创公司来说，成本压力较大。非关系型数据库的优势水平扩展（Scale-Out）：NoSQL 数据库通常设计为支持水平扩展，通过增加更多的服务器节点来提升性能。这种方式在大规模数据和高并发场景下非常有效。灵活的数据模型：NoSQL数据库通常采用无模式（Schema
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -less）或弱模式（Schema-flexible）的设计，允许数据以更灵活的方式存储。高性能：NoSQL 数据库针对特定的应用场景进行了优化，通常具有更高的读写性能。低成本：NoSQL数据库通常采用开源软件，硬件要求较低，适合在云环境中部署，降低了总体拥有成本（TCO）。分布式架构：NoSQL 数据库通常采用分布式架构，能够更好地处理大规模数据和高并发请求。关系型数据库与非关系型数据库的对比数据模型：关系型数据库以表格形式存储数据，而 NoSQL数据库以键值对、文档、列族或图的形式存储数据。扩展性：关系型数据库采用垂直扩展，而 NoSQL数据库采用水平扩展。数据一致性：关系型数据库强调强一致性（ACID），而 NoSQL 数据库通常采用最终一致性（BASE）。数据模式：关系型数据库需要固定模式，而 NoSQL 数据库通常无模式或弱模式。性能：关系型数据库适合事务处理和小规模数据，而 NoSQL数据库适合大规模数据和高并发。成本：关系型数据库成本较高，而 NoSQL数据库成本较低。结论NoSQL数据库的出现并不是为了取代关系型数据库，而是为了填补关系型数据库在某些场景下的不足。NoSQL 数据库提供了更高的
2025-08-11 10:56:00.288 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 扩展性、灵活性和性能，适合处理大规模数据和高并发请求。然而，关系型数据库在事务处理和企业应用中仍然具有不可替代的优势。
2025-08-11 10:56:00.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:56:00.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:56:00.645 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.yizhaoqi.smartpai.service.VectorizationService - 开始向量化文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:56:00.655 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.client.EmbeddingClient - 开始生成向量，文本数量: 952
2025-08-11 10:56:00.656 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:00.656 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:00.883 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] [10d4b7b0-13] Response 400 BAD_REQUEST
2025-08-11 10:56:00.883 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] [10d4b7b0-13] Read 312 bytes
2025-08-11 10:56:01.700 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:56:01.705 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:56:01.705 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:56:01.705 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:56:01.707 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:01.707 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:56:01.708 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:01.884 [parallel-10] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:01.884 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:01.963 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:01.968 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:01.968 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12", parameters={masked}
2025-08-11 10:56:01.968 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.ConversationController#getConversations(String, String, String)
2025-08-11 10:56:01.971 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:01.971 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{role=user, content=解释一下社区论坛项目, timestamp=2025-08-10T10:40:56}, {role=assistant, co (truncated)...]
2025-08-11 10:56:01.972 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:02.082 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] [10d4b7b0-14] Response 400 BAD_REQUEST
2025-08-11 10:56:02.082 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] [10d4b7b0-14] Read 312 bytes
2025-08-11 10:56:02.614 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:56:02.618 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:56:02.618 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:56:02.618 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:56:02.620 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:02.620 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:56:02.622 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:03.084 [parallel-11] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:03.084 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:03.241 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] [10d4b7b0-15] Response 400 BAD_REQUEST
2025-08-11 10:56:03.242 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] [10d4b7b0-15] Read 312 bytes
2025-08-11 10:56:03.294 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/conversation?userid=1&start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:03.298 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/conversation?userid=1&start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:03.298 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/conversation?userid=1&start_date=2025-08-04&end_date=2025-08-12", parameters={masked}
2025-08-11 10:56:03.298 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getAllConversations(String, String, String, String)
2025-08-11 10:56:03.307 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:03.308 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{role=user, content=解释一下社区论坛项目, timestamp=2025-08-10T10:40:56, username=admin}, {ro (truncated)...]
2025-08-11 10:56:03.308 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:03.601 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=999&orgTag=default
2025-08-11 10:56:03.605 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=999&orgTag=default
2025-08-11 10:56:03.606 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=999&orgTag=default", parameters={masked}
2025-08-11 10:56:03.606 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 10:56:03.611 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:03.611 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=999, totalPages=1, content=[{primaryOrg=default, createdAt=2025-08-09T16:10:46 (truncated)...]
2025-08-11 10:56:03.613 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:04.248 [parallel-12] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:04.250 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:04.408 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] [10d4b7b0-16] Response 400 BAD_REQUEST
2025-08-11 10:56:04.409 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [559a96eb] [10d4b7b0-16] Read 312 bytes
2025-08-11 10:56:04.409 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [559a96eb] Cancel signal (to close connection)
2025-08-11 10:56:04.409 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR com.yizhaoqi.smartpai.client.EmbeddingClient - 调用向量化 API 失败: Retries exhausted: 3/3
reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:04.411 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.yizhaoqi.smartpai.service.VectorizationService - 向量化失败，fileMd5: c8f8cebf90c764b93d862694096a2af9
java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:04.412 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.y.smartpai.consumer.FileProcessingConsumer - Error processing task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:06.671 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 10:56:06.677 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 10:56:06.677 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 10:56:06.678 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 10:56:06.680 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:06.680 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 10:56:06.682 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:06.931 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:06.937 [http-nio-8081-exec-9] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 10:56:06.937 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12", parameters={masked}
2025-08-11 10:56:06.937 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.ConversationController#getConversations(String, String, String)
2025-08-11 10:56:06.944 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 10:56:06.944 [http-nio-8081-exec-9] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{role=user, content=解释一下社区论坛项目, timestamp=2025-08-10T10:40:56}, {role=assistant, co (truncated)...]
2025-08-11 10:56:06.946 [http-nio-8081-exec-9] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 10:56:07.437 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR o.s.kafka.listener.KafkaMessageListenerContainer - Error handler threw an exception
org.springframework.kafka.KafkaException: Seek to current after exception
	at org.springframework.kafka.listener.SeekUtils.seekOrRecover(SeekUtils.java:227)
	at org.springframework.kafka.listener.DefaultErrorHandler.handleRemaining(DefaultErrorHandler.java:168)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeErrorHandler(KafkaMessageListenerContainer.java:2836)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2713)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.springframework.kafka.listener.ListenerExecutionFailedException: Listener method 'public void com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(com.yizhaoqi.smartpai.model.FileProcessingTask)' threw exception
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.decorateException(KafkaMessageListenerContainer.java:2869)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2814)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	... 10 common frames omitted
	Suppressed: org.springframework.kafka.listener.ListenerExecutionFailedException: Restored Stack Trace
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:435)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
Caused by: java.lang.RuntimeException: Error processing task
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:67)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	... 12 common frames omitted
Caused by: java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	... 25 common frames omitted
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:07.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Received task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
2025-08-11 10:56:07.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件权限信息: userId=1, orgTag=PRIVATE_sy, isPublic=true
2025-08-11 10:56:07.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Downloading file from storage: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:56:07.441 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Detected remote URL: http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f
2025-08-11 10:56:07.447 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - Successfully connected to URL, starting download...
2025-08-11 10:56:07.448 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 开始解析文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文件元数据:
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:unmappedUnicodeCharsPerPage: 0
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:PDFVersion: 1.7
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmp:CreatorTool: WPS 文字
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXFA: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:modify_annotations: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print_degraded: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By-Full-Set: org.apache.tika.parser.DefaultParser
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:creator: SongYu
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:num3DAnnotations: 0
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:created: 2025-08-04T09:36:05Z
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dcterms:modified: 2025-08-04T09:36:05Z
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - dc:format: application/pdf; version=1.7
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator_tool: WPS 文字
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:overallPercentageUnmappedUnicodeChars: 0.0
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:fill_in_form: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:modified: 2025-08-04T09:36:05Z
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasCollection: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:encrypted: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsNonEmbeddedFont: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:custom:SourceModified: D:20250804173605+08'00'
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasMarkedContent: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - Content-Type: application/pdf
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:creator: SongYu
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:totalUnmappedUnicodeChars: 0
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_for_accessibility: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:assemble_document: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - xmpTPg:NPages: 133
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:hasXMP: false
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:charsPerPage: 1441
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:extract_content: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_print: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:trapped: False
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - SourceModified: D:20250804173605+08'00'
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - X-TIKA:Parsed-By: org.apache.tika.parser.DefaultParser
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - access_permission:can_modify: true
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:docinfo:created: 2025-08-04T09:36:05Z
2025-08-11 10:56:08.170 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - pdf:containsDamagedFont: false
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 提取的文本内容长度: 121403
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆状态的检查、为游客和已登录用户展示不同界面与功能。实现不同用户的权限控制和网站数据统计(UV、DAU)，管理员可以查看网站数据统计和网站监控信息。支持用户上传头像，实现发布帖子、评论帖子、热帖排行、发送私信与敏感词过滤等功能。实现了点赞关注与系统通知功能。支持全局搜索帖子信息的功能。核心功能具体实现1. 通过对登录用户颁发登录凭证，将登陆凭证存进 Redis 中来记录登录用户登录状态，使用拦截器进行登录状态检查，使用 Spring Security 实现权限控制，解决了 http 无状态带来的缺陷，保护需登录或权限才能使用的特定资源。（登入时将生成的 Ticket存入 Redies, 然后在登入请求成功时，将 Redies中的Ticket存入新建的 Cookie 中，然后反馈给浏览器，随后在该浏览器访问其他请求时，会先经过 LoginTicketInterceptor，判断请求中是否有 Ticket，是否和 Redies中的 T
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: icket一致，如果一致会将用户信息存入到 hostHolder（属于线程局部缓存）中，以便后续在请求处理过程中可以方便地获取到当前登录用户的信息，在请求处理完成后，清除当前线程中存储的用户信息。通过这种方式，确保每个请求都是独立处理的，不会因为线程复用而导致用户信息泄露或混淆。 注意可以将用户信息存入 Redis缓存中来减少 DB 的访问量，但是当用户数据更新时，必须即使删除 Redis中的用户数据，以保证数据的一致性和准确性。Spring Security 的用户认证是在自定义的过滤器中，也是获取请求 Cookie 中的 Ticket 和 Redis中的Ticket是否一致，然后将认证用户存到安全上下文中，在 Security 配置类中根据安全上下文获取用户信息，判断用户对各个资源的访问权限。Security 认证应当放到过滤器中而不是拦截器，因为过滤器比拦截器先执行，在拦截器中配置安全上下文会导致 Security 配置类获取不到用户信息，因为此时还没执行拦截器。）2. 使用 ThreadLocal 在当前线程中存储用户数据，代替 session 的功能便于分布式部署。在拦截器的 preHandle 中
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存储用户数据，在 postHandle 中将用户数据存入 Model，在 afterCompletion 中清理用户数据。（ThreadLocal 为每个线程提供独立的变量副本。每个线程操作自己的副本，互不干扰。在 Web 应用中，一个请求从开始到结束通常由同一个线程处理。因此，在拦截器的 preHandle 中存储的数据，可在整个请求链路（Controller、Service、Dao）中通过 ThreadLocal 获取。 在分布式部署中，由于 Session需要共享，使用 ThreadLocal存储用户数据，我们并不需要在多个服务器之间共享这些数据。因为每个请求都是独立的，处理完一个请求后，数据就被清除了。所以，在分布式环境下，我们只需要确保每个服务器能够独立处理请求即可，不需要考虑多个服务器之间的 Session同步问题。）3. 使用 Redis 的集合数据类型来解决踩赞、相互关注功能，采用事务管理，保证数据的正确，采用“先更新数据库，再删除缓存”策略保证数据库与缓存数据的一致性。采用 Redis 存储验证码，解决性能问题和分布式部署时的验证码需求。采用 Redis 的 HyperLogLog 存储每日
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  UV、Bitmap 存储 DAU，实现网站数据统计的需求。（使用 RedisTemplate执行一个 Redis 事务。SessionCallback 接口的 execute方法会在一个事务中执行所有操作，确保操作的原子性。通过调用 multi()方法，开启一个 Redis事务。在这个事务中执行的命令会被缓存，直到 exec()方法被调用时才会一次性提交。数据结构：HyperLogLog，12KB 内存可计算 2^64 个不重复元素 误差率仅 0.81% 数据结构：Bitmap 一连串二进制数组，可以进行二值状态统计）4. 使用 Kafka 作为消息队列，在用户被点赞、评论、关注后以系统通知的方式推送给用户，用户发布或删除帖子后向 elasticsearch 同步，对系统进行解耦、削峰。（在这个系统中 kafka 就办了三件事，一是用户在被点赞、评论、关注后会借助kafka 消费者来异步的生成系统通知，二三是在用户发布帖子和删除帖子时，将内容添加到 elasticsearch或者从 elasticsearch 中删除）5. 使用 elasticsearch + ik 分词插件实现全局搜索功能，当用户发布、修
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 改或删除帖子时，使用 Kafka 消息队列去异步将帖子信息给 elasticsearch 同步。（Elasticsearch（简称 ES）是一个开源的分布式搜索和分析引擎，它专为处理海量数据设计，提供近实时的全文搜索能力。IK Analyzer 是专为中文设计的开源分词插件，解决中文文本分析的核心难题。ik_smart：粗粒度切分 ik_max_word 细粒度切分）6. 使用分布式定时任务 Quartz 定时计算帖子分数，来实现热帖排行的业务功能。对频繁需要访问的数据，如用户信息、帖子总数、热帖的单页帖子列表，使用Caffeine 本地缓存 + Redis 分布式缓存的多级缓存，提高服务器性能，实现系统的高可用。（上面三个部分就是 Quartz的基本组成部分：调度器：Scheduler任务：JobDetail触发器：Trigger，包括 SimpleTrigger 和 CronTrigger定义一个 Quartz定时任务及其触发器。具体来说，它配置了一个名为postScoreRefreshJob的任务，该任务属于 communityJobGroup 组，并且被设置为持久化和请求恢复。同时，它还配置了一个名为
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  postScoreRefreshTrigger 的触发器，该触发器也属于 communityTriggerGroup组，并且每 5 分钟触发一次postScoreRefreshJob任务。因此，这段代码的主要目的是确保 PostScoreRefreshJob类中定义的任务每 5分钟执行一次）核心技术Spring Boot、SSMRedis、Kafka、ElasticsearchSpring Security、Quartz、Caffeine项目亮点项⽬构建在 Spring Boot+SSM 框架之上，并统⼀的进⾏了状态管理、事务管理、异常处理；利⽤ Redis 实现了点赞和关注功能，单机可达 5000TPS；利⽤ Kafka 实现了异步的站内通知，单机可达 7000TPS；利⽤ Elasticsearch 实现了全⽂搜索功能，可准确匹配搜索结果，并⾼亮显示关键词；利⽤ Caffeine+Redis 实现了两级缓存，并优化了热⻔帖⼦的访问，单机可达8000QPS。利⽤ Spring Security 实现了权限控制，实现了多重⻆⾊、URL 级别的权限管理；利⽤ HyperLogLog、Bitmap 分别实现了 
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: UV、DAU 的统计功能，100 万⽤户数据只需*M 内存空间；利⽤ Quartz 实现了任务调度功能，并实现了定时计算帖⼦分数、定时清理垃圾⽂件等功能；利⽤ Actuator 对应⽤的 Bean、缓存、⽇志、路径等多个维度进⾏了监控，并通过⾃定义的端点对数据库连接进⾏了监控。面试题：1.你提到使用 Spring Security 实现权限控制。能具体说明如何整合登录凭证（Redis存储）与 Spring Security？如何实现 URL 级别的动态权限管理？答：用户登入成功时系统会生成一个 Ticket并存入 Redis，新建一个 cookie 存入Ticket；在自定义的过滤器中验证请求携带的 Ticket与 Redis内的 Ticket是否一致，构建 Authentication对象存入 SecurityContextHolder；在 Security 的配置类中对固定 URL 如/admin/**）使用 antMatchers().hasRole("ADMIN")，即根据用户权限赋予访问资源的能力。2. 你提到点赞功能采用‘先更新 DB 再删缓存’策略。如果删除缓存失败导致不一致，如何解决？为何不用
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ‘更新缓存’方案？答：“删除失败兜底方案：设置缓存短过期时间（如 30s），容忍短期不一致异步重试：将失败操作推入 Kafka，消费者重试删除监听 MySQL Binlog（如 Canal）触发缓存删除不更新缓存的原因：写冲突： 并发更新可能导致缓存脏数据（如线程 A更新 DB 后未更新缓存时，线程 B又更新）浪费资源： 频繁更新但低读取的数据会占用带宽复杂度： 需维护缓存与 DB的强一致性逻辑（如分布式锁），而删除策略更简单可靠。”3. 系统通知使用 Kafka异步推送。如果通知发送失败（如网络抖动），如何保证用户最终能收到通知？答：“我们通过三级保障实现可靠性：生产者确认： 设置 Kafka acks=all，确保消息写入所有副本；消费者容错：开启手动提交 Offset，业务处理成功后才提交捕获异常后重试（如 3次），仍失败则存入死信队列补偿机制：定时任务扫描未通知记录（DB状态标记）重新投递死信队列消息人工介入处理此外，消息体包含唯一 ID 防重复消费。”4. 热帖列表用了 Caffeine+Redis两级缓存。如何解决缓存穿透？如何同步本地缓存（Caffeine）的数据？答：“缓存穿透防护：布隆过滤器
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ：将所有的 key 提前存入布隆过滤器，在访问缓存层之前，查询前校验 Key 是否存在，不存在返回空值。缓存空值：对不存在的帖子 ID 缓存 NULL（短过期时间）本地缓存同步过期同步： Caffeine设置 refreshAfterWrite=30s自动刷新主动推送： 当帖子更新时，通过 Redis Pub/Sub 广播失效事件，节点监听后删除本地缓存兜底策略： 本地缓存过期时间短于 Redis（如本地 60s vs Redis 300s），确保最终一致。”5. 定时计算帖子分数时，如何避免分布式环境下的重复执行？如果计算耗时过长导致阻塞，如何优化？答：“防重复执行：使用 Quartz集群模式：数据库锁（QRTZ_LOCKS 表）保证同一任务仅一个节点执行性能优化：分片处理： 按帖子 ID 范围分片（如 0-10000, 10001-20000），多线程并行计算增量计算： 仅扫描最近 X 小时变化的帖子（如 last_modified_time > now()-6h）异步化： 将计算任务拆解为多个子任务投递到 Kafka，消费者并发处理降级策略： 超时后记录断点，下次任务从断点继续。”6. 你使用 Elas
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ticsearch 实现全文搜索并高亮关键词。请说明：如何设计索引映射（Mapping）以优化搜索效率和准确性？如何实现搜索结果的高亮显示？遇到 HTML 标签转义问题如何处理？搜索性能瓶颈可能在哪里？如何优化？答: 1. 索引映射设计：分词策略： 对帖子标题和内容字段使用 ik_max_word 分词器进行细粒度分词（索引时），搜索时结合 ik_smart 提高相关性。字段类型： 标题用 text（分词） + keyword（不分词，用于精确匹配/聚合），ID 用 keyword，发布时间用 date。副本分片： 设置合理副本数（如 1-2）提高查询吞吐量和容错性。关闭不必要特性： 对不需聚合/排序的字段关闭 doc_values 节省存储。2. 高亮实现与转义：高亮请求： 在搜索请求中添加 highlight 部分，指定字段、pre_tags（如 <em>）、post_tags（如 </em>）。HTML 转义： ES 默认会转义高亮片段中的 HTML。我们确保存入 ES 的内容是纯文本（不含用户输入的原始 HTML），避免 XSS 同时解决转义混乱。前端渲染高亮片段时使用 textContent 而非 
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: innerHTML。3. 性能优化点：瓶颈： 复杂查询（多条件+聚合）、深度分页（from + size 过大）、索引设计不佳。优化措施：避免深度分页：使用 search_after + 唯一排序值（如 ID+时间戳）替代 from/size。限制查询范围： 使用 filter 缓存（如时间范围、状态）减少 query 计算量。冷热数据分离： 历史数据迁移到低性能节点或归档索引。合理硬件： SSD、充足内存（ES 堆内存 ≈ 50% 物理内存，不超过 31GB）。7. 项目用 HyperLogLog (HLL) 统计 UV，Bitmap 统计 DAU。请解释：HLL 如何用极小空间估算大基数？它的误差范围是多少？Bitmap 如何统计 DAU？如何解决用户 ID 非连续导致的空间浪费？如果某天 UV 突增，HLL 合并结果会怎样？如何验证其准确性？答：“1. HLL 原理与误差：原理： 对每个用户 ID 做哈希，计算哈希值二进制表示中 ‘1’ 的最高位位置（如 0001... 最高位=4），维护一个 ‘寄存器数组’ 记录每个桶的最大位置。最终通过调和平均数估算基数。核心是利用概率分布。误差： Redis 的 
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: PF 实现标准误差约 0.81%（使用 16384 个寄存器时）。空间仅需 12KB（固定大小）。2. Bitmap 与 DAU：实现： 每天一个 Bitmap Key（如 dau:20230702），用户 ID 作为 Offset，访问则设位为 1。BITCOUNT 获取当日活跃用户数。稀疏优化： 使用 RLE (Run-Length Encoding) 压缩的 Bitmap 库（如 RoaringBitmap）或 Redis 的 BITFIELD 命令动态管理非连续 ID，避免传统 Bitmap 的空间浪费。3. HLL 突增与验证：突增影响：HLL 是基数估计，突增时估算值会上升，误差仍在理论范围内（0.81%）。合并多个 HLL（如按小时合并成天）误差会累积但可控。验证： 定期抽样对比：对某小段时间用 SET 精确计算 UV，与 HLL 结果对比，监控误差是否符合预期。业务上接受近似值是其使用前提。8. 敏感词过滤是社区必备功能。你如何实现它？如何平衡过滤效率和敏感词库的更新？“技术选型：Trie 树 (前缀树)实现：初始化： 服务启动时将敏感词库（DB 或文件）加载到内存中的 Trie 树。节点标记
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是否为词尾。过滤过程： 对用户输入（帖子/评论）进行滑动窗口扫描。匹配到 Trie 树路径且到达词尾时，替换为 *** 或阻断提交。优化： 结合 DFA (确定有限状态自动机) 减少回溯，支持跳过无关字符（如 敏*感*词）。词库更新：热更新： 后台管理添加敏感词后，通过 ZooKeeper 配置中心 或 Redis Pub/Sub广播到所有服务节点，节点异步重建 Trie 树。降级： 更新期间短暂使用旧词库，避免服务中断。词库版本号控制。效率：Trie 树查询时间复杂度 O(n) (n=文本长度)，内存占用可控（可压缩节点）。避免正则表达式（性能差）。”9. 你提到用 Spring Boot Actuator 进行监控并自定义了数据库监控端点。请说明：暴露了哪些关键内置端点？（至少 3个）如何自定义一个端点监控数据库连接池状态（如活跃连接数、等待连接数）？如何保证这些监控接口的安全？答：“1. 关键内置端点：/health：应用健康状态（DB, Redis, Disk 等）/metrics：JVM 内存、线程、HTTP 请求指标等/loggers：动态查看/调整日志级别/threaddump：获取线程快照（排
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 查死锁）创建一个类实现 Endpoint 接口或使用 @Endpoint(id = "dbpool") 注解。注入连接池对象（如 HikariDataSource）。在 @ReadOperation 方法中返回关键指标：安全保障：访问控制：通过 management.endpoints.web.exposure.include/exclude 精确控制暴露的端点。安全加固：集成 Spring Security：只允许管理员角色访问 /actuator/** 路径。修改默认端口：management.server.port 使用与管理网络隔离的端口。HTTPS： 强制要求监控端点使用 HTTPS。10. 在“点赞后发通知”这个场景，涉及更新数据库点赞数 (DB) 和发送 Kafka 消息 (通知) 。如何保证这两个操作的原子性？如果 Kafka 发送失败，如何处理？答：“核心思路：最终一致性 + 本地事务 + 可靠消息原子性保障： 将 ‘更新点赞状态/计数’ 和 ‘写入待通知消息’ 放在同一个数据库事务中。使用 ‘本地消息表’ 方案：在业务数据库创建 message_event 表 (含业务 ID、消息体、状态
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: [PENDING, SENT])。事务内操作：更新点赞相关 DB 数据。向 message_event 插入一条 PENDING 状态的通知记录。事务提交。Kafka 发送与补偿：后台定时任务扫描 PENDING 的消息。发送消息到 Kafka，成功后将状态改为 SENT。发送失败处理：记录重试次数和错误信息，下次任务重试（指数退避）。超过最大重试则标记为失败，告警人工介入。消费者幂等： 通知消费者根据业务 ID 去重，避免重复处理。为什么不强一致？ 跨系统（DB 与 MQ）的强一致（如 2PC）成本高且降低可用性。本方案在 CAP 中优先保证 AP，通过可靠消息实现最终一致，满足业务需求。”11.你提到使用 ThreadLocal 存储用户数据以替代 Session。这在单机中可行，但分布式部署时（如多台 Tomcat 节点）会失效。如何解决分布式场景下的用户状态共享问题？业界主流方案是什么？答：“ThreadLocal 的局限： 它绑定于单个 JVM 线程，无法跨节点共享。在负载均衡（如 Nginx 轮询）下，用户请求落到不同节点会导致状态丢失。方案演进：Session 复制： 利用 Tomcat Red
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: is Session Manager 等工具将 Session 存入Redis。所有节点从 Redis 读写 Session，实现共享。无状态 Token (JWT)： 当前项目采用的核心方案。用户登录后生成包含用户 ID和权限的 JWT Token 返回客户端（通常存于 Cookie 或 Header）。后续请求携带 Token，服务端无需存储 Session，仅需验证 Token 签名和有效期并从 Token中解析用户信息（如注入到 SecurityContext）。这天然支持分布式。项目整合： 我们实际采用了 JWT + Redis 黑名单 的增强方案：JWT 本身无状态，解析快速。主动登出/失效： 将需提前失效的 Token ID 存入 Redis 并设置 TTL（作为黑名单）。校验 Token 时额外检查黑名单。安全性： Token 使用强密钥签名（如 HMAC-SHA256），防止篡改。优点： 彻底解决分布式状态问题，减轻服务端存储压力，更适合 RESTful API。12. 热帖列表使用了 Caffeine 本地缓存。请说明：你选择了哪种缓存淘汰策略（如 LRU、LFU）？依据是什么？如何配置缓
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存大小和过期时间？如何监控缓存命中率？本地缓存导致不同节点数据不一致的风险如何缓解？（例如帖子被删除）答：“1. 淘汰策略与依据：策略： 使用 Window TinyLFU (W-TinyLFU)，Caffeine 的默认算法。它结合了 LRU（近期使用）和 LFU（频率统计）的优点，对突发流量和长期热点都有良好表现。依据：论坛热帖访问模式既有突发（新热帖），也有长尾（持续热帖）。W-TinyLFU在有限空间内能最大化命中率，优于纯 LRU/LFU。2. 配置与监控：配置：javaCaffeine.newBuilder().maximumSize(10_000) // 最大条目数.expireAfterWrite(5, TimeUnit.MINUTES) // 写入后 5 分钟过期.recordStats() // 开启统计.build();监控： 通过 Cache.stats() 获取 CacheStats 对象，关键指标：hitRate()：命中率evictionCount()：淘汰数量averageLoadPenalty()：平均加载耗时可定期输出到日志或监控系统（如 Prometheus）。3. 数据
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不一致风险缓解：主动失效： 核心策略！当帖子被删除或更新时：更新数据库。删除 Redis 中的缓存 Key。通过 Redis Pub/Sub 或 专门的广播方案（如 RabbitMQ Fanout Exchange） 发布“帖子失效”事件。所有服务节点监听到事件后，删除本地 Caffeine 缓存中对应的条目。兜底： 设置较短的本地缓存过期时间（如 5分钟），确保最终一致。”13.你提到点赞功能单机 TPS 达 5000，通知单机 TPS 7000，热帖访问 QPS 8000。请说明：这些数据是如何测试得到的？（工具、场景、环境）TPS 和 QPS 的区别是什么？测试中发现了哪些性能瓶颈？如何定位和优化的？（如 GC、慢 SQL）1. 测试方法：工具： JMeter（模拟并发用户）。场景：点赞 TPS： 持续模拟用户对随机帖子点赞（高并发写）。通知 TPS： 模拟触发通知事件（评论/关注），测量 Kafka 生产者吞吐量。热帖 QPS： 持续请求热帖列表接口（高并发读）。环境： 明确标注是 单机测试（如 4C8G Linux, JDK 17, Tomcat, Redis/Kafka 同机或独立）。2. TPS
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  vs QPS：QPS (Queries Per Second)： 服务器每秒处理的查询请求数（如 HTTP 请求）。适用于读场景。TPS (Transactions Per Second)： 服务器每秒处理的事务数（一个事务可能包含多个操作/请求）。点赞（写 DB + Redis + 发 Kafka）是一个事务。此处 5000 TPS 指每秒完成 5000 次点赞事务。3. 瓶颈发现与优化：发现工具： Arthas (监控方法耗时)、JVisualVM/PerfMa (GC 分析)、Redis Slowlog、MySQL Slow Query Log。典型瓶颈 & 优化：GC 频繁 (Young GC >1s)： 优化 JVM 参数（如 -XX:+UseG1GC, 调整MaxGCPauseMillis），减少大对象分配（如缓存 DTO 复用）。慢 SQL (全表扫描)： 添加索引（如 post_id 在点赞表），优化查询（避免 SELECT*）。Redis 单线程阻塞： 避免长命令（如 KEYS *），分片（Cluster），热点 Key 本地缓存（Caffeine）。Kafka 生产瓶颈： 调优 batc
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: h.size 和 linger.ms，增加分区数，提高 acks 级别换取可靠性（需权衡）。”14.如果这个论坛用户量增长 100 倍（如日活千万级），当前架构在哪些地方可能最先遇到瓶颈？你会如何改造？（请结合你已用技术栈思考）“潜在瓶颈与改造方向：数据库 (MySQL)：瓶颈： 写压力（点赞、发帖）、复杂查询（搜索、统计）、单表数据量过大。改造：读写分离： 主库写，多个从库读（评论列表、用户信息查询）。分库分表： 按 user_id 或 post_id 分片（如 ShardingSphere）。将点赞/关注等高频写操作分离到独立库。冷热数据分离： 归档旧帖到分析型数据库（如 HBase）。Redis：瓶颈： 单机内存容量、带宽、单线程处理。改造：集群化： Redis Cluster 自动分片。区分数据类型： 热点数据（用户信息）用集群；超大 Value（如长帖缓存）考虑其他存储或压缩；统计类（UV/DAU）可保留。Elasticsearch：瓶颈： 索引过大导致查询慢、写入堆积。改造：分片策略优化： 增加主分片数（提前规划）。按时间分索引： 如 posts-202307，便于管理/查询/删除旧数据。Kafk
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: a：瓶颈： 单个 Topic 分区数限制吞吐量。改造： 增加分区数，生产者根据 Key（如 user_id）分区保证顺序性。应用层：瓶颈： 单节点处理能力。改造： 水平扩展无状态节点（更多 Tomcat 实例），通过 Nginx 负载均衡。微服务化拆分（如独立用户服务、帖子服务、消息服务），便于独立伸缩。监控与治理：加强：引入 APM（如 SkyWalking）、集中日志（ELK）、更强健的配置中心（Nacos）和熔断限流（Sentinel）。RedisIO 多路复用是一种允许单个进程同时监视多个文件描述符的技术，使得程序能够高效处理多个并发连接而无需创建大量线程。IO 多路复用的核心思想是：让单个线程可以等待多个文件描述符就绪，然后对就绪的描述符进行操作。这样可以在不使用多线程或多进程的情况下处理并发连接。举个例子说一下 IO 多路复用？比如说我是一名数学老师，上课时提出了一个问题：“今天谁来证明一下勾股定律？”同学小王举手，我就让小王回答；小李举手，我就让小李回答；小张举手，我就让小张回答。这种模式就是 IO 多路复用，我只需要在讲台上等，谁举手谁回答，不需要一个一个去问。举例子说一下阻塞 IO 和 IO
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  多路复用的差别？假设我是一名老师，让学生解答一道题目。我的第一种选择：按顺序逐个检查，先检查 A 同学，然后是 B，之后是 C、D。。。这中间如果有一个学生卡住，全班都会被耽误。这种就是阻塞 IO，不具有并发能力我的第二种选择，我站在讲台上等，谁举手我去检查谁。C、D 举手，我去检查C、D 的答案，然后继续回到讲台上等。此时 E、A 又举手，然后去处理 E 和 ARedis的持久化方式有哪些？主要有两种，RDB 和 AOF。RDB 通过创建时间点快照来实现持久化，AOF 通过记录每个写操作命令来实现持久化。RDB 持久化机制可以在指定的时间间隔内将 Redis 某一时刻的数据保存到磁盘上的 RDB 文件中，当 Redis 重启时，可以通过加载这个 RDB 文件来恢复数据。AOF 通过记录每个写操作命令，并将其追加到 AOF 文件来实现持久化，Redis 服务器宕机后可以通过重新执行这些命令来恢复数据。子进程在执行 AOF 重写的同时，主进程可以继续处理来自客户端的命令。为了保证数据一致性，Redis 使用了 AOF 重写缓冲区机制，主进程在执行写操作时，会将命令同时写入旧的 AOF 文件和重写缓冲区。等子进
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程完成重写后，会向主进程发送一个信号，主进程收到后将重写缓冲区中的命令追加到新的 AOF 文件中，然后调用操作系统的 rename，将旧的 AOF 文件替换为新的 AOF 文件。AOF 重写期间命令会同时写入现有 AOF 文件和重写缓冲区，这种机制是有意设计的，并不会导致数据重复或不一致问题。因为新旧文件是分离的，现有命令写入当前 AOF 文件，重写缓冲区的命令最终写入新的 AOF 文件，完成后，新文件通过原子性的 rename 操作替换旧文件。两个文件是完全分离的，不会导致同一个 AOF 文件中出现重复命令。RDB 通过 fork 子进程在特定时间点对内存数据进行全量备份，生成二进制格式的快照文件。其最大优势在于备份恢复效率高，文件紧凑，恢复速度快，适合大规模数据的备份和迁移场景。缺点是可能丢失两次快照期间的所有数据变更AOF 会记录每一条修改数据的写命令。这种日志追加的方式让 AOF 能够提供接近实时的数据备份，数据丢失风险可以控制在 1 秒内甚至完全避免。缺点是文件体积较大，恢复速度慢。在选择 Redis 持久化方案时，我会从业务需求和技术特性两个维度来考虑。如果是缓存场景，可以接受一定程度的数据丢失，
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 我会倾向于选择 RDB 或者完全不使用持久化。RDB 的快照方式对性能影响小，而且恢复速度快，非常适合这类场景但如果是处理订单或者支付这样的核心业务，数据丢失将造成严重后果，那么AOF 就成为必然选择。通过配置每秒同步一次，可以将潜在的数据丢失风险限制在可接受范围内。当 Redis 服务重启时，它会优先查找 AOF 文件，如果存在就通过重放其中的命令来恢复数据；如果不存在或未启用 AOF，则会尝试加载 RDB 文件，直接将二进制数据载入内存来恢复。混合持久化的工作原理非常巧妙：在 AOF 重写期间，先以 RDB 格式将内存中的数据快照保存到 AOF 文件的开头，再将重写期间的命令以 AOF 格式追加到文件末尾。这样，当需要恢复数据时，Redis 先加载 RDB 格式的数据来快速恢复大部分的数据，然后通过重放命令恢复最近的数据，这样就能在保证数据完整性的同时，提升恢复速度Redis 的主从复制是指通过异步复制将主节点的数据变更同步到从节点，从而实现数据备份和读写分离。这个过程大致可以分为三个阶段：建立连接、同步数据和传播命令。Redis 主从复制的最大挑战来自于它的异步特性，主节点处理完写命令后会立即响应客户端
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，而不会等待从节点确认，这就导致在某些情况下可能出现数据不一致脑裂问题了解吗？在 Redis 的哨兵架构中，脑裂的典型表现为：主节点与哨兵、从节点之间的网络发生故障了，但与客户端的连接是正常的，就会出现两个“主节点”同时对外提供服务。哨兵认为主节点已经下线了，于是会将一个从节点选举为新的主节点。但原主节点并不知情，仍然在继续处理客户端的请求等主节点网络恢复正常了，发现已经有新的主节点了，于是原主节点会自动降级为从节点。在降级过程中，它需要与新主节点进行全量同步，此时原主节点的数据会被清空。导致客户端在原主节点故障期间写入的数据全部丢失Redis 中的哨兵用于监控主从集群的运行状态，并在主节点故障时自动进行故障转移。哨兵的工作原理可以概括为 4 个关键步骤：定时监控、主观下线、领导者选举和故障转移。首先，哨兵会定期向所有 Redis 节点发送 PING 命令来检测它们是否可达。如果在指定时间内没有收到回复，哨兵会将该节点标记为“主观下线”当一个哨兵判断主节点主观下线后，会询问其他哨兵的意见，如果达到配置的法定人数，主节点会被标记为“客观下线”然后开始故障转移，这个过程中，哨兵会先选举出一个领导者，领导者再从从节
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 点中选择一个最适合的节点作为新的主节点，选择标准包括复制偏移量、优先级等因素确定新主节点后，哨兵会向其发送 SLAVEOF NO ONE 命令使其升级为主节点，然后向其他从节点发送 SLAVEOF 命令指向新主节点，最后通过发布/订阅机制通知客户端主节点已经发生变化。主从复制实现了读写分离和数据备份，哨兵机制实现了主节点故障时自动进行故障转移。集群架构是对前两种方案的进一步扩展和完善，通过数据分片解决 Redis 单机内存大小的限制，当用户基数从百万增长到千万级别时，我们只需简单地向集群中添加节点，就能轻松应对不断增长的数据量和访问压力。Redis Cluster 是 Redis 官方提供的一种分布式集群解决方案。其核心理念是去中心化，采用 P2P 模式，没有中心节点的概念。每个节点都保存着数据和整个集群的状态，节点之间通过 gossip 协议交换信息。在数据分片方面，Redis Cluster 使用哈希槽机制将整个集群划分为 16384 个单元。在计算哈希槽编号时，Redis Cluster 会通过 CRC16 算法先计算出键的哈希值，再对这个哈希值进行取模运算，得到一个 0 到 16383 之间的整数。当
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 需要存储或查询一个键值对时，Redis Cluster 会先计算这个键的哈希槽编号，然后根据哈希槽编号找到对应的节点进行操作。常见的数据分区有三种：节点取余、一致性哈希和哈希槽。节点取余分区简单明了，通过计算键的哈希值，然后对节点数量取余，结果就是目标节点的索引。缺点是增加一个新节点后，节点数量从 N 变为 N+1，几乎所有的取余结果都会改变，导致大部分缓存失效。一致性哈希分区出现了：它将整个哈希值空间想象成一个环，节点和数据都映射到这个环上。数据被分配到顺时针方向上遇到的第一个节点。但一致性哈希仍然有一个问题：数据分布不均匀。比如说在上面的例子中，节点 1 和节点 2 的数据量差不多，但节点 3 的数据量却远远小于它们。Redis Cluster 的哈希槽分区在一致性哈希和节点取余的基础上，做了一些改进。它将整个哈希值空间划分为 16384 个槽位，每个节点负责一部分槽，数据通过CRC16 算法计算后对 16384 取模，确定它属于哪个槽。布隆过滤器是一种空间效率极高的概率性数据结构，用于快速判断一个元素是否在一个集合中。它的特点是能够以极小的内存消耗，判断一个元素“一定不在集合中”或“可能在集合中”，常用
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来解决 Redis 缓存穿透的问题。布隆过滤器并不支持删除操作，这是它的一个重要限制。如何保证缓存和数据库数据的一致性？具体做法是读取时先查 Redis，未命中再查 MySQL，同时为缓存设置一个合理的过期时间；更新时先更新 MySQL，再删除 Redis。最初设计缓存策略时，我也考虑过直接更新缓存，但通过实践发现，删除缓存是更优的选择。那再说说为什么要先更新数据库，再删除缓存？这个操作顺序的选择也是我在实际项目中踩过坑才深刻理解的。假设我们采用先删缓存再更新数据库的策略，在高并发场景下就可能出现这样的问题：线程 A 要更新用户信息，先删除了缓存线程 B 恰好此时要读取该用户信息，发现缓存为空，于是查询数据库，此时还是旧值线程 B 将查到的旧值重新放入缓存线程 A 完成数据库更新结果就是数据库是新的值，但缓存中还是旧值当业务对缓存与数据库的一致性要求很高时，比如支付系统、库存管理等场景，我会采用多种策略来保证强一致性。第一种，引入消息队列来保证缓存最终被删除，比如说在数据库更新的事务中插入一条本地消息记录，事务提交后异步发送给 MQ 进行缓存删除。即使缓存删除失败，消息队列的重试机制也能保证最终一致性。第二种
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，使用 Canal 监听 MySQL 的 binlog，在数据更新时，将数据变更记录到消息队列中，消费者消息监听到变更后去删除缓存。如何保证本地缓存和分布式缓存的一致性？为了保证 Caffeine 和 Redis 缓存的一致性，我采用的策略是当数据更新时，通过 Redis 的 pub/sub 机制向所有应用实例发送缓存更新通知，收到通知后的实例立即更新或者删除本地缓存。Redis 可以部署在多个节点上，支持数据分片、主从复制和集群。而本地缓存只能在单个服务器上使用。对于读取频率极高、数据相对稳定、允许短暂不一致的数据，我优先选择本地缓存。比如系统配置信息、用户权限数据、商品分类信息等。而对于需要实时同步、数据变化频繁、多个服务需要共享的数据，我会选择 Redis。比如用户会话信息、购物车数据、实时统计信息等。缓存预热是指在系统启动或者特定时间点，提前将热点数据加载到缓存中，避免冷启动时大量请求直接打到数据库。Redis 主要采用了两种过期删除策略来保证过期的 key 能够被及时删除，包括惰性删除和定期删除。当内存使用接近 maxmemory 限制时，Redis 会依据内存淘汰策略来决定删除哪些 key 以缓解
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内存压力。lru 会删除最近最少使用的 key，在纯缓存场景中最常用，能自动保留热点数据；lfu 会删除访问频率最低的 key，更适合长期运行的系统；LRU 是 Least Recently Used 的缩写，基于时间维度，淘汰最近最少访问的键。LFU 是 Least Frequently Used 的缩写，基于次数维度，淘汰访问频率最低的键。延时消息队列在实际业务中很常见，比如订单超时取消、定时提醒等场景。Redis虽然不是专业的消息队列，但可以很好地实现延时队列功能。核心思路是利用 ZSet 的有序特性，将消息作为 member，把消息的执行时间作为 score。这样消息就会按照执行时间自动排序，我们只需要定期扫描当前时间之前的消息进行处理就可以了。分布式锁是一种用于控制多个不同进程在分布式系统中访问共享资源的锁机制。它能确保在同一时刻，只有一个节点可以对资源进行访问，从而避免分布式场景下的并发问题。可以使用 Redis 的 SETNX 命令实现简单的分布式锁。比如 SET key value NX PX3000 就创建了一个锁名为 key 的分布式锁，锁的持有者为 value。NX 保证只有在 key 
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不存在时才能创建成功，EX 设置过期时间用以防止死锁。Kafka，是一个分布式、支持分区的（partition）、多副本的（replica），基于 zookeeper协调的分布式消息系统，它的最大的特性就是可以实时的处理大量数据以满足各种需求场景Kafka 的设计Kafka 将消息以 topic 为单位进行归纳，发布消息的程序称为 Producer，消费消息的程序称为 Consumer。它是以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 Broker，Producer 通过网络将消息发送到 kafka 集群，集群向消费者提供消息，broker 在中间起到一个代理保存消息的中转站。Kafka 性能高原因利用了 PageCache 缓存磁盘顺序写零拷贝技术pull 拉模式优点高性能、高吞吐量、低延迟：Kafka 生产和消费消息的速度都达到每秒 10 万级高可用：所有消息持久化存储到磁盘，并支持数据备份防止数据丢失高并发：支持数千个客户端同时读写容错性：允许集群中节点失败（若副本数量为 n，则允许 n-1 个节点失败）高扩展性：Kafka 集群支持热伸缩，无须停机缺点没有完整的监控工具集不支持通配符主
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 题选择Kafka 的应用场景日志聚合：可收集各种服务的日志写入 kafka 的消息队列进行存储消息系统：广泛用于消息中间件系统解耦：在重要操作完成后，发送消息，由别的服务系统来完成其他操作流量削峰：一般用于秒杀或抢购活动中，来缓冲网站短时间内高流量带来的压力异步处理：通过异步处理机制，可以把一个消息放入队列中，但不立即处理它，在需要的时候再进行处理Kafka 为什么要把消息分区方便扩展：因为一个 topic 可以有多个 partition，每个 Partition 可用通过调整以适应它所在的机器，而一个 Topic 又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了提高并发：以 partition 为读写单位，可以多个消费者同时消费数据，提高了消息的处理效率Kafka 中生产者运行流程一条消息发过来首先会被封装成一个 ProducerRecord 对象对该对象进行序列化处理（可以使用默认，也可以自定义序列化）对消息进行分区处理，分区的时候需要获取集群的元数据，决定这个消息会被发送到哪个主题的哪个分区分好区的消息不会直接发送到服务端，而是放入生产者的缓存区，多条消息会被封装成一个批次（
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Batch），默认一个批次的大小是 16KBSender 线程启动以后会从缓存里面去获取可以发送的批次Sender 线程把一个一个批次发送到服务端Kafka采用大部分消息系统遵循的传统模式：Producer 将消息推送到 Broker，Consumer 从 Broker 获取消息。负载均衡是指让系统的负载根据一定的规则均衡地分配在所有参与工作的服务器上，从而最大限度保证系统整体运行效率与稳定性负载均衡Kakfa 的负载均衡就是每个 Broker 都有均等的机会为 Kafka 的客户端（生产者与消费者）提供服务，可以负载分散到所有集群中的机器上。Kafka 通过智能化的分区领导者选举来实现负载均衡，提供智能化的 Leader 选举算法，可在集群的所有机器上均匀分散各个 Partition的 Leader，从而整体上实现负载均衡。故障转移Kafka 的故障转移是通过使用会话机制实现的，每台 Kafka 服务器启动后会以会话的形式把自己注册到 Zookeeper 服务器上。一旦服务器运转出现问题，就会导致与 Zookeeper 的会话不能维持从而超时断连，此时 Kafka 集群会选举出另一台服务器来完全替代这台服务
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 器继续提供服务。Kafka 中 Zookeeper 的作用Kafka 是一个使用 Zookeeper 构建的分布式系统。Kafka 的各 Broker 在启动时都要在 Zookeeper上注册，由 Zookeeper统一协调管理。如果任何节点失败，可通过Zookeeper从先前提交的偏移量中恢复，因为它会做周期性提交偏移量工作。同一个 Topic 的消息会被分成多个分区并将其分布在多个 Broker 上，这些分区信息及与 Broker 的对应关系也是 Zookeeper在维护Kafka 中消费者与消费者组的关系与负载均衡实现Consumer Group 是 Kafka 独有的可扩展且具有容错性的消费者机制。一个组内可以有多个 Consumer，它们共享一个全局唯一的 Group ID。组内的所有 Consumer协调在一起来消费订阅主题（Topic）内的所有分区（Partition）。当然，每个 Partition只能由同一个 Consumer Group内的一个 Consumer 来消费。消费组内的消费者可以使用多线程的方式实现，消费者的数量通常不超过分区的数量，且二者最好保持整数倍的关系，这样不会造成有空
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲的消费者。Consumer 订阅的是 Topic 的 Partition，而不是 Message。所以在同一时间点上，订阅到同一个分区的 Consumer 必然属于不同的 Consumer GroupConsumer Group与 Consumer的关系是动态维护的，当一个 Consumer 进程挂掉或者是卡住时，该 Consumer 所订阅的 Partition会被重新分配到改组内的其他Consumer 上，当一个 Consumer加入到一个 Consumer Group中时，同样会从其他的 Consumer 中分配出一个或者多个 Partition到这个新加入的 Consumer。当生产者试图发送消息的速度快于 Broker 可以处理的速度时，通常会发生QueueFullException首先先进行判断生产者是否能够降低生产速率，如果生产者不能阻止这种情况，为了处理增加的负载，用户需要添加足够的 Broker。或者选择生产阻塞，设置Queue.enQueueTimeout.ms 为 -1，通过这样处理，如果队列已满的情况，生产者将组织而不是删除消息。或者容忍这种异常，进行消息丢弃。Consumer 如何
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 消费指定分区消息Cosumer 消费消息时，想 Broker 发出 fetch 请求去消费特定分区的消息，Consumer 可以通过指定消息在日志中的偏移量 offset，就可以从这个位置开始消息消息，Consumer 拥有了 offset 的控制权，也可以向后回滚去重新消费之前的消息。也可以使用 seek(Long topicPartition) 来指定消费的位置。Replica、Leader 和 Follower 三者的概念:Kafka 中的 Partition 是有序消息日志，为了实现高可用性，需要采用备份机制，将相同的数据复制到多个 Broker 上，而这些备份日志就是 Replica，目的是为了防止数据丢失。所有 Partition 的副本默认情况下都会均匀地分布到所有 Broker 上,一旦领导者副本所在的 Broker 宕机，Kafka 会从追随者副本中选举出新的领导者继续提供服务。Leader： 副本中的领导者。负责对外提供服务，与客户端进行交互。生产者总是向 Leader 副本些消息，消费者总是从 Leader 读消息Follower： 副本中的追随者。被动地追随 Leader，不能与外界进
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行交付。只是向 Leader 发送消息，请求 Leader把最新生产的消息发给它，进而保持同步。Kafka 中 AR、ISR、OSR 三者的概念AR：分区中所有副本称为 ARISR：所有与主副本保持一定程度同步的副本（包括主副本）称为 ISROSR：与主副本滞后过多的副本组成 OSR分区副本什么情况下会从 ISR 中剔出Leader 会维护一个与自己基本保持同步的 Replica列表，该列表称为 ISR，每个Partition都会有一个 ISR，而且是由 Leader 动态维护。所谓动态维护，就是说如果一个 Follower比一个 Leader 落后太多，或者超过一定时间未发起数据复制请求，则 Leader 将其从 ISR 中移除。当 ISR 中所有 Replica 都向 Leader 发送 ACK（Acknowledgement确认）时，Leader 才 commit分区副本中的 Leader 如果宕机但 ISR 却为空该如何处理可以通过配置 unclean.leader.election ：true：允许 OSR 成为 Leader，但是 OSR 的消息较为滞后，可能会出现消息不一致的问题false：会一
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直等待旧 leader 恢复正常，降低了可用性Kafka的 Producer 有三种 ack机制，参数值有 0、1 和 -10： 相当于异步操作，Producer 不需要 Leader 给予回复，发送完就认为成功，继续发送下一条（批）Message。此机制具有最低延迟，但是持久性可靠性也最差，当服务器发生故障时，很可能发生数据丢失。1： Kafka 默认的设置。表示 Producer 要 Leader 确认已成功接收数据才发送下一条（批）Message。不过 Leader 宕机，Follower 尚未复制的情况下，数据就会丢失。此机制提供了较好的持久性和较低的延迟性。-1： Leader 接收到消息之后，还必须要求 ISR 列表里跟 Leader 保持同步的那些Follower都确认消息已同步，Producer 才发送下一条（批）Message。此机制持久性可靠性最好，但延时性最差Kafka 的 consumer 如何消费数据在 Kafka中，Producers 将消息推送给 Broker 端，在 Consumer 和 Broker 建立连接之后，会主动去 Pull（或者说 Fetch）消息。这种模式有些优点
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，首先 Consumer端可以根据自己的消费能力适时的去 fetch消息并处理，且可以控制消息消费的进度（offset）；此外，消费者可以控制每次消费的数，实现批量消费。Kafka 的 Topic 中 Partition 数据是怎么存储到磁盘的用磁盘顺序写+内存页缓存+稀疏索引Topic 中的多个 Partition 以文件夹的形式保存到 Broker，每个分区序号从 0递增，且消息有序。Partition 文件下有多个 Segment（xxx.index，xxx.log），Segment文件里的大小和配置文件大小一致。默认为 1GB，但可以根据实际需要修改。如果大小大于 1GB时，会滚动一个新的 Segment并且以上一个 Segment 最后一条消息的偏移量命名。生产者发送消息│▼Leader Partition│▼ (追加写入)当前活跃 Segment: [00000000000000.log]│▼ (每隔 4KB 数据)更新 .index/.timeindex│▼ (Segment 满 1GB)创建新 Segment: [00000000000015.log]消费者请求 offset=520│▼查 .
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: index 文件 → 找到 offset=500 → position=10240│▼从 .log 文件 10240 位置顺序扫描│▼找到 offset=520 的消息返回Kafka 创建 Topic 后如何将分区放置到不同的 Broker 中Kafka创建 Topic 将分区放置到不同的 Broker 时遵循以下规则：副本因子不能大于 Broker 的个数。第一个分区（编号为 0）的第一个副本放置位置是随机从 Broker List 中选择的。其他分区的第一个副本放置位置相对于第 0个分区依次往后移。也就是如果有 3个 Broker，3 个分区，假设第一个分区放在第二个 Broker 上，那么第二个分区将会放在第三个 Broker 上；第三个分区将会放在第一个 Broker 上，更多 Broker 与更多分区依此类推。剩余的副本相对于第一个副本放置位置其实是由nextReplicaShift决定的，而这个数也是随机产生的。Kafka 中如何进行主从同步Kafka动态维护了一个同步状态的副本的集合（a set of In-SyncReplicas），简称 ISR，在这个集合中的结点都是和 Leader 保持高
2025-08-11 10:56:08.171 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度一致的，任何一条消息只有被这个集合中的每个结点读取并追加到日志中，才会向外部通知“这个消息已经被提交”同步复制Producer 会先通过 Zookeeper识别到 Leader，然后向 Leader 发送消息，Leader收到消息后写入到本地 log文件。这个时候 Follower 再向 Leader Pull 消息，Pull回来的消息会写入的本地 log 中，写入完成后会向 Leader 发送 Ack 回执，等到 Leader 收到所有 Follower 的回执之后，才会向 Producer 回传 Ack。异步复制Kafka 中 Producer 异步发送消息是基于同步发送消息的接口来实现的，异步发送消息的实现很简单，客户端消息发送过来以后，会先放入一个 BlackingQueue队列中然后就返回了。Producer 再开启一个线程 ProducerSendTread 不断从队列中取出消息，然后调用同步发送消息的接口将消息发送给 Broker。Kafka 中什么情况下会出现消息丢失/不一致的问题消息发送时消息发送有两种方式：同步 - sync 和 异步 - async。默认是同步的方式，可以通过 prod
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ucer.type 属性进行配置，kafka 也可以通过配置 acks 属性来确认消息的生产0：表示不进行消息接收是否成功的确认1：表示当 leader 接收成功时的确认-1：表示 leader 和 follower 都接收成功的确认当 acks = 0 时，不和 Kafka 进行消息接收确认，可能会因为网络异常，缓冲区满的问题，导致消息丢失当 acks = 1 时，只有 leader 同步成功而 follower 尚未完成同步，如果 leader挂了，就会造成数据丢失消息消费时Kafka 有两个消息消费的 consumer 接口，分别是 low-level 和 hign-levellow-level：消费者自己维护 offset 等值，可以实现对 kafka 的完全控制high-level：封装了对 partition 和 offset，使用简单如果使用高级接口，可能存在一个消费者提取了一个消息后便提交了 offset，那么还没来得及消费就已经挂了，下次消费时的数据就是 offset + 1 的位置，那么原先 offset 的数据就丢失了Kafa 中如何保证顺序消费Kafka 的消费单元是 Partitio
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n，同一个 Partition 使用 offset 作为唯一标识保证顺序性，但这只是保证了在 Partition 内部的顺序性而不是 Topic 中的顺序，因此我们需要将所有消息发往统一 Partition 才能保证消息顺序消费，那么可以在发送的时候指定 MessageKey，同一个 key 的消息会发到同一个 Partition 中。Java介绍一下 javajava 是一门开源的跨平台的面向对象的计算机语言.跨平台是因为 java 的 class 文件是运行在虚拟机上的,其实跨平台的,而虚拟机是不同平台有不同版本,所以说 java 是跨平台的.面向对象有几个特点:1.封装两层含义：一层含义是把对象的属性和行为看成一个密不可分的整体，将这两者'封装'在一个不可分割的独立单元(即对象)中另一层含义指'信息隐藏，把不需要让外界知道的信息隐藏起来，有些对象的属性及行为允许外界用户知道或使用，但不允许更改，而另一些属性或行为，则不允许外界知晓，或只允许使用对象的功能，而尽可能隐藏对象的功能实现细节。2.继承继承就是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有父类相同的行为。3.多态多态是同一个行为具有多个不同表现形式或形态的能力。Java 语言中含有方法重载与对象多态两种形式的多态：1.方法重载：在一个类中，允许多个方法使用同一个名字，但方法的参数不同，完成的功能也不同。2.对象多态：子类对象可以与父类对象进行转换，而且根据其使用的子类不同完成的功能也不同（重写父类的方法）。Java有哪些数据类型？java 主要有两种数据类型1.基本数据类型基本数据有八个,byte,short,int,long 属于数值型中的整数型float,double属于数值型中的浮点型char属于字符型boolean属于布尔型2.引用数据类型引用数据类型有三个,分别是类,接口和数组接口和抽象类有什么区别？1.接口是抽象类的变体，接口中所有的方法都是抽象的。而抽象类是声明方法的存在而不去实现它的类。2.接口可以多继承，抽象类不行。3.接口定义方法，不能实现，默认是 public abstract，而抽象类可以实现部分方法。4.接口中基本数据类型为 public static final 并且需要给出初始值，而抽类象不是的。重载和重写什么区别？重写：1.参数列表必须完全与被重写的方法相同，
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否则不能称其为重写而是重载.2.返回的类型必须一直与被重写的方法的返回类型相同，否则不能称其为重写而是重载。3.访问修饰符的限制一定要大于被重写方法的访问修饰符4.重写方法一定不能抛出新的检查异常或者比被重写方法申明更加宽泛的检查型异常。重载：1.必须具有不同的参数列表；2.可以有不同的返回类型，只要参数列表不同就可以了；3.可以有不同的访问修饰符；4.可以抛出不同的异常；常见的异常有哪些？NullPointerException 空指针异常ArrayIndexOutOfBoundsException 索引越界异常InputFormatException 输入类型不匹配SQLException SQL 异常IllegalArgumentException 非法参数NumberFormatException 类型转换异常 等等....异常要怎么解决？Java标准库内建了一些通用的异常，这些类以 Throwable 为顶层父类。Throwable又派生出 Error 类和 Exception类。错误：Error类以及他的子类的实例，代表了 JVM本身的错误。错误不能被程序员通过代码处理，Error 很少出现。因此
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，程序员应该关注 Exception 为父类的分支下的各种异常类。异常：Exception 以及他的子类，代表程序运行时发送的各种不期望发生的事件。可以被 Java异常处理机制使用，是异常处理的核心。hashMap 线程不安全体现在哪里？在 hashMap1.7 中扩容的时候，因为采用的是头插法，所以会可能会有循环链表产生，导致数据有问题，在 1.8 版本已修复，改为了尾插法在任意版本的 hashMap 中，如果在插入数据时多个线程命中了同一个槽，可能会有数据覆盖的情况发生，导致线程不安全。说说进程和线程的区别？进程是系统资源分配和调度的基本单位，它能并发执行较高系统资源的利用率.线程是比进程更小的能独立运行的基本单位,创建、销毁、切换成本要小于进程,可以减少程序并发执行时的时间和空间开销，使得操作系统具有更好的并发性Integer a = 1000，Integer b = 1000，a==b 的结果是什么？那如果 a，b 都为 1，结果又是什么？Integer a = 1000，Integer b = 1000，a==b 结果为 falseInteger a = 1，Integer b = 1，a==b 结
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果为 true这道题主要考察 Integer 包装类缓存的范围,在-128~127 之间会缓存起来,比较的是直接缓存的数据,在此之外比较的是对象JMM 就是 Java 内存模型(java memory model)。因为在不同的硬件生产商和不同的操作系统下，内存的访问有一定的差异，所以会造成相同的代码运行在不同的系统上会出现各种问题。所以 java 内存模型(JMM)屏蔽掉各种硬件和操作系统的内存访问差异，以实现让 java 程序在各种平台下都能达到一致的并发效果。Java内存模型规定所有的变量都存储在主内存中，包括实例变量，静态变量，但是不包括局部变量和方法参数。每个线程都有自己的工作内存，线程的工作内存保存了该线程用到的变量和主内存的副本拷贝，线程对变量的操作都在工作内存中进行。线程不能直接读写主内存中的变量。每个线程的工作内存都是独立的，线程操作数据只能在工作内存中进行，然后刷回到主存。这是 Java 内存模型定义的线程基本工作方式cas 是什么？cas 叫做 CompareAndSwap，比较并交换，很多地方使用到了它，比如锁升级中自旋锁就有用到，主要是通过处理器的指令来保证操作的原子性，它主要包含三
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个变量：当一个线程需要修改一个共享变量的值，完成这个操作需要先取出共享变量的值，赋给 A，基于 A 进行计算，得到新值 B，在用预期原值 A 和内存中的共享变量值进行比较，如果相同就认为其他线程没有进行修改，而将新值写入内存聊聊 ReentrantLock 吧ReentrantLock 意为可重入锁，说起 ReentrantLock 就不得不说 AQS ，因为其底层就是使用 AQS 去实现的。ReentrantLock有两种模式，一种是公平锁，一种是非公平锁。公平模式下等待线程入队列后会严格按照队列顺序去执行非公平模式下等待线程入队列后有可能会出现插队情况公平锁第一步：获取状态的 state 的值如果 state=0 即代表锁没有被其它线程占用，执行第二步。如果 state!=0 则代表锁正在被其它线程占用，执行第三步。第二步：判断队列中是否有线程在排队等待如果不存在则直接将锁的所有者设置成当前线程，且更新状态 state 。如果存在就入队。第三步：判断锁的所有者是不是当前线程如果是则更新状态 state 的值。如果不是，线程进入队列排队等待。非公平锁获取状态的 state 的值如果 state=0 即代表锁
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 没有被其它线程占用，则设置当前锁的持有者为当前线程，该操作用 CAS 完成。如果不为 0或者设置失败，代表锁被占用进行下一步。此时获取 state 的值如果是，则给 state+1，获取锁如果不是，则进入队列等待如果是 0，代表刚好线程释放了锁，此时将锁的持有者设为自己如果不是 0，则查看线程持有者是不是自己多线程的创建方式有哪些？继承 Thread类，重写 run()方法public class Demo extends Thread{//重写父类 Thread的 run()public void run() {}public static void main(String[] args) {Demo d1 = new Demo();Demo d2 = new Demo();d1.start();d2.start();}}实现 Runnable接口，重写 run()public class Demo2 implements Runnable{//重写 Runnable接口的 run()public void run() {}public static void main(String[] args) {Th
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: read t1 = new Thread(new Demo2());Thread t2 = new Thread(new Demo2());t1.start();t2.start();}}实现 Callable 接口public class Demo implements Callable<String>{public String call() throws Exception {System.out.println("正在执行新建线程任务");Thread.sleep(2000);return "结果";}public static void main(String[] args) throws InterruptedException,ExecutionException {Demo d = new Demo();FutureTask<String> task = new FutureTask<>(d);Thread t = new Thread(task);t.start();//获取任务执行后返回的结果String result = task.get();}}使用线程池创建public class 
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Demo {public static void main(String[] args) {Executor threadPool = Executors.newFixedThreadPool(5);for(int i = 0 ;i < 10 ; i++) {threadPool.execute(new Runnable() {public void run() {//todo}});}}}线程池有哪些参数？corePoolSize：核心线程数，线程池中始终存活的线程数。2.maximumPoolSize: 最大线程数，线程池中允许的最大线程数。3.keepAliveTime: 存活时间，线程没有任务执行时最多保持多久时间会终止。4.unit: 单位，参数 keepAliveTime 的时间单位，7种可选。5.workQueue: 一个阻塞队列，用来存储等待执行的任务，均为线程安全，7 种可选。6.threadFactory: 线程工厂，主要用来创建线程，默及正常优先级、非守护线程。7.handler：拒绝策略，拒绝处理任务时的策略，4 种可选，默认为 AbortPolicy。线程池的执行流程？判断线程池中的
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程数是否大于设置的核心线程数如果小于，就创建一个核心线程来执行任务如果大于，就会判断缓冲队列是否满了如果没有满，则放入队列，等待线程空闲时执行任务如果队列已经满了，则判断是否达到了线程池设置的最大线程数如果没有达到，就创建新线程来执行任务如果已经达到了最大线程数，则执行指定的拒绝策略深拷贝、浅拷贝是什么？浅拷贝并不是真的拷贝，只是复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象聊聊 ThreadLocal 吧ThreadLocal其实就是线程本地变量，他会在每个线程都创建一个副本，那么在线程之间访问内部副本变量就行了，做到了线程之间互相隔离。一个对象的内存布局是怎么样的?1.对象头: 对象头又分为 MarkWord 和 Class Pointer 两部分。MarkWord:包含一系列的标记位，比如轻量级锁的标记位，偏向锁标记位,gc 记录信息等等。ClassPointer:用来指向对象对应的 Class 对象（其对应的元数据对象）的内存地址。在 32 位系统占 4 字节，在 64 位系统中占 8 字节
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 。2.Length:只在数组对象中存在，用来记录数组的长度，占用 4 字节3.Instance data: 对象实际数据，对象实际数据包括了对象的所有成员变量，其大小由各个成员变量的大小决定。(这里不包括静态成员变量，因为其是在方法区维护的)4.Padding:Java 对象占用空间是 8 字节对齐的，即所有 Java 对象占用 bytes 数必须是 8 的倍数,是因为当我们从磁盘中取一个数据时，不会说我想取一个字节就是一个字节，都是按照一块儿一块儿来取的，这一块大小是 8 个字节，所以为了完整，padding 的作用就是补充字节，保证对象是 8 字节的整数倍。HashMapHashMap的底层数据结构是什么？JDK 7 中，HashMap 由“数组+链表”组成，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的。在 JDK 8 中，HashMap 由“数组+链表+红黑树”组成。链表过长，会严重影响HashMap 的性能，而红黑树搜索的时间复杂度是 O(logn)，而链表是糟糕的 O(n)。因此，JDK 8 对数据结构做了进一步的优化，引入了红黑树，链表和红黑树在达到一定条件会进行转换：当链
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 表超过 8 且数据总量超过 64 时会转红黑树。将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树，以减少搜索时间。解决 hash冲突的办法有哪些？HashMap用的哪种？解决 Hash 冲突方法有：开放定址法：也称为再散列法，基本思想就是，如果 p=H(key)出现冲突时，则以p为基础，再次 hash，p1=H(p),如果 p1再次出现冲突，则以 p1为基础，以此类推，直到找到一个不冲突的哈希地址 pi。因此开放定址法所需要的 hash表的长度要大于等于所需要存放的元素，而且因为存在再次 hash，所以只能在删除的节点上做标记，而不能真正删除节点。再哈希法：双重散列，多重散列，提供多个不同的 hash函数，当 R1=H1(key1)发生冲突时，再计算 R2=H2(key1)，直到没有冲突为止。这样做虽然不易产生堆集，但增加了计算的时间。链地址法：拉链法，将哈希值相同的元素构成一个同义词的单链表，并将单链表的头指针存放在哈希表的第 i 个单元中，查找、插入和删除主要在同义词链表中进行。链表法适用于经常进行插入和删除的情况。建立公共溢出区：将哈希表分为公共表和
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 溢出表，当溢出发生时，将所有溢出数据统一放到溢出区。HashMap中采用的是链地址法为什么在解决 hash 冲突的时候，不直接用红黑树？而选择先用链表，再转红黑树?因为红黑树需要进行左旋，右旋，变色这些操作来保持平衡，而单链表不需要。当元素小于 8 个的时候，此时做查询操作，链表结构已经能保证查询性能。当元素大于 8 个的时候， 红黑树搜索时间复杂度是 O(logn)，而链表是 O(n)，此时需要红黑树来加快查询速度，但是新增节点的效率变慢了。因此，如果一开始就用红黑树结构，元素太少，新增效率又比较慢，无疑这是浪费性能的。为什么 hash 值要与 length-1 相与？把 hash 值对数组长度取模运算，模运算的消耗很大，没有位运算快。当 length 总是 2 的 n次方时，h& (length-1) 运算等价于对 length取模，也就是 h%length，但是 & 比 % 具有更高的效率HashMap数组的长度为什么是 2 的幂次方？2 的 N 次幂有助于减少碰撞的几率。如果 length 为 2的幂次方，则 length-1 转化为二进制必定是 11111……的形式，在与 h 的二进制与操作效率会非
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 常的快，而且空间不浪费。HashMap 的 put方法流程？首先根据 key 的值计算 hash 值，找到该元素在数组中存储的下标；2、如果数组是空的，则调用 resize 进行初始化；3、如果没有哈希冲突直接放在对应的数组下标里；4、如果冲突了，且 key 已经存在，就覆盖掉 value；5、如果冲突后，发现该节点是红黑树，就将这个节点挂在树上；6、如果冲突后是链表，判断该链表是否大于 8 ，如果大于 8 并且数组容量小于 64，就进行扩容；如果链表节点大于 8 并且数组的容量大于 64，则将这个结构转换为红黑树；否则，链表插入键值对，若 key 存在，就覆盖掉 value。一般用什么作为 HashMap的 key?一般用 Integer、String 这种不可变类当作 HashMap 的 key，String 最为常见。因为字符串是不可变的，所以在它创建的时候 hashcode 就被缓存了，不需要重新计算。因为获取对象的时候要用到 equals() 和 hashCode() 方法，那么键对象正确的重写这两个方法是非常重要的。Integer、String 这些类已经很规范的重写了hashCode() 以及 
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: equals() 方法。MySQL关系型和非关系型数据库的区别？关系型数据库的优点容易理解，因为它采用了关系模型来组织数据。可以保持数据的一致性。数据更新的开销比较小。支持复杂查询（带 where 子句的查询）非关系型数据库（NOSQL）的优点无需经过 SQL 层的解析，读写效率高。基于键值对，读写性能很高，易于扩展可以支持多种类型数据的存储，如图片，文档等等。扩展（可分为内存性数据库以及文档型数据库，比如 Redis，MongoDB，HBase 等，适合场景：数据量大高可用的日志系统/地理位置存储系统）。详细说一下一条 MySQL 语句执行的步骤Server 层按顺序执行 SQL 的步骤为：客户端请求 -> 连接器（验证用户身份，给予权限）查询缓存（存在缓存则直接返回，不存在则执行后续操作）分析器（对 SQL 进行词法分析和语法分析操作）优化器（主要对执行的 SQL 优化选择最优的执行方案方法）执行器（执行时会先看用户是否有执行权限，有才去使用这个引擎提供的接口）-> 去引擎层获取数据返回（如果开启查询缓存则会缓存查询结果）MySQL 使用索引的原因？根本原因索引的出现，就是为了提高数据查询的效率，就像书的
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 目录一样。对于数据库的表而言，索引其实就是它的“目录”。扩展创建唯一性索引，可以保证数据库表中每一行数据的唯一性。帮助引擎层避免排序和临时表将随机 IO 变为顺序 IO，加速表和表之间的连接索引的三种常见底层数据结构以及优缺点三种常见的索引底层数据结构：分别是哈希表、有序数组和搜索树。哈希表这种适用于等值查询的场景，比如 memcached 以及其它一些 NoSQL 引擎，不适合范围查询，哈希表的数据是完全无序存储的。它只能回答“某个键值等于多少”的记录在哪，无法高效地查询“键值在某个范围之间”的所有记录（如WHERE id BETWEEN 10 AND 20）。需要扫描全表或遍历所有桶，效率极低 (O(n))。有序数组索引只适用于静态存储引擎，等值和范围查询性能好，但更新数据成本高。N 叉树由于读写上的性能优点以及适配磁盘访问模式以及广泛应用在数据库引擎中。扩展（以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200。棵树高是 4 的时候，就可以存 1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了。）索引的常见类型以及它是如何发挥作用的？根据叶子节点的内容，索引类型分为主键索引和非主键索引。主键索引的叶子节点存的整行数据，在 InnoDB 里也被称为聚簇索引。非主键索引叶子节点存的主键的值，在 InnoDB 里也被称为二级索引MyISAM 和 InnoDB 实现 B 树索引方式的区别是什么？InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身，其数据文件本身就是索引文件。MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址，叶节点的 data域存放的是数据记录的地址，索引文件和数据文件是分离的InnoDB 为什么设计 B+ 树索引？两个考虑因素：InnoDB 需要执行的场景和功能需要在特定查询上拥有较强的性能。CPU 将磁盘上的数据加载到内存中需要花费大量时间。为什么选择 B+ 树：哈希索引虽然能提供 O（1）复杂度查询，但对范围查询和排序却无法很好的支持，最终会导致全表扫描。B 树能够在非叶子节点存储数据，但会导致在查询连续数据可能带来更多的随机IO。而 B+ 树的所有叶节点可以通过指针来相互连接
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，减少顺序遍历带来的随机 IO。普通索引还是唯一索引？由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发建议你优先考虑非唯一索引。什么是覆盖索引和索引下推？覆盖索引：在某个查询里面，索引 k 已经“覆盖了”我们的查询需求，称为覆盖索引。覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。索引下推：MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。MySQL 的 change buffer 是什么？当需要更新一个数据页时，如果数据页在内存中就直接更新；而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InnoDB 会将这些更新操作缓存在 change buffer 中。这样就不需要从磁盘中读入这个数据页了，在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。注意唯一索引的更新就不
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 能使用 change buffer，实际上也只有普通索引可以使用。适用场景：对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 changebuffer 的维护代价。MySQL 是如何判断一行扫描数的？MySQL 在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条。而只能根据统计信息来估算记录数。这个统计信息就是索引的“区分度。redo log 和 binlog 的区别？为什么需要 redo log？redo log 主要用于 MySQL 异常重启后的一种数据恢复手段，确保了数据的一致性。其实是为了配合 MySQL 的 WAL 机制。因为 MySQL 进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是这样，会存在 crash 后 内存
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据丢失的隐患，而 redo log 具备 crash safe 崩溃恢复 的能力。为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？第一点：redo log 可确保 innoDB 判断哪些数据已经刷盘，哪些数据还没有redo log 和 binlog 有一个很大的区别就是，一个是循环写，一个是追加写。也就是说 redo log 只会记录未刷盘的日志，已经刷入磁盘的数据都会从 redo log这个有限大小的日志文件里删除。binlog 是追加日志，保存的是全量的日志。当数据库 crash 后，想要恢复未刷盘但已经写入 redo log 和 binlog 的数据到内存时，binlog 是无法恢复的。虽然 binlog 拥有全量的日志，但没有一个标志让innoDB 判断哪些数据已经刷盘，哪些数据还没有。但 redo log 不一样，只要刷入磁盘的数据，都会从 redo log 中抹掉，因为是循环写！数据库重启后，直接把 redo log 中的数据都恢复至内存就可以了。第二点：如果 redo log 写入失败，说明此次操作失败，事务也不可能提交redo log 每次更新操作
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 完成后，就一定会写入日志，如果写入失败，说明此次操作失败，事务也不可能提交。redo log 内部结构是基于页的，记录了这个页的字段值变化，只要 crash 后读取redo log 进行重放，就可以恢复数据。这就是为什么 redo log 具有 crash-safe 的能力，而 binlog 不具备当数据库 crash 后，如何恢复未刷盘的数据到内存中？根据 redo log 和 binlog 的两阶段提交，未持久化的数据分为几种情况：change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失。change buffer 写入，redo log fsync 未 commit，binlog 已经 fsync 到磁盘，先从binlog 恢复 redo log，再从 redo log 恢复 change buffer。change buffer 写入，redo log 和 binlog 都已经 fsync，直接从 redo log 里恢复。redo log 写入方式？redo log 包括两部分内容，分别是内存中的日志缓冲(re
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: do log buffer)和磁盘上的日志文件(redo log file)。MySQL 每执行一条 DML 语句，会先把记录写入 redo log buffer（用户空间） ，再保存到内核空间的缓冲区 OS-buffer 中，后续某个时间点再一次性将多个操作记录写到 redo log file（刷盘） 。这种先写日志，再写磁盘的技术，就是 WAL。可以发现，redo log buffer 写入到 redo log file，是经过 OS buffer 中转的。其实可以通过参数 innodb_flush_log_at_trx_commit 进行配置，参数值含义如下：0：称为延迟写，事务提交时不会将 redo log buffer 中日志写入到 OS buffer，而是每秒写入 OS buffer 并调用写入到 redo log file 中。1：称为实时写，实时刷”，事务每次提交都会将 redo log buffer 中的日志写入 OS buffer 并保存到 redo log file 中。2： 称为实时写，延迟刷。每次事务提交写入到 OS buffer，然后是每秒将日志写入到 redo log file。
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: redo log 的执行流程?MySQL 客户端将请求语句 update T set a =1 where id =666，发往 MySQL Server层。MySQL Server 层接收到 SQL 请求后，对其进行分析、优化、执行等处理工作，将生成的 SQL 执行计划发到 InnoDB 存储引擎层执行。InnoDB 存储引擎层将 a修改为 1的这个操作记录到内存中。记录到内存以后会修改 redo log 的记录，会在添加一行记录，其内容是需要在哪个数据页上做什么修改。此后，将事务的状态设置为 prepare ，说明已经准备好提交事务了。等到 MySQL Server 层处理完事务以后，会将事务的状态设置为 commit，也就是提交该事务。在收到事务提交的请求以后，redo log 会把刚才写入内存中的操作记录写入到磁盘中，从而完成整个日志的记录过程。binlog 的概念是什么，起到什么作用， 可以保证 crash-safe 吗?binlog 是归档日志，属于 MySQL Server 层的日志。可以实现主从复制和数据恢复两个作用。当需要恢复数据时，可以取出某个时间范围内的 binlog 进行重放恢复。但是
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  binlog 不可以做 crash safe，因为 crash 之前，binlog 可能没有写入完全 MySQL 就挂了。所以需要配合 redo log 才可以进行 crash safe。什么是两阶段提交？MySQL 将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入 binlog，这就是"两阶段提交"。而两阶段提交就是让这两个状态保持逻辑上的一致。redolog 用于恢复主机故障时的未更新的物理数据，binlog 用于备份操作。两者本身就是两个独立的个体，要想保持一致，就必须使用分布式事务的解决方案来处理。为什么需要两阶段提交呢?如果不用两阶段提交的话，可能会出现这样情况先写 redo log，crash 后 bin log 备份恢复时少了一次更新，与当前数据不一致。先写 bin log，crash 后，由于 redo log 没写入，事务无效，所以后续 bin log备份恢复时，数据不一致。两阶段提交就是为了保证 redo log 和 binlog 数据的安全一致性。只有在这两个日志文件逻辑上高度一致了才能放心的使用。在恢复数据时，redolog 状态为 com
2025-08-11 10:56:08.172 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: mit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。MySQL 怎么知道 binlog 是完整的?一个事务的 binlog 是有完整格式的：statement 格式的 binlog，最后会有 COMMIT；row 格式的 binlog，最后会有一个 XID event什么是 WAL 技术，有什么优点？WAL，中文全称是 Write-Ahead Logging，它的关键点就是日志先写内存，再写磁盘。MySQL 执行更新操作后，在真正把数据写入到磁盘前，先记录日志。好处是不用每一次操作都实时把数据写盘，就算 crash 后也可以通过 redo log恢复，所以能够实现快速响应 SQL 语句redo log 日志格式redo log buffer (内存中)是由首尾相连的四个文件组成的，它们分别是：ib_logfile_1、ib_logfile_2、ib_logfile_3、ib_logfile_4。write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。chec
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: kpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。有了 redo log，当数据库发生宕机重启后，可通过 redo log 将未落盘的数据（check point 之后的数据）恢复，保证已经提交的事务记录不会丢失，这种能力称为 crash-safe。InnoDB 数据页结构一个数据页大致划分七个部分File Header：表示页的一些通用信息，占固定的 38 字节。page Header：表示数据页专有信息，占固定的 56 字节。inimum+Supermum：两个虚拟的伪记录，分别表示页中的最小记录和最大记录，占固定的 26 字节。User Records：真正存储我们插入的数据，大小不固定。Free Space：页中尚未使用的部分，大小不固定。Page Directory：页中某些记录的相对位置，也就是各个槽对
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 应的记录在页面中的地址偏移量。File Trailer：用于检验页是否完整，占固定大小 8 字节。MySQL 是如何保证数据不丢失的？只要 redolog 和 binlog 保证持久化磁盘就能确保 MySQL 异常重启后回复数据在恢复数据时，redolog 状态为 commit 则说明 binlog 也成功，直接恢复数据；如果 redolog 是 prepare，则需要查询对应的 binlog 事务是否成功，决定是回滚还是执行。28、误删数据怎么办？DBA 的最核心的工作就是保证数据的完整性，先要做好预防，预防的话大概是通过这几个点：权限控制与分配(数据库和服务器权限)制作操作规范定期给开发进行培训搭建延迟备库做好 SQL 审计，只要是对线上数据有更改操作的语句(DML 和 DDL)都需要进行审核做好备份。备份的话又分为两个点 (1)如果数据量比较大，用物理备份xtrabackup。定期对数据库进行全量备份，也可以做增量备份。 (2)如果数据量较少，用 mysqldump 或者 mysqldumper。再利用 binlog 来恢复或者搭建主从的方式来恢复数据。 定期备份 binlog 文件也是很有必要的如果发
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 生了数据删除的操作，又可以从以下几个点来恢复:DML 误操作语句造成数据不完整或者丢失。可以通过 flashback，美团的myflash，也是一个不错的工具，本质都差不多，都是先解析 binlog event，然后在进行反转。把 delete 反转为 insert，insert 反转为 delete，update 前后 image 对调。所以必须设置 binlog_format=row 和 binlog_row_image=full，切记恢复数据的时候，应该先恢复到临时的实例，然后在恢复回主库上。DDL 语句误操作(truncate 和 drop)，由于 DDL 语句不管 binlog_format 是 row还是 statement ，在 binlog 里都只记录语句，不记录 image 所以恢复起来相对要麻烦得多。只能通过全量备份+应用 binlog 的方式来恢复数据。一旦数据量比较大，那么恢复时间就特别长rm 删除：使用备份跨机房，或者最好是跨城市保存。DDL（数据定义语言） DML（数据操纵语言） DQL（数据查询语言）drop、truncate 和 delete 的区别DELETE 语句执行删除的
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程是每次从表中删除一行，并且同时将该行的删除操作作为事务记录在日志中保存以便进行进行回滚操作。TRUNCATE TABLE 则一次性地从表中删除所有的数据并不把单独的删除操作记录记入日志保存，删除行是不能恢复的。并且在删除的过程中不会激活与表有关的删除触发器。执行速度快。drop 语句将表所占用的空间全释放掉。在速度上，一般来说，drop> truncate > delete。如果想删除部分数据用 delete，注意带上 where 子句，回滚段要足够大；如果想删除表，当然用 drop； 如果想保留表而将所有数据删除，如果和事务无关，用 truncate 即可；如果和事务有关，或者想触发 trigger，还是用 delete； 如果是整理表内部的碎片，可以用 truncate 跟上 reuse stroage，再重新导入/插入数据MySQL 存储引擎介绍（InnoDB、MyISAM、MEMORY）InnoDB 是事务型数据库的首选引擎，支持事务安全表 (ACID)，支持行锁定和外键。MySQL5.5.5 之后，InnoDB 作为默认存储引擎MyISAM 基于 ISAM 的存储引擎，并对其进行扩展。它是在 W
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: eb、数据存储和其他应用环境下最常用的存储引擎之一。MyISAM 拥有较高的插入、查询速度，但不支持事务。在 MySQL5.5.5 之前的版本中，MyISAM 是默认存储引擎MEMORY 存储引擎将表中的数据存储到内存中，为查询和引用其他表数据提供快速访问。都说 InnoDB 好，那还要不要使用 MEMORY 引擎？内存表就是使用 memory 引擎创建的表为什么我不建议你在生产环境上使用内存表。这里的原因主要包括两个方面：锁粒度问题；数据持久化问题。由于重启会丢数据，如果一个备库重启，会导致主备同步线程停止；如果主库跟这个备库是双 M 架构，还可能导致主库的内存表数据被删掉MySQL 是如何保证主备同步？主备关系的建立：一开始创建主备关系的时候，是由备库指定的，比如基于位点的主备关系，备库说“我要从 binlog 文件 A的位置 P”开始同步，主库就从这个指定的位置开始往后发。而主备关系搭建之后，是主库决定要发给数据给备库的，所以主库有新的日志也会发给备库。MySQL 主备切换流程：客户端读写都是直接访问 A，而节点 B是备库，只要将 A的更新都同步过来，到本地执行就可以保证数据是相同的。当需要切换的时候就
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 把节点换一下，A的节点 B的备库一个事务完整的同步过程：备库 B和主库 A建立来了长链接，主库 A内部专门线程用于维护了这个长链接。在备库B上通过changemaster命令设置主库A的IP端口用户名密码以及从哪个位置开始请求 binlog 包括文件名和日志偏移量在备库 B上执行 start-slave 命令备库会启动两个线程：io_thread 和sql_thread 分别负责建立连接和读取中转日志进行解析执行备库读取主库传过来的 binlog 文件备库收到文件写到本地成为中转日志后来由于多线程复制方案的引入，sql_thread 演化成了多个线程什么是主备延迟主库和备库在执行同一个事务的时候出现时间差的问题，主要原因有：有些部署条件下，备库所在机器的性能要比主库性能差。备库的压力较大。大事务，一个主库上语句执行 10 分钟，那么这个事务可能会导致从库延迟 10分钟MySQL 的一主一备和一主多从有什么区别？在一主一备的双 M 架构里，主备切换只需要把客户端流量切到备库；而在一主多从架构里，主备切换除了要把客户端流量切到备库外，还需要把从库接到新主库上短时间提高 MySQL 性能的方法第一种方法：先处理掉那
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 些占着连接但是不工作的线程。或者再考虑断开事务内空闲太久的连接。 kill connection + id第二种方法：减少连接过程的消耗：慢查询性能问题在 MySQL 中，会引发性能问题的慢查询，大体有以下三种可能：索引没有设计好；SQL 语句没写好；MySQL选错了索引（force index）。InnoDB 为什么要用自增 ID 作为主键？自增主键的插入模式，符合递增插入，每次都是追加操作，不涉及挪动记录，也不会触发叶子节点的分裂。每次插入新的记录就会顺序添加到当前索引节点的后续位置，当一页写满，就会自动开辟一个新的页。而有业务逻辑的字段做主键，不容易保证有序插入，由于每次插入主键的值近似于随机因此每次新纪录都要被插到现有索引页得中间某个位置， 频繁的移动、分页操作造成了大量的碎片，得到了不够紧凑的索引结构，写数据成本较高。说一下 MySQL 的锁MySQL 在 server 层 和 存储引擎层 都运用了大量的锁MySQL server 层需要讲两种锁，第一种是 MDL(metadata lock) 元数据锁，第二种则 Table Lock 表锁。MDL 又名元数据锁，那么什么是元数据呢，任何描述数据库的
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内容就是元数据，比如我们的表结构、库结构等都是元数据。那为什么需要 MDL 呢？主要解决两个问题：事务隔离问题；数据复制问题InnoDB 有五种表级锁：IS（意向读锁）；IX（意向写锁）；S（读）；X（写）；AUTO-INC在对表进行 select/insert/delete/update 语句时候不会加表级锁IS 和 IX 的作用是为了判断表中是否有已经被加锁的记录自增主键的保障就是有 AUTO-INC 锁，是语句级别的：为表的某个列添加AUTO_INCREMENT 属性，之后在插⼊记录时，可以不指定该列的值，系统会⾃动为它赋上单调递增的值。InnoDB 4 种行级锁RecordLock：记录锁GapLock：间隙锁解决幻读；前一次查询不存在的东西在下一次查询出现了，其实就是事务 A中的两次查询之间事务 B执行插入操作被事务 A感知了Next-KeyLock：锁住某条记录又想阻止其它事务在改记录前面的间隙插入新纪录InsertIntentionLock：插入意向锁;如果插入到同一行间隙中的多个事务未插入到间隙内的同一位置则无须等待行锁和表锁的抉择全表扫描用行级锁索引是一种能提高数据库查询效率的数据结构。它可
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 以比作一本字典的目录，可以帮你快速找到对应的记录。索引一般存储在磁盘的文件中，它是占用物理空间的。正所谓水能载舟，也能覆舟。适当的索引能提高查询效率，过多的索引会影响数据库表的插入和更新功能。数据结构维度B+树索引：所有数据存储在叶子节点，复杂度为 O(logn)，适合范围查询。哈希索引: 适合等值查询，检索效率高，一次到位。全文索引：MyISAM 和 InnoDB 中都支持使用全文索引，一般在文本类型char,text,varchar 类型上创建。R-Tree 索引: 用来对 GIS 数据类型创建 SPATIAL 索引物理存储维度聚集索引：聚集索引就是以主键创建的索引，在叶子节点存储的是表中的数据。（Innodb 存储引擎）非聚集索引：非聚集索引就是以非主键创建的索引，在叶子节点存储的是主键和索引列。（Innodb 存储引擎）逻辑维度主键索引：一种特殊的唯一索引，不允许有空值。普通索引：MySQL 中基本索引类型，允许空值和重复值。联合索引：多个字段创建的索引，使用时遵循最左前缀原则。唯一索引：索引列中的值必须是唯一的，但是允许为空值。空间索引：MySQL5.7 之后支持空间索引，在空间索引这方面遵循 Op
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: enGIS 几何数据模型规则索引什么时候会失效？查询条件包含 or，可能导致索引失效如果字段类型是字符串，where 时一定用引号括起来，否则索引失效like 通配符可能导致索引失效。联合索引，查询时的条件列不是联合索引中的第一个列，索引失效。在索引列上使用 mysql 的内置函数，索引失效。对索引列运算（如，+、-、*、/），索引失效。索引字段上使用（！= 或者 < >，not in）时，可能会导致索引失效。索引字段上使用 is null， is not null，可能导致索引失效。左连接查询或者右连接查询查询关联的字段编码格式不一样，可能导致索引失效。mysql 估计使用全表扫描要比使用索引快,则不使用索引哪些场景不适合建立索引？数据量少的表，不适合加索引更新比较频繁的也不适合加索引区分度低的字段不适合加索引（如性别）where、group by、order by 等后面没有使用到的字段，不需要建立索引已经有冗余的索引的情况（比如已经有 a,b 的联合索引，不需要再单独建立 a索引）为什么不是一般二叉树？如果二叉树特殊化为一个链表，相当于全表扫描。平衡二叉树相比于二叉查找 树来说，查找效率更稳定，总体的查
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 找速度也更快。为什么不是平衡二叉树呢？我们知道，在内存比在磁盘的数据，查询效率快得多。如果树这种数据结构作 为索引，那我们每查找一次数据就需要从磁盘中读取一个节点，也就是我们说 的一个磁盘块，但是平衡二叉树可是每个节点只存储一个键值和数据的，如果 是 B树，可以存储更多的节点数据，树的高度也会降低，因此读取磁盘的次数 就降下来啦，查询效率就快啦。那为什么不是 B 树而是 B+树呢？B+树非叶子节点上是不存储数据的，仅存储键值，而 B 树节点中不仅存储 键值，也会存储数据。innodb 中页的默认大小是 16KB，如果不存储数据，那 么就会存储更多的键值，相应的树的阶数（节点的子节点树）就会更大，树就 会更矮更胖，如此一来我们查找数据进行磁盘的 IO 次数有会再次减少，数据查 询的效率也会更快。B+树索引的所有数据均存储在叶子节点，而且数据是按照顺序排列的，链 表连着的。那么 B+树使得范围查找，排序查找，分组查找以及去重查找变得 异常简单。一次 B+树索引树查找过程：select * from Temployee where age=32;这条 SQL 查询语句执行大概流程是这样的：搜索 idx_age 索引
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 树，将磁盘块 1加载到内存，由于 32<43,搜索左路分支，到磁盘寻址磁盘块 2。将磁盘块 2加载到内存中，由于 32<36,搜索左路分支，到磁盘寻址磁盘块 4。将磁盘块 4加载到内存中，在内存继续遍历，找到 age=32 的记录，取得 id = 400.拿到 id=400 后，回到 id 主键索引树。搜索 id 主键索引树，将磁盘块 1加载到内存，因为 300<400<500,所以在选择中间分支，到磁盘寻址磁盘块 3。虽然在磁盘块 3，找到了 id=400，但是它不是叶子节点，所以会继续往下找。到磁盘寻址磁盘块 8。将磁盘块 8加载内存，在内存遍历，找到 id=400 的记录，拿到 R4 这一行的数据，好的，大功告成。什么是回表？如何减少回表？当查询的数据在索引树中，找不到的时候，需要回到主键索引树中去获取，这个过程叫做回表。比如在第 6小节中，使用的查询 SQLselect * from Temployee where age=32;需要查询所有列的数据，idx_age 普通索引不能满足，需要拿到主键 id 的值后，再回到 id 主键索引查找获取，这个过程就是回表。什么是覆盖索引？如果我们查询 SQL 的
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  select * 修改为 select id, age 的话，其实是不需要回表的。因为 id 和 age 的值，都在 idx_age 索引树的叶子节点上，这就涉及到覆盖索引的知识点了。覆盖索引是 select 的数据列只用从索引中就能够取得，不必回表，换句话说，查询列要被所建的索引覆盖聊聊索引的最左前缀原则、索引的最左前缀原则，可以是联合索引的最左 N个字段。比如你建立一个组合索引（a,b,c），其实可以相当于建了（a），（a,b）,(a,b,c)三个索引，大大提高了索引复用能力。索引下推了解过吗？什么是索引下推select * from employee where name like '小%' and age=28 and sex='0';其中，name 和 age 为联合索引（idx_name_age）。如果是Mysql5.6之前，在idx_name_age索引树，找出所有名字第一个字是“小”的人，拿到它们的主键 id，然后回表找出数据行，再去对比年龄和性别等其他字段。有些朋友可能觉得奇怪，idx_name_age（name,age)不是联合索引嘛？为什么选出包含“小”字后，不再顺便看下年龄 age 
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 再回表呢，不是更高效嘛？所以呀，MySQL 5.6 就引入了索引下推优化，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。因此，MySQL5.6 版本之后，选出包含“小”字后，顺表过滤 age=28大表如何添加索引？如果一张表数据量级是千万级别以上的，那么，如何给这张表添加索引？我们需要知道一点，给表添加索引的时候，是会对表加锁的。如果不谨慎操作，有可能出现生产事故的。可以参考以下方法：先创建一张跟原表 A数据结构相同的新表 B。在新表 B添加需要加上的新索引。把原表 A数据导到新表 Brename 新表 B为原表的表名 A，原表 A换别的表名Hash 索引和 B+树区别是什么？你在设计索引是怎么抉择的？B+树可以进行范围查询，Hash 索引不能。B+树支持联合索引的最左侧原则，Hash 索引不支持。B+树支持 order by 排序，Hash 索引不支持。Hash 索引在等值查询上比 B+树效率更高。（但是索引列的重复值很多的话，Hash冲突，效率降低）。B+树使用 like 进行模糊查询的时候，like 后面（比如%开头）的话可以起到优化的作用，Hash 索引根
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 本无法进行模糊查询。索引有哪些优缺点？优点：索引可以加快数据查询速度，减少查询时间唯一索引可以保证数据库表中每一行的数据的唯一性缺点：创建索引和维护索引要耗费时间索引需要占物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间以表中的数据进行增、删、改的时候，索引也要动态的维护。聚簇索引与非聚簇索引的区别聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。它表示索引结构和数据一起存放的索引。非聚集索引是索引结构和数据分开存放的索引。接下来，我们分不同存存储引擎去聊哈~在 MySQL 的 InnoDB 存储引擎中， 聚簇索引与非聚簇索引最大的区别，在于叶节点是否存放一整行记录。聚簇索引叶子节点存储了一整行记录，而非聚簇索引叶子节点存储的是主键信息，因此，一般非聚簇索引还需要回表查询。一个表中只能拥有一个聚集索引（因为一般聚簇索引就是主键索引），而非聚集索引一个表则可以存在多个。一般来说，相对于非聚簇索引，聚簇索引查询效率更高，因为不用回表。而在 MyISM 存储引擎中，它的主键索引，普通索引都是非聚簇索引，因为数据和索引是分开的，叶子节点都使用一个地址指向真正的表数据。Nginx什么是 Ng
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: inx？Nginx 是一个 轻量级/高性能的反向代理 Web 服务器，用于 HTTP、HTTPS、SMTP、POP3 和 IMAP 协议。他实现非常高效的反向代理、负载平衡，他可以处理 2-3万并发连接数，官方监测能支持 5万并发，现在中国使用 nginx 网站用户有很多，例如：新浪、网易、 腾讯等。Nginx 怎么处理请求的？首先，Nginx 在启动时，会解析配置文件，得到需要监听的端口与 IP 地址，然后在 Nginx 的 Master 进程里面先初始化好这个监控的 Socket(创建 S ocket，设置 addr、reuse 等选项，绑定到指定的 ip 地址端口，再 listen 监听)。然后，再 fork(一个现有进程可以调用 fork 函数创建一个新进程。由 fork 创建的新进程被称为子进程 )出多个子进程出来。之后，子进程会竞争 accept 新的连接。此时，客户端就可以向 nginx 发起连接了。当客户端与 nginx 进行三次握手，与 nginx 建立好一个连接后。此时，某一个子进程会 accept 成功，得到这个建立好的连接的 Socket ，然后创建nginx 对连接的封装，即 ngx
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: _connection_t 结构体。接着，设置读写事件处理函数，并添加读写事件来与客户端进行数据的交换。最后，Nginx 或客户端来主动关掉连接，到此，一个连接就寿终正寝了。Nginx 是如何实现高并发的？如果一个 server 采用一个进程(或者线程)负责一个 request 的方式，那么进程数就是并发数。那么显而易见的，就是会有很多进程在等待中。等什么？最多的应该是等待网络传输。而 Nginx 的异步非阻塞工作方式正是利用了这点等待的时间。在需要等待的时候，这些进程就空闲出来待命了。因此表现为少数几个进程就解决了大量的并发问题。每进来一个 request ，会有一个 worker 进程去处理。但不是全程的处理，处理到什么程度呢？处理到可能发生阻塞的地方，比如向上游（后端）服务器转发 request ，并等待请求返回。那么，这个处理的 worker 不会这么傻等着，他会在发送完请求后，注册一个事件：“如果 upstream 返回了，告诉我一声，我再接着干”。于是他就休息去了。此时，如果再有 request 进来，他就可以很快再按这种方式处理。而一旦上游服务器返回了，就会触发这个事件，worker 才会来接手
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，这个 request 才会接着往下走。这就是为什么说，Nginx 基于事件模型。由于 web server 的工作性质决定了每个 request 的大部份生命都是在网络传输中，实际上花费在 server 机器上的时间片不多。这是几个进程就解决高并发的秘密所在。即：webserver 刚好属于网络 IO 密集型应用，不算是计算密集型。异步，非阻塞，使用 epoll ，和大量细节处的优化。也正是 Nginx 之所以然的技术基石。什么是正向代理？一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端才能使用正向代理。正向代理总结就一句话：代理端代理的是客户端。例如说：我们使用的 OpenVPN 等等。什么是反向代理？反向代理（Reverse Proxy）方式，是指以代理服务器来接受 Internet 上的连接请求，然后将请求，发给内部网络上的服务器并将从服务器上得到的结果返回给 Internet 上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 反向代理总结就一句话：代理端代理的是服务端。反向代理服务器的优点是什么?反向代理服务器可以隐藏源服务器的存在和特征。它充当互联网云和 web 服务器之间的中间层。这对于安全方面来说是很好的，特别是当您使用 web 托管服务时。cookie 和 session 区别？共同：存放用户信息。存放的形式：key-value 格式 变量和变量内容键值对。区别：cookie存放在客户端浏览器每个域名对应一个 cookie，不能跨跃域名访问其他 cookie用户可以查看或修改 cookiehttp 响应报文里面给你浏览器设置钥匙（用于打开浏览器上锁头）session:存放在服务器（文件，数据库，redis）存放敏感信息锁头为什么 Nginx 不使用多线程？Apache: 创建多个进程或线程，而每个进程或线程都会为其分配 cpu 和内存（线程要比进程小的多，所以 worker 支持比 perfork 高的并发），并发过大会榨干服务器资源。Nginx: 采用单线程来异步非阻塞处理请求（管理员可以配置 Nginx 主进程的工作进程的数量）(epoll)，不会为每个请求分配 cpu 和内存资源，节省了大量资源，同时也减少了大量的 
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: CPU 的上下文切换。所以才使得 Nginx 支持更高的并发。nginx 和 apache 的区别轻量级，同样起 web 服务，比 apache 占用更少的内存和资源。抗并发，nginx 处理请求是异步非阻塞的，而 apache 则是阻塞性的，在高并发下 nginx 能保持低资源，低消耗高性能。高度模块化的设计，编写模块相对简单。最核心的区别在于 apache 是同步多进程模型，一个连接对应一个进程，nginx是异步的，多个连接可以对应一个进程什么是动态资源、静态资源分离？动态资源、静态资源分离，是让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后我们就可以根据静态资源的特点将其做缓存操作，这就是网站静态化处理的核心思路。动态资源、静态资源分离简单的概括是：动态文件与静态文件的分离。为什么要做动、静分离？在我们的软件开发中，有些请求是需要后台处理的（如：.jsp,.do 等等），有些请求是不需要经过后台处理的（如：css、html、jpg、js 等等文件），这些不需要经过后台处理的文件称为静态文件，否则动态文件。因此我们后台处理忽略静态文件。这会有人又说那我后台忽略静
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 态文件不就完了吗？当然这是可以的，但是这样后台的请求次数就明显增多了。在我们对资源的响应速度有要求的时候，我们应该使用这种动静分离的策略去解决动、静分离将网站静态资源（HTML，JavaScript，CSS，img 等文件）与后台应用分开部署，提高用户访问静态代码的速度，降低对后台应用访问这里我们将静态资源放到 Nginx 中，动态资源转发到 Tomcat 服务器中去。当然，因为现在七牛、阿里云等 CDN 服务已经很成熟，主流的做法，是把静态资源缓存到 CDN 服务中，从而提升访问速度。相比本地的 Nginx 来说，CDN 服务器由于在国内有更多的节点，可以实现用户的就近访问。并且，CDN 服务可以提供更大的带宽，不像我们自己的应用服务，提供的带宽是有限的。什么叫 CDN 服务？CDN ，即内容分发网络。其目的是，通过在现有的 Internet 中 增加一层新的网络架构，将网站的内容发布到最接近用户的网络边缘，使用户可就近取得所需的内容，提高用户访问网站的速度。一般来说，因为现在 CDN 服务比较大众，所以基本所有公司都会使用 CDN 服务Nginx 怎么做的动静分离？只需要指定路径对应的目录。locatio
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: n/可以使用正则表达式匹配。并指定对应的硬盘中的目录Nginx 负载均衡的算法怎么实现的?策略有哪些?为了避免服务器崩溃，大家会通过负载均衡的方式来分担服务器压力。将对台服务器组成一个集群，当用户访问时，先访问到一个转发服务器，再由转发服务器将访问分发到压力更小的服务器。Nginx 负载均衡实现的策略有以下种：1 .轮询(默认)每个请求按时间顺序逐一分配到不同的后端服务器，如果后端某个服务器宕机，能自动剔除故障系统。upstream backserver {server 192.168.0.12;server 192.168.0.13;}2. 权重 weightweight 的值越大，分配到的访问概率越高，主要用于后端每台服务器性能不均衡的情况下。其次是为在主从的情况下设置不同的权值，达到合理有效的地利用主机资源。# 权重越高，在被访问的概率越大，如上例，分别是 20%，80%。upstream backserver {server 192.168.0.12 weight=2;server 192.168.0.13 weight=8;}3. ip_hash( IP 绑定)每个请求按访问 IP 的哈希结果分配，
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 使来自同一个 IP 的访客固定访问一台后端服务器，并且可以有效解决动态网页存在的 session 共享问题upstream backserver {ip_hash;server 192.168.0.12:88;server 192.168.0.13:80;}Nginx 虚拟主机怎么配置?1、基于域名的虚拟主机，通过域名来区分虚拟主机——应用：外部网站2、基于端口的虚拟主机，通过端口来区分虚拟主机——应用：公司内部网站，外部网站的管理后台3、基于 ip 的虚拟主机。location 的作用是什么？location 指令的作用是根据用户请求的 URI 来执行不同的应用，也就是根据用户请求的网站 URL 进行匹配，匹配成功即进行相关的操作限流怎么做的？Nginx 限流就是限制用户请求速度，防止服务器受不了限流有 3种正常限制访问频率（正常流量）突发限制访问频率（突发流量）限制并发连接数Nginx 的限流都是基于漏桶流算法漏桶流算法和令牌桶算法知道？漏桶算法漏桶算法思路很简单，我们把水比作是请求，漏桶比作是系统处理能力极限，水先进入到漏桶里，漏桶里的水按一定速率流出，当流出的速率小于流入的速率时，由于漏桶容量有限，后
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 续进入的水直接溢出（拒绝请求），以此实现限流。令牌桶算法令牌桶算法的原理也比较简单，我们可以理解成医院的挂号看病，只有拿到号以后才可以进行诊病。系统会维护一个令牌（token）桶，以一个恒定的速度往桶里放入令牌（token），这时如果有请求进来想要被处理，则需要先从桶里获取一个令牌（token），当桶里没有令牌（token）可取时，则该请求将被拒绝服务。令牌桶算法通过控制桶的容量、发放令牌的速率，来达到对请求的限制。Nginx 配置高可用性怎么配置？当上游服务器(真实访问服务器)，一旦出现故障或者是没有及时相应的话，应该直接轮训到下一台服务器，保证服务器的高可用生产中如何设置 worker 进程的数量呢？在有多个 cpu 的情况下，可以设置多个 worker，worker 进程的数量可以设置到和 cpu 的核心数一样多，如果在单个 cpu 上起多个 worker 进程，那么操作系统会在多个 worker 之间进行调度，这种情况会降低系统性能，如果只有一个 cpu，那么只启动一个 worker 进程就可以了。Java 基础八股文Java 语言具有哪些特点？Java 为纯面向对象的语言。它能够直接反应现实生活中的
2025-08-11 10:56:08.173 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象。具有平台无关性。Java 利用 Java 虚拟机运行字节码，无论是在 Windows、Linux还是 MacOS 等其它平台对 Java 程序进行编译，编译后的程序可在其它平台运行。Java 为解释型语言，编译器把 Java 代码编译成平台无关的中间代码，然后在JVM 上解释运行，具有很好的可移植性。Java 提供了很多内置类库。如对多线程支持，对网络通信支持，最重要的一点是提供了垃圾回收器。Java 具有较好的安全性和健壮性。Java 提供了异常处理和垃圾回收机制，去除了 C++中难以理解的指针特性JDK 与 JRE 有什么区别？JDK：Java 开发工具包（Java Development Kit），提供了 Java 的开发环境和运行环境。JRE：Java 运行环境(Java Runtime Environment)，提供了 Java 运行所需的环境。JDK 包含了 JRE。如果只运行 Java 程序，安装 JRE 即可。要编写 Java 程序需安装 JDK简述 Java 基本数据类型byte: 占用 1 个字节，取值范围-128 ~ 127short: 占用 2 个字节，取值范围-215 ~ 21
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 5-1int：占用 4 个字节，取值范围-231 ~ 231-1long：占用 8 个字节float：占用 4 个字节double：占用 8 个字节char: 占用 2 个字节boolean：占用大小根据实现虚拟机不同有所差异简述自动装箱拆箱对于 Java 基本数据类型，均对应一个包装类。装箱就是自动将基本数据类型转换为包装器类型，如 int->Integer拆箱就是自动将包装器类型转换为基本数据类型，如 Integer->int简述 Java 访问修饰符default: 默认访问修饰符，在同一包内可见private: 在同一类内可见，不能修饰类protected : 对同一包内的类和所有子类可见，不能修饰类public: 对所有类可见构造方法、成员变量初始化以及静态成员变量三者的初始化顺序？先后顺序：静态成员变量、成员变量、构造方法。详细的先后顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数。面向对象的三大特性？继承：对象的一个新类可以从现有的类中派生，派生类可以从它的基类那继承方法和实例变量，且
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 派生类可以修改或新增新的方法使之更适合特殊的需求。封装：将客观事物抽象成类，每个类可以把自身数据和方法只让可信的类或对象操作，对不可信的进行信息隐藏。多态：允许不同类的对象对同一消息作出响应。不同对象调用相同方法即使参数也相同，最终表现行为是不一样的为什么 Java 语言不支持多重继承？为了程序的结构能够更加清晰从而便于维护。假设 Java 语言支持多重继承，类C 继承自类 A 和类 B，如果类 A 和 B 都有自定义的成员方法 f()，那么当代码中调用类 C 的 f() 会产生二义性。Java 语言通过实现多个接口间接支持多重继承，接口由于只包含方法定义，不能有方法的实现，类 C 继承接口 A 与接口 B 时即使它们都有方法 f()，也不能直接调用方法，需实现具体的 f()方法才能调用，不会产生二义性。多重继承会使类型转换、构造方法的调用顺序变得复杂，会影响到性能Java 提供的多态机制？Java 提供了两种用于多态的机制，分别是重载与覆盖(重写)。重载：重载是指同一个类中有多个同名的方法，但这些方法有不同的参数，在编译期间就可以确定调用哪个方法。覆盖：覆盖是指派生类重写基类的方法，使用基类指向其子类的实例
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 对象，或接口的引用变量指向其实现类的实例对象，在程序调用的运行期根据引用变量所指的具体实例对象调用正在运行的那个对象的方法，即需要到运行期才能确定调用哪个方法重载与覆盖的区别？覆盖是父类与子类之间的关系，是垂直关系；重载是同一类中方法之间的关系，是水平关系。覆盖只能由一个方法或一对方法产生关系；重载是多个方法之间的关系。覆盖要求参数列表相同；重载要求参数列表不同。覆盖中，调用方法体是根据对象的类型来决定的，而重载是根据调用时实参表与形参表来对应选择方法体。重载方法可以改变返回值的类型，覆盖方法不能改变返回值的类型。接口和抽象类的相同点和不同点？相同点:都不能被实例化。接口的实现类或抽象类的子类需实现接口或抽象类中相应的方法才能被实例化。不同点：接口只能有方法定义，不能有方法的实现，而抽象类可以有方法的定义与实现。实现接口的关键字为 implements，继承抽象类的关键字为 extends。一个类可以实现多个接口，只能继承一个抽象类。当子类和父类之间存在逻辑上的层次结构，推荐使用抽象类，有利于功能的累积。当功能不需要，希望支持差别较大的两个或更多对象间的特定交互行为，推荐使用接口。使用接口能降低软件系统的耦合
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 度，便于日后维护或添加删除方法Java 语言中关键字 static 的作用是什么？static 的主要作用有两个：为某种特定数据类型或对象分配与创建对象个数无关的单一的存储空间。使得某个方法或属性与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性。具体而言 static 又可分为 4 种使用方式：修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用。修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法。修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次。修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方法为什么要把 String 设计为不可变？节省空间：字符串常量存储在 JVM 的字符串池中可以被用户共享。提高效率：String 可以被不同线程共享，是线程安全的。在涉及多线程操作中不需要同步操作。安全：String 常被用于用户名、密码、文件名等使用，由于其不可变，可避免黑客行为对其恶意修改简述 String/StringBuffer 与 StringBuilderString 类采用利用 final 修饰的字符数组进行字符串保存，因此不可变。如果对 String 类型对象修改，需要新建对象，将老字符和新增加的字符一并存进去。StringBuilder，采用无 final 修饰的字符数组进行保存，因此可变。但线程不安全。StringBuffer，采用无 final 修饰的字符数组进行保存，可理解为实现线程安全的 StringBuilder。判等运算符==与 equals 的区别？== 比较的是引用，equals 比较的是内容。如果变量是基础数据类型，== 用于比较其对应值是否相等。如果变量指向的是对象，== 用于比较两个对象是否指向同一块存储空间。equals 是 Object 类提供的方法之一，每个 J
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 类都继承自 Object 类，所以每个对象都具有 equals 这个方法。Object 类中定义的 equals 方法内部是直接调用 == 比较对象的。但通过覆盖的方法可以让它不是比较引用而是比较数据内容简述 Java 异常的分类Java 异常分为 Error（程序无法处理的错误），和 Exception（程序本身可以处理的异常）。这两个类均继承 Throwable。Error 常见的有 StackOverFlowError、OutOfMemoryError 等等。Exception 可分为运行时异常和非运行时异常。对于运行时异常，可以利用 trycatch 的方式进行处理，也可以不处理。对于非运行时异常，必须处理，不处理的话程序无法通过编译final、finally 和 finalize 的区别是什么？final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承。finally 作为异常处理的一部分，只能在 try/catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下。finalize 是 Object 类的一个方法
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存简述 Java 中 Class 对象java 中对象可以分为实例对象和 Class 对象，每一个类都有一个 Class 对象，其包含了与该类有关的信息。获取 Class 对象的方法：Class.forName(“类的全限定名”)实例对象.getClass()类名.classJava 反射机制是什么？Java 反射机制是指在程序的运行过程中可以构造任意一个类的对象、获取任意一个类的成员变量和成员方法、获取任意一个对象所属的类信息、调用任意一个对象的属性和方法。反射机制使得 Java 具有动态获取程序信息和动态调用对象方法的能力。可以通过以下类调用反射 API。简述 Java 序列化与反序列化的实现序列化：将 java 对象转化为字节序列，由此可以通过网络对象进行传输。反序列化：将字节序列转化为 java 对象。具体实现：实现 Serializable 接口，或实现 Externalizable 接口中的writeExte
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: rnal()与 readExternal()方法。简述 Java 的 ListList 是一个有序队列，在 Java 中有两种实现方式:ArrayList 使用数组实现，是容量可变的非线程安全列表，随机访问快，集合扩容时会创建更大的数组，把原有数组复制到新数组。LinkedList 本质是双向链表，与 ArrayList 相比插入和删除速度更快，但随机访问元素很慢。Java 中线程安全的基本数据结构有哪些HashTable: 哈希表的线程安全版，效率低ConcurrentHashMap：哈希表的线程安全版，效率高，用于替代 HashTableVector：线程安全版 ArraylistStack：线程安全版栈BlockingQueue 及其子类：线程安全版队列简述 Java 的 SetSet 即集合，该数据结构不允许元素重复且无序。Java 对 Set 有三种实现方式：HashSet 通过 HashMap 实现，HashMap 的 Key 即 HashSet 存储的元素，Value系统自定义一个名为 PRESENT 的 Object 类型常量。判断元素是否相同时，先比较 hashCode，相同后再利用 equ
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: als 比较，查询 O(1)LinkedHashSet 继承自 HashSet，通过 LinkedHashMap 实现，使用双向链表维护元素插入顺序。TreeSet 通过 TreeMap 实现的，底层数据结构是红黑树，添加元素到集合时按照比较规则将其插入合适的位置，保证插入后的集合仍然有序。查询 O(logn)简述 Java 的 HashMapJDK8 之前底层实现是数组 + 链表，JDK8 改为数组 + 链表/红黑树。主要成员变量包括存储数据的 table 数组、元素数量 size、加载因子 loadFactor。HashMap 中数据以键值对的形式存在，键对应的 hash 值用来计算数组下标，如果两个元素 key 的 hash 值一样，就会发生哈希冲突，被放到同一个链表上。table 数组记录 HashMap 的数据，每个下标对应一条链表，所有哈希冲突的数据都会被存放到同一条链表，Node/Entry 节点包含四个成员变量：key、value、next 指针和 hash 值。在 JDK8 后链表超过 8 会转化为红黑树为何 HashMap 线程不安全在 JDK1.7 中，HashMap 采用头插法插入元素
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，因此并发情况下会导致环形链表，产生死循环。虽然 JDK1.8 采用了尾插法解决了这个问题，但是并发下的 put 操作也会使前一个 key 被后一个 key 覆盖。由于 HashMap 有扩容机制存在，也存在 A 线程进行扩容后，B 线程执行 get 方法出现失误的情况。简述 Java 的 TreeMapTreeMap 是底层利用红黑树实现的 Map 结构，底层实现是一棵平衡的排序二叉树，由于红黑树的插入、删除、遍历时间复杂度都为 O(logN)，所以性能上低于哈希表。但是哈希表无法提供键值对的有序输出，红黑树可以按照键的值的大小有序输出ArrayList、Vector 和 LinkedList 有什么共同点与区别？ArrayList、Vector 和 LinkedList 都是可伸缩的数组，即可以动态改变长度的数组。ArrayList 和 Vector 都是基于存储元素的 Object[] array 来实现的，它们会在内存中开辟一块连续的空间来存储，支持下标、索引访问。但在涉及插入元素时可能需要移动容器中的元素，插入效率较低。当存储元素超过容器的初始化容量大小，ArrayList 与 Vector 均会进
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 行扩容。Vector 是线程安全的，其大部分方法是直接或间接同步的。ArrayList 不是线程安全的，其方法不具有同步性质。LinkedList 也不是线程安全的。LinkedList 采用双向列表实现，对数据索引需要从头开始遍历，因此随机访问效率较低，但在插入元素的时候不需要对数据进行移动，插入效率较高HashMap 和 Hashtable 有什么区别？HashMap 是 Hashtable 的轻量级实现，HashMap 允许 key 和 value 为 null，但最多允许一条记录的 key 为 null.而 HashTable 不允许。HashTable 中的方法是线程安全的，而 HashMap 不是。在多线程访问 HashMap需要提供额外的同步机制。Hashtable 使用 Enumeration 进行遍历，HashMap 使用 Iterator 进行遍历。如何决定使用 HashMap 还是 TreeMap?如果对 Map 进行插入、删除或定位一个元素的操作更频繁，HashMap 是更好的选择。如果需要对 key 集合进行有序的遍历，TreeMap 是更好的选择Collection 和 Colle
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ctions 有什么区别？Collection 是一个集合接口，它提供了对集合对象进行基本操作的通用接口方法，所有集合都是它的子类，比如 List、Set 等。Collections 是一个包装类，包含了很多静态方法、不能被实例化，而是作为工具类使用，比如提供的排序方法：Collections.sort(list);提供的反转方法：Collections.reverse(list)。Java 并发编程1.并行跟并发有什么区别？并行是多核 CPU 上的多任务处理，多个任务在同一时间真正地同时执行。并发是单核 CPU 上的多任务处理，多个任务在同一时间段内交替执行，通过时间片轮转实现交替执行，用于解决 IO 密集型任务的瓶颈。你是如何理解线程安全的？如果一段代码块或者一个方法被多个线程同时执行，还能够正确地处理共享数据，那么这段代码块或者这个方法就是线程安全的。可以从三个要素来确保线程安全：1 、原子性：一个操作要么完全执行，要么完全不执行，不会出现中间状态2 、可见性：当一个线程修改了共享变量，其他线程能够立即看到变化。有序性：要确保线程不会因为死锁、饥饿、活锁等问题导致无法继续执行2.说说进程和线程的区别？进
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 程说简单点就是我们在电脑上启动的一个个应用。它是操作系统分配资源的最小单位。线程是进程中的独立执行单元。多个线程可以共享同一个进程的资源，如内存；每个线程都有自己独立的栈和寄存器。线程间是如何进行通信的？原则上可以通过消息传递和共享内存两种方法来实现。Java 采用的是共享内存的并发模型。这个模型被称为 Java 内存模型，简写为 JMM，它决定了一个线程对共享变量的写入，何时对另外一个线程可见。当然了，本地内存是 JMM 的一个抽象概念，并不真实存在。用一句话来概括就是：共享变量存储在主内存中，每个线程的私有本地内存，存储的是这个共享变量的副本。线程 A 与线程 B 之间如要通信，需要要经历 2 个步骤：线程 A 把本地内存 A 中的共享变量副本刷新到主内存中。线程 B 到主内存中读取线程 A 刷新过的共享变量，再同步到自己的共享变量副本中3. 说说线程有几种创建方式？分别是继承 Thread 类、实现 Runnable 接口、实现 Callable 接口启动一个 Java 程序，你能说说里面有哪些线程吗？首先是 main 线程，这是程序执行的入口。然后是垃圾回收线程，它是一个后台线程，负责回收不再使用的对
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象。还有编译器线程，比如 JIT，负责把一部分热点代码编译后放到 codeCache 中。调用 start 方法时会执行 run 方法，那怎么不直接调用 run 方法？调用 start() 会创建一个新的线程，并异步执行 run() 方法中的代码。直接调用 run() 方法只是一个普通的同步方法调用，所有代码都在当前线程中执行，不会创建新线程。没有新的线程创建，也就达不到多线程并发的目的。线程有哪些常用的调度方法？比如说 start 方法用于启动线程并让操作系统调度执行；sleep 方法用于让当前线程休眠一段时间；wait 方法会让当前线程等待，notify 会唤醒一个等待的线程。说说 wait 方法和 notify 方法？当线程 A 调用共享对象的 wait() 方法时，线程 A 会被阻塞挂起，直到：线程 B 调用了共享对象的 notify() 方法或者 notifyAll() 方法、当线程 A 调用共享对象的 notify() 方法后，会唤醒一个在这个共享对象上调用 wait 系列方法被挂起的线程。共享对象上可能会有多个线程在等待，具体唤醒哪个线程是随机的。如果调用的是 notifyAll 方法，会唤醒所
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 有在这个共享变量上调用 wait 系列方法而被挂起的线程。说说 sleep 方法？当线程 A 调用了 Thread 的 sleep 方法后，线程 A 会暂时让出指定时间的执行权。指定的睡眠时间到了后该方法会正常返回，接着参与 CPU 调度，获取到 CPU 资源后可以继续执行。6.线程有几种状态？6 种。new 代表线程被创建但未启动；runnable 代表线程处于就绪或正在运行状态，由操作系统调度；blocked 代表线程被阻塞，等待获取锁；waiting 代表线程等待其他线程的通知或中断；timed_waiting 代表线程会等待一段时间，超时后自动恢复；terminated 代表线程执行完毕，生命周期结束。什么是线程上下文切换？线程上下文切换是指 CPU 从一个线程切换到另一个线程执行时的过程。在线程切换的过程中，CPU 需要保存当前线程的执行状态，并加载下一个线程的上下文。之所以要这样，是因为 CPU 在同一时刻只能执行一个线程，为了实现多线程并发执行，需要不断地在多个线程之间切换。为了让用户感觉多个线程是在同时执行的， CPU 资源的分配采用了时间片轮转的方式，线程在时间片内占用 CPU 执行任务。当
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程使用完时间片后，就会让出 CPU 让其他线程占用。守护线程了解吗？了解，守护线程是一种特殊的线程，它的作用是为其他线程提供服务。Java 中的线程分为两类，一种是守护线程，另外一种是用户线程。JVM 启动时会调用 main 方法，main 方法所在的线程就是一个用户线程。在 JVM内部，同时还启动了很多守护线程，比如垃圾回收线程。守护线程和用户线程有什么区别呢？区别之一是当最后一个非守护线程束时， JVM 会正常退出，不管当前是否存在守护线程，也就是说守护线程是否结束并不影响 JVM 退出。换而言之，只要有一个用户线程还没结束，正常情况下 JVM 就不会退出。请说说 sleep 和 wait 的区别？（补充）sleep 会让当前线程休眠，不需要获取对象锁，属于 Thread 类的方法；wait 会让获得对象锁的线程等待，要提前获得对象锁，属于 Object 类的方法。sleep() 方法专属于 Thread 类。wait() 方法专属于 Object 类。waitingThread 必须等待 sleepingThread 完成睡眠后才能进入同步代码块。而当线程执行 wait 方法时，它会释放持有的对象锁，
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 因此其他线程也有机会获取该对象的锁。有个 int 的变量为 0，十个线程轮流对其进行++操作（循环 10000 次），结果大于 10 万还是小于等于 10 万，为什么？在这个场景中，最终的结果会小于 100000，原因是多线程环境下，++ 操作并不是一个原子操作，而是分为读取、加 1、写回三个步骤。读取变量的值。将读取到的值加 1。将结果写回变量。这样的话，就会有多个线程读取到相同的值，然后对这个值进行加 1 操作，最终导致结果小于 100000。详细解释下。多个线程在并发执行 ++ 操作时，可能出现以下竞态条件：线程 1 读取变量值为 0。线程 2 也读取变量值为 0。线程 1 进行加法运算并将结果 1 写回变量。线程 2 进行加法运算并将结果 1 写回变量，覆盖了线程 1 的结果。能说一下 Hashtable 的底层数据结构吗？与 HashMap 类似，Hashtable 的底层数据结构也是一个数组加上链表的方式，然后通过 synchronized 加锁来保证线程安全。.ThreadLocal 是什么？ThreadLocal 是一种用于实现线程局部变量的工具类。它允许每个线程都拥有自己的独立副本，从而实现
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 线程隔离。在 Web 应用中，可以使用 ThreadLocal 存储用户会话信息，这样每个线程在处理用户请求时都能方便地访问当前用户的会话信息。在数据库操作中，可以使用 ThreadLocal 存储数据库连接对象，每个线程有自己独立的数据库连接，从而避免了多线程竞争同一数据库连接的问题。在格式化操作中，例如日期格式化，可以使用 ThreadLocal 存储SimpleDateFormat 实例，避免多线程共享同一实例导致的线程安全问题ThreadLocal 有哪些优点？每个线程访问的变量副本都是独立的，避免了共享变量引起的线程安全问题。由于 ThreadLocal 实现了变量的线程独占，使得变量不需要同步处理，因此能够避免资源竞争ThreadLocal 可用于跨方法、跨类时传递上下文数据，不需要在方法间传递参数。ThreadLocal 怎么实现的呢？当我们创建一个 ThreadLocal 对象并调用 set 方法时，其实是在当前线程中初始化了一个 ThreadLocalMap。ThreadLocalMap 是 ThreadLocal 的一个静态内部类，它内部维护了一个 Entry数组，key 是 Thread
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Local 对象，value 是线程的局部变量，这样就相当于为每个线程维护了一个变量副本。Entry 继承了 WeakReference，它限定了 key 是一个弱引用，弱引用的好处是当内存不足时，JVM 会回收 ThreadLocal 对象，并且将其对应的 Entry.value设置为 null，这样可以在很大程度上避免内存泄漏。ThreadLocal 的实现原理是，每个线程维护一个 Map，key 为 ThreadLocal 对象，value 为想要实现线程隔离的对象。1、通过 ThreadLocal 的 set 方法将对象存入 Map 中。2、通过 ThreadLocal 的 get 方法从 Map 中取出对象。3、Map 的大小由 ThreadLocal 对象的多少决定。15.ThreadLocal 内存泄露是怎么回事？ThreadLocalMap 的 Key 是 弱引用，但 Value 是强引用。如果一个线程一直在运行，并且 value 一直指向某个强引用对象，那么这个对象就不会被回收，从而导致内存泄漏。那怎么解决内存泄漏问题呢？很简单，使用完 ThreadLocal 后，及时调用 remove()
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法释放内存空间。那为什么 key 要设计成弱引用？弱引用的好处是，当内存不足的时候，JVM 能够及时回收掉弱引用的对象。ThreadLocalMap 的源码看过吗？有研究过。ThreadLocalMap 虽然被叫做 Map，但它并没有实现 Map 接口，是一个简单的线性探测哈希表底层的数据结构也是数组，数组中的每个元素是一个 Entry 对象，Entry 对象继承了 WeakReference，key 是 ThreadLocal 对象，value 是线程的局部变量。ThreadLocalMap 怎么解决 Hash 冲突的？开放定址法。如果计算得到的槽位 i 已经被占用，ThreadLocalMap 会采用开放地址法中的线性探测来寻找下一个空闲槽位：如果 i 位置被占用，尝试 i+1。如果 i+1 也被占用，继续探测 i+2，直到找到一个空位。如果到达数组末尾，则回到数组头部，继续寻找空位。为什么要用线性探测法而不是 HashMap 的拉链法来解决哈希冲突？ThreadLocalMap 设计的目的是存储线程私有数据，不会有大量的 Key，所以采用线性探测更节省空间。拉链法还需要单独维护一个链表，甚至红黑树，
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 不适合 ThreadLocal 这种场景ThreadLocalMap 扩容机制了解吗？了解。与 HashMap 不同，ThreadLocalMap 并不会直接在元素数量达到阈值时立即扩容，而是先清理被 GC 回收的 key，然后在填充率达到四分之三时进行扩容。父线程能用 ThreadLocal 给子线程传值吗？不能。因为 ThreadLocal 变量存储在每个线程的 ThreadLocalMap 中，而子线程不会继承父线程的 ThreadLocalMap。可以使用 InheritableThreadLocal 来解决这个问题。InheritableThreadLocal 的原理了解吗？了解。在 Thread 类的定义中，每个线程都有两个 ThreadLocalMap：普通 ThreadLocal 变量存储在 threadLocals 中，不会被子线程继承。InheritableThreadLocal 变量存储在 inheritableThreadLocals 中，当 newThread() 创建一个子线程时，Thread 的 init() 方法会检查父线程是否有inheritableThreadLocals，
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 如果有，就会拷贝 InheritableThreadLocal 变量到子线程：说一下你对 Java 内存模型的理解？Java 内存模型是 Java 虚拟机规范中定义的一个抽象模型，用来描述多线程环境中共享变量的内存可见性共享变量存储在主内存中，每个线程都有一个私有的本地内存，存储了共享变量的副本。当一个线程更改了本地内存中共享变量的副本，它需要 JVM 刷新到主内存中，以确保其他线程可以看到这些更改。当一个线程需要读取共享变量时，它一版会从本地内存中读取。如果本地内存中的副本是过时的，JVM 会将主内存中的共享变量最新值刷新到本地内存中。为什么线程要用自己的内存？线程从主内存拷贝变量到工作内存，可以减少 CPU 访问 RAM 的开销。每个线程都有自己的变量副本，可以避免多个线程同时修改共享变量导致的数据冲突volatile 了解吗？了解。第一，保证可见性，线程修改 volatile 变量后，其他线程能够立即看到最新值；第二，防止指令重排，volatile 变量的写入不会被重排序到它之前的代码。volatile 怎么保证可见性的？当线程对 volatile 变量进行写操作时，JVM 会在这个变量写入之后插入一个
2025-08-11 10:56:08.174 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 写屏障指令，这个指令会强制将本地内存中的变量值刷新到主内存中。当线程对 volatile 变量进行读操作时，JVM 会插入一个读屏障指令，这个指令会强制让本地内存中的变量值失效，从而重新从主内存中读取最新的值。volatile 怎么保证有序性的？JVM 会在 volatile 变量的读写前后插入 “内存屏障”，以约束 CPU 和编译器的优化行为：StoreStore 屏障可以禁止普通写操作与 volatile 写操作的重排StoreLoad 屏障会禁止 volatile 写与 volatile 读重排LoadLoad 屏障会禁止 volatile 读与后续普通读操作重排LoadStore 屏障会禁止 volatile 读与后续普通写操作重排volatile 和 synchronized 的区别？volatile 关键字用于修饰变量，确保该变量的更新操作对所有线程是可见的，即一旦某个线程修改了 volatile 变量，其他线程会立即看到最新的值。synchronized 关键字用于修饰方法或代码块，确保同一时刻只有一个线程能够执行该方法或代码块，从而实现互斥访问。锁synchronized 用过吗？用过，频率还
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 很高。synchronized 在 JDK 1.6 之后，进行了锁优化，增加了偏向锁、轻量级锁，大大提升了 synchronized 的性能synchronized 上锁的对象是什么？synchronized 用在普通方法上时，上锁的是执行这个方法的对象synchronized 用在静态方法上时，上锁的是这个类的 Class 对象。synchronized 用在代码块上时，上锁的是括号中指定的对象，比如说当前对象this。synchronized 的实现原理了解吗？synchronized 依赖 JVM 内部的 Monitor 对象来实现线程同步。使用的时候不用手动去 lock 和 unlock，JVM 会自动加锁和解锁。synchronized 加锁代码块时，JVM 会通过 monitorenter、monitorexit 两个指令来实现同步：前者表示线程正在尝试获取 lock 对象的 Monitor；后者表示线程执行完了同步代码块，正在释放锁。你对 Monitor 了解多少？Monitor 是 JVM 内置的同步机制，每个对象在内存中都有一个对象头——MarkWord，用于存储锁的状态，以及 Monito
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: r 对象的指针。synchronized 依赖对象头的 Mark Word 进行状态管理，支持无锁、偏向锁、轻量级锁，以及重量级锁。synchronized 怎么保证可见性？通过两步操作：加锁时，线程必须从主内存读取最新数据。释放锁时，线程必须将修改的数据刷回主内存，这样其他线程获取锁后，就能看到最新的数据synchronized 怎么保证有序性？synchronized 通过 JVM 指令 monitorenter 和 monitorexit，来确保加锁代码块内的指令不会被重排synchronized 怎么实现可重入的呢？可重入意味着同一个线程可以多次获得同一个锁，而不会被阻塞synchronized 之所以支持可重入，是因为 Java 的对象头包含了一个 Mark Word，用于存储对象的状态，包括锁信息。当一个线程获取对象锁时，JVM 会将该线程的 ID 写入 Mark Word，并将锁计数器设为 1。如果一个线程尝试再次获取已经持有的锁，JVM 会检查 Mark Word 中的线程ID。如果 ID 匹配，表示的是同一个线程，锁计数器递增。当线程退出同步块时，锁计数器递减。如果计数器值为零，JVM 将锁
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 标记为未持有状态，并清除线程 ID 信息。synchronized 锁升级了解吗？JDK 1.6 的时候，为了提升 synchronized 的性能，引入了锁升级机制，从低开销的锁逐步升级到高开销的锁，以最大程度减少锁的竞争。没有线程竞争时，就使用低开销的“偏向锁”，此时没有额外的 CAS 操作；轻度竞争时，使用“轻量级锁”，采用 CAS 自旋，避免线程阻塞；只有在重度竞争时，才使用“重量级锁”，由 Monitor 机制实现，需要线程阻塞。了解 synchronized 四种锁状态吗？了解。①、无锁状态，对象未被锁定，Mark Word 存储对象的哈希码等信息。②、偏向锁，当线程第一次获取锁时，会进入偏向模式。Mark Word 会记录线程 ID，后续同一线程再次获取锁时，可以直接进入 synchronized 加锁的代码，无需额外加锁③、轻量级锁，当多个线程在不同时段获取同一把锁，即不存在锁竞争的情况时，JVM 会采用轻量级锁来避免线程阻塞。未持有锁的线程通过 CAS 自旋等待锁释放④、重量级锁，如果自旋超过一定的次数，或者一个线程持有锁，一个自旋，又有第三个线程进入 synchronized 加锁的代码时
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，轻量级锁就会升级为重量级锁。此时，对象头的锁类型会更新为“10”，Mark Word 会存储指向 Monitor 对象的指针，其他等待锁的线程都会进入阻塞状态synchronized 做了哪些优化？在 JDK 1.6 之前，synchronized 是直接调用 ObjectMonitor 的 enter 和 exit 指令实现的，这种锁也被称为重量级锁，性能较差。随着 JDK 版本的更新，synchronized 的性能得到了极大的优化：①、偏向锁：同一个线程可以多次获取同一把锁，无需重复加锁。②、轻量级锁：当没有线程竞争时，通过 CAS 自旋等待锁，避免直接进入阻塞。③、锁消除：JIT 可以在运行时进行代码分析，如果发现某些锁操作不可能被多个线程同时访问，就会对这些锁进行消除，从而减少上锁开销详细解释一下：①、从无锁到偏向锁：当一个线程首次访问同步代码时，如果此对象处于无锁状态且偏向锁未被禁用，JVM 会将该对象头的锁标记改为偏向锁状态，并记录当前线程 ID。此时，对象头中的 Mark Word 中存储了持有偏向锁的线程 ID。如果另一个线程尝试获取这个已被偏向的锁，JVM 会检查当前持有偏向锁的线程是否
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 活跃。如果持有偏向锁的线程不活跃，可以将锁偏向给新的线程；否则撤销偏向锁，升级为轻量级锁。②、偏向锁的轻量级锁：进行偏向锁撤销时，会遍历堆栈的所有锁记录，暂停拥有偏向锁的线程，并检查锁对象。如果这个过程中发现有其他线程试图获取这个锁，JVM 会撤销偏向锁，并将锁升级为轻量级锁。当有两个或以上线程竞争同一个偏向锁时，偏向锁模式不再有效，此时偏向锁会被撤销，对象的锁状态会升级为轻量级锁。③、轻量级锁到重量级锁：轻量级锁通过自旋来等待锁释放。如果自旋超过预定次数（自旋次数是可调的，并且是自适应的，失败次数多自旋次数就少），表明锁竞争激烈。当自旋多次失败，或者有线程在等待队列中等待相同的轻量级锁时，轻量级锁会升级为重量级锁。在这种情况下，JVM 会在操作系统层面创建一个互斥锁——Mutex，所有进一步尝试获取该锁的线程将会被阻塞，直到锁被释放。30.synchronized 和 ReentrantLock 的区别了解吗？两句话回答：synchronized 由 JVM 内部的 Monitor 机制实现，ReentrantLock基于 AQS 实现。synchronized 可以自动加锁和解锁，ReentrantLoc
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: k 需要手动 lock() 和 unlock()。并发量大的情况下，使用 synchronized 还是 ReentrantLock？我更倾向于 ReentrantLock，因为：ReentrantLock 提供了超时和公平锁等特性，可以应对更复杂的并发场景。ReentrantLock 允许更细粒度的锁控制，能有效减少锁竞争。ReentrantLock 支持条件变量 Condition，可以实现比 synchronized 更友好的线程间通信机制AQS 了解多少？AQS 是一个抽象类，它维护了一个共享变量 state 和一个线程等待队列，为ReentrantLock 等类提供底层支持。AQS 的思想是，如果被请求的共享资源处于空闲状态，则当前线程成功获取锁；否则，将当前线程加入到等待队列中，当其他线程释放锁时，从等待队列中挑选一个线程，把锁分配给它。说说 ReentrantLock 的实现原理？ReentrantLock 是基于 AQS 实现的 可重入排他锁，使用 CAS 尝试获取锁，失败的话，会进入 CLH 阻塞队列，支持公平锁、非公平锁，可以中断、超时等待。内部通过一个计数器 state 来跟踪锁的状态和
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 持有次数。当线程调用 lock() 方法获取锁时，ReentrantLock 会检查 state 的值，如果为 0，通过 CAS 修改为 1，表示成功加锁。否则根据当前线程的公平性策略，加入到等待队列中。线程首次获取锁时，state 值设为 1；如果同一个线程再次获取锁时，state 加 1；每释放一次锁，state 减 1。当线程调用 unlock() 方法时，ReentrantLock 会将持有锁的 state 减 1，如果state = 0，则释放锁，并唤醒等待队列中的线程来竞争锁。非公平锁和公平锁有什么不同？两句话回答：公平锁意味着在多个线程竞争锁时，获取锁的顺序与线程请求锁的顺序相同，即先来先服务。非公平锁不保证线程获取锁的顺序，当锁被释放时，任何请求锁的线程都有机会获取锁，而不是按照请求的顺序CAS 了解多少？CAS 是一种乐观锁，用于比较一个变量的当前值是否等于预期值，如果相等，则更新值，否则重试。在 CAS 中，有三个值：V：要更新的变量(var)E：预期值(expected)N：新值(new)先判断 V 是否等于 E，如果等于，将 V 的值设置为 N；如果不等，说明已经有其它线程更新了 V，
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 当前线程就放弃更新。这个比较和替换的操作需要是原子的，不可中断的。Java 中的 CAS 是由 Unsafe类实现的。怎么保证 CAS 的原子性？CPU 会发出一个 LOCK 指令进行总线锁定，阻止其他处理器对内存地址进行操作，直到当前指令执行完成CAS 有什么问题？CAS 存在三个经典问题，ABA 问题、自旋开销大、只能操作一个变量等。什么是 ABA 问题？ABA 问题指的是，一个值原来是 A，后来被改为 B，再后来又被改回 A，这时 CAS会误认为这个值没有发生变化。可以使用版本号/时间戳的方式来解决 ABA 问题。比如说，每次变量更新时，不仅更新变量的值，还更新一个版本号。CAS 操作时，不仅比较变量的值，还比较版本号自旋开销大怎么解决？CAS 失败时会不断自旋重试，如果一直不成功，会给 CPU 带来非常大的执行开销。可以加一个自旋次数的限制，超过一定次数，就切换到 synchronized 挂起线程涉及到多个变量同时更新怎么办？可以将多个变量封装为一个对象，使用 AtomicReference 进行 CAS 更新Java 有哪些保证原子性的方法？比如说以 Atomic 开头的原子类，synchroni
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: zed 关键字，ReentrantLock 锁等原子操作类了解多少？原子操作类是基于 CAS + volatile 实现的，底层依赖于 Unsafe 类，最常用的有AtomicInteger、AtomicLong、AtomicReference 等。线程死锁了解吗？死锁发生在多个线程相互等待对方释放锁时。比如说线程 1 持有锁 R1，等待锁R2；线程 2 持有锁 R2，等待锁 R1。第一条件是互斥：资源不能被多个线程共享，一次只能由一个线程使用。如果一个线程已经占用了一个资源，其他请求该资源的线程必须等待，直到资源被释放。第二个条件是持有并等待：一个线程已经持有一个资源，并且在等待获取其他线程持有的资源。第三个条件是不可抢占：资源不能被强制从线程中夺走，必须等线程自己释放。第四个条件是循环等待：存在一种线程等待链，线程 A 等待线程 B 持有的资源，线程 B 等待线程 C 持有的资源，直到线程 N 又等待线程 A 持有的资源该如何避免死锁呢？第一，所有线程都按照固定的顺序来申请资源。例如，先申请 R1 再申请 R2。第二，如果线程发现无法获取某个资源，可以先释放已经持有的资源，重新尝试申请聊聊线程同步和互斥？
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: （补充）同步，意味着线程之间要密切合作，按照一定的顺序来执行任务。比如说，线程A 先执行，线程 B 再执行。互斥，意味着线程之间要抢占资源，同一时间只能有一个线程访问共享资源。比如说，线程 A 在访问共享资源时，线程 B 不能访问。同步关注的是线程之间的协作，互斥关注的是线程之间的竞争。如何实现同步和互斥？可以使用 synchronized 关键字或者 Lock 接口的实现类，如 ReentrantLock 来给资源加锁。锁在操作系统层面的意思是 Mutex，某个线程进入临界区后，也就是获取到锁后，其他线程不能再进入临界区，要阻塞等待持有锁的线程离开临界区说说自旋锁？自旋锁是指当线程尝试获取锁时，如果锁已经被占用，线程不会立即阻塞，而是通过自旋，也就是循环等待的方式不断尝试获取锁。 适用于锁持有时间短的场景，ReentrantLock 的 tryLock 方法就用到了自旋锁。自旋锁的优点是可以避免线程切换带来的开销，缺点是如果锁被占用时间过长，会导致线程空转，浪费CPU 资源。聊聊悲观锁和乐观锁？（补充）悲观锁认为每次访问共享资源时都会发生冲突，所在在操作前一定要先加锁，防止其他线程修改数据。乐观锁认为冲突不
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 会总是发生，所以在操作前不加锁，而是在更新数据时检查是否有其他线程修改了数据。如果发现数据被修改了，就会重试。乐观锁发现有线程过来修改数据，怎么办？可以重新读取数据，然后再尝试更新，直到成功为止或达到最大重试次数。CountDownLatch 了解吗？CountDownLatch 是 JUC 中的一个同步工具类，用于协调多个线程之间的同步，确保主线程在多个子线程完成任务后继续执行。它的核心思想是通过一个倒计时计数器来控制多个线程的执行顺序。场景题：假如要查 10万多条数据，用线程池分成 20 个线程去执行，怎么做到等所有的线程都查找完之后，即最后一条结果查找结束了，才输出结果？很简单，可以使用 CountDownLatch 来实现。CountDownLatch 非常适合这个场景。第一步，创建 CountDownLatch 对象，初始值设定为 20，表示 20 个线程需要完成任务。第二步，创建线程池，每个线程执行查询操作，查询完毕后调用 countDown() 方法，计数器减 1。第三步，主线程调用 await() 方法，等待所有线程执行完毕。CyclicBarrier 和 CountDownLatch 有什么
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区别？CyclicBarrier 让所有线程相互等待，全部到达后再继续；CountDownLatch 让主线程等待所有子线程执行完再继续。能说一下 ConcurrentHashMap 的实现吗？（补充）好的。ConcurrentHashMap 是 HashMap 的线程安全版本。JDK 7 采用的是分段锁，整个 Map 会被分为若干段，每个段都可以独立加锁。不同的线程可以同时操作不同的段，从而实现并发。JDK 8 使用了一种更加细粒度的锁——桶锁，再配合 CAS + synchronized 代码块控制并发写入，以最大程度减少锁的竞争。对于读操作，ConcurrentHashMap 使用了 volatile 变量来保证内存可见性。对于写操作，ConcurrentHashMap 优先使用 CAS 尝试插入，如果成功就直接返回；否则使用 synchronized 代码块进行加锁处理。说一下 JDK 8 中 ConcurrentHashMap 的实现原理？JDK 8 中的 ConcurrentHashMap 取消了分段锁，采用 CAS + synchronized 来实现更细粒度的桶锁，并且使用红黑树来优化链表以提
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 高哈希冲突时的查询效率，性能比 JDK 7 有了很大的提升。说一下 JDK 8 中 ConcurrentHashMap 的 put 流程？第一步，计算 key 的 hash，以确定桶在数组中的位置。如果数组为空，采用 CAS的方式初始化，以确保只有一个线程在初始化数组。第二步，如果桶为空，直接 CAS 插入节点。如果 CAS 操作失败，会退化为synchronized 代码块来插入节点。插入的过程中会判断桶的哈希是否小于 0（f.hash >= 0），小于 0 说明是红黑树，大于等于 0 说明是链表。这里补充一点：在 ConcurrentHashMap 的实现中，红黑树节点 TreeBin 的 hash值固定为 -2。第三步，如果链表长度超过 8，转换为红黑树。第四步，在插入新节点后，会调用 addCount() 方法检查是否需要扩容。为什么 ConcurrentHashMap 在 JDK 1.7 中要用 ReentrantLock，而在 JDK 1.8 要用 synchronizedJDK 1.7 中的 ConcurrentHashMap 使用了分段锁机制，每个 Segment 都继承了ReentrantL
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ock，这样可以保证每个 Segment 都可以独立地加锁。而在 JDK 1.8 中，ConcurrentHashMap 取消了 Segment 分段锁，采用了更加精细化的锁——桶锁，以及 CAS 无锁算法，每个桶都可以独立地加锁，只有在 CAS失败时才会使用 synchronized 代码块加锁，这样可以减少锁的竞争，提高并发性能ConcurrentHashMap 怎么保证可见性？（补充）ConcurrentHashMap 中的 Node 节点中，value 和 next 都是 volatile 的，这样就可以保证对 value 或 next 的更新会被其他线程立即看到。为什么 ConcurrentHashMap 比 Hashtable 效率高（补充）Hashtable 在任何时刻只允许一个线程访问整个 Map，是通过对整个 Map 加锁来实现线程安全的。比如 get 和 put 方法，是直接在方法上加的 synchronized关键字。而 ConcurrentHashMap 在 JDK 8 中是采用 CAS + synchronized 实现的，仅在必要时加锁。比如说 put 的时候优先使用 CAS 尝试
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 插入，如果失败再使用 synchronized 代码块加锁。get 的时候是完全无锁的，因为 value 是 volatile 变量 修饰的，保证了内存可见性。能说一下 CopyOnWriteArrayList 的实现原理吗？（补充）CopyOnWriteArrayList 是 ArrayList 的线程安全版本，适用于读多写少的场景。它的核心思想是写操作时创建一个新数组，修改后再替换原数组，这样就能够确保读操作无锁，从而提高并发性能。缺点就是写操作的时候会复制一个新数组，如果数组很大，写操作的性能会受到影响什么是线程池？线程池是用来管理和复用线程的工具，它可以减少线程的创建和销毁开销。在 Java 中，ThreadPoolExecutor 是线程池的核心实现，它通过核心线程数、最大线程数、任务队列和拒绝策略来控制线程的创建和执行。说一下线程池的工作流程？可以简单总结为：任务提交 → 核心线程执行 → 任务队列缓存 → 非核心线程执行 → 拒绝策略处理。第一步，线程池通过 submit() 提交任务。第二步，线程池会先创建核心线程来执行任务。第三步，如果核心线程都在忙，任务会被放入任务队列中。第四步，如果任务
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 队列已满，且当前线程数量小于最大线程数，线程池会创建新的线程来处理任务。第五步，如果线程池中的线程数量已经达到最大线程数，且任务队列已满，线程池会执行拒绝策略。另外一版回答。第一步，创建线程池。第二步，调用线程池的 execute()方法，准备执行任务。如果正在运行的线程数量小于 corePoolSize，那么线程池会创建一个新的线程来执行这个任务；如果正在运行的线程数量大于或等于 corePoolSize，那么线程池会将这个任务放入等待队列；如果等待队列满了，而且正在运行的线程数量小于 maximumPoolSize，那么线程池会创建新的线程来执行这个任务；如果等待队列满了，而且正在运行的线程数量大于或等于 maximumPoolSize，那么线程池会执行拒绝策略。第三步，线程执行完毕后，线程并不会立即销毁，而是继续保持在池中等待下一个任务。第四步，当线程空闲时间超出指定时间，且当前线程数量大于核心线程数时，线程会被回收。线程池的主要参数有哪些？线程池有 7 个参数，需要重点关注的有核心线程数、最大线程数、等待队列、拒绝策略。①、corePoolSize：核心线程数，长期存活，执行任务的主力。②、maxim
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: umPoolSize：线程池允许的最大线程数。③、workQueue：任务队列，存储等待执行的任务。④、handler：拒绝策略，任务超载时的处理方式。也就是线程数达到maximumPoolSiz，任务队列也满了的时候，就会触发拒绝策略。⑤、threadFactory：线程工厂，用于创建线程，可自定义线程名。一句话：任务优先使用核心线程执行，满了进入等待队列，队列满了启用非核心线程备用，线程池达到最大线程数量后触发拒绝策略，非核心线程的空闲时间超过存活时间就被回收。线程池的拒绝策略有哪些？AbortPolicy：默认的拒绝策略，会抛 RejectedExecutionException 异常。CallerRunsPolicy：让提交任务的线程自己来执行这个任务，也就是调用 execute方法的线程。DiscardOldestPolicy：等待队列会丢弃队列中最老的一个任务，也就是队列中等待最久的任务，然后尝试重新提交被拒绝的任务。DiscardPolicy：丢弃被拒绝的任务，不做任何处理也不抛出异常。线程池有哪几种阻塞队列？常用的有五种，有界队列 ArrayBlockingQueue；无界队列 LinkedB
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: lockingQueue；优先级队列 PriorityBlockingQueue；延迟队列 DelayQueue；同步队列SynchronousQueue。1 、ArrayBlockingQueue：一个有界的先进先出的阻塞队列，底层是一个数组，适合固定大小的线程池。2 、LinkedBlockingQueue：底层是链表，如果不指定大小，默认大小是Integer.MAX_VALUE，几乎相当于一个无界队列。3 、PriorityBlockingQueue：一个支持优先级排序的无界阻塞队列。任务按照其自然顺序或 Comparator 来排序。适用于需要按照给定优先级处理任务的场景，比如优先处理紧急任务4 、DelayQueue：类似于 PriorityBlockingQueue，由二叉堆实现的无界优先级阻塞队列。5 、SynchronousQueue：每个插入操作必须等待另一个线程的移除操作，同样，任何一个移除操作都必须等待另一个线程的插入操作线程池提交 execute 和 submit 有什么区别？execute 方法没有返回值，适用于不关心结果和异常的简单任务。submit 有返回值，适用于需要获取结果或
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 处理异常的场景。线程池怎么关闭知道吗？可以调用线程池的 shutdown 或 shutdownNow方法来关闭线程池。shutdown 不会立即停止线程池，而是等待所有任务执行完毕后再关闭线程池。shutdownNow 会尝试通过一系列动作来停止线程池，包括停止接收外部提交的任务、忽略队列里等待的任务、尝试将正在跑的任务 interrupt 中断。线程池的线程数应该怎么配置？首先，我会分析线程池中执行的任务类型是 CPU 密集型还是 IO 密集型？1 、对于 CPU 密集型任务，我的目标是尽量减少线程上下文切换，以优化 CPU使用率。一般来说，核心线程数设置为处理器的核心数或核心数加一是较理想的选择。2 、对于 IO 密集型任务，由于线程经常处于等待状态，等待 IO 操作完成，所以可以设置更多的线程来提高并发，比如说 CPU 核心数的两倍。有哪几种常见的线程池？主要有四种：固定大小的线程池 Executors.newFixedThreadPool(int nThreads);，适合用于任务数量确定，且对线程数有明确要求的场景。例如，IO 密集型任务、数据库连接池等缓存线程池 Executors.newCach
2025-08-11 10:56:08.175 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: edThreadPool();，适用于短时间内任务量波动较大的场景。例如，短时间内有大量的文件处理任务或网络请求。定时任务线程池 Executors.newScheduledThreadPool(int corePoolSize);，适用于需要定时执行任务的场景。例如，定时发送邮件、定时备份数据等。单线程线程池 Executors.newSingleThreadExecutor();，适用于需要按顺序执行任务的场景。例如，日志记录、文件处理等。能说一下四种常见线程池的原理吗？说说固定大小线程池的原理？线程池大小是固定的，corePoolSize == maximumPoolSize，默认使用LinkedBlockingQueue 作为阻塞队列，适用于任务量稳定的场景，如数据库连接池、RPC 处理等。新任务提交时，如果线程池有空闲线程，直接执行；如果没有，任务进入 LinkedBlockingQueue 等待。缺点是任务队列默认无界，可能导致任务堆积，甚至 OOM。说说缓存线程池的原理？线程池大小不固定，corePoolSize = 0，maximumPoolSize = Integer.MAX_VALUE。空
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 闲线程超过 60 秒会被销毁，使用 SynchronousQueue 作为阻塞队列，适用于短时间内有大量任务的场景。提交任务时，如果线程池没有空闲线程，直接新建线程执行任务；如果有，复用线程执行任务。线程空闲 60 秒后销毁，减少资源占用。缺点是线程数没有上限，在高并发情况下可能导致 OOM。说说单线程线程池的原理？线程池只有 1 个线程，保证任务按提交顺序执行，使用 LinkedBlockingQueue 作为阻塞队列，适用于需要按顺序执行任务的场景。始终只创建 1 个线程，新任务必须等待前一个任务完成后才能执行，其他任务都被放入 LinkedBlockingQueue 排队执行。缺点是无法并行处理任务。说说定时任务线程池的原理？定时任务线程池的大小可配置，支持定时 & 周期性任务执行，使用DelayedWorkQueue 作为阻塞队列，适用于周期性执行任务的场景。执行定时任务时，schedule() 方法可以将任务延迟一定时间后执行一次；scheduleAtFixedRate()方法可以将任务延迟一定时间后以固定频率执行；scheduleWithFixedDelay() 方法可以将任务延迟一定时间后以固定
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 延迟执行。缺点是，如果任务执行时间 > 设定时间间隔，scheduleAtFixedRate 可能会导致任务堆积。线程池异常怎么处理知道吗？常见的处理方式有，使用 try-catch 捕获、使用 Future 获取异常、自定义ThreadPoolExecutor 重写 afterExecute 方法、使用 UncaughtExceptionHandler 捕获异常。能说一下线程池有几种状态吗？有 5 种状态，它们的转换遵循严格的状态流转规则，不同状态控制着线程池的任务调度和关闭行为。状态由 RUNNING→ SHUTDOWN→ STOP → TIDYING → TERMINATED 依次流转。RUNNING 状态的线程池可以接收新任务，并处理阻塞队列中的任务；SHUTDOWN状态的线程池不会接收新任务，但会处理阻塞队列中的任务；STOP 状态的线程池不会接收新任务，也不会处理阻塞队列中的任务，并且会尝试中断正在执行的任务；TIDYING 状态表示所有任务已经终止；TERMINATED 状态表示线程池完全关闭，所有线程销毁。线程池如何实现参数的动态修改？线程池提供的 setter 方法就可以在运行时动态修改参数
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，比如说setCorePoolSize 可以用来修改核心线程数、setMaximumPoolSize 可以用来修改最大线程数。线程池在使用的时候需要注意什么？（补充）我认为有 3 个比较重要的关注点：第一个，选择合适的线程池大小。过小的线程池可能会导致任务一直在排队；过大的线程池可能会导致大家都在竞争 CPU 资源，增加上下文切换的开销第二个，选择合适的任务队列。使用有界队列可以避免资源耗尽的风险，但是可能会导致任务被拒绝；使用无界队列虽然可以避免任务被拒绝，但是可能会导致内存耗尽比如在使用 LinkedBlockingQueue 的时候，可以传入参数来限制队列中任务的数量，这样就不会出现 OOM。第三个，尽量使用自定义的线程池，而不是使用 Executors 创建的线程池。因为 newFixedThreadPool 线程池由于使用了 LinkedBlockingQueue，队列的容量默认无限大，任务过多时会导致内存溢出；newCachedThreadPool 线程池由于核心线程数无限大，当任务过多的时候会导致创建大量的线程，导致服务器负载过高宕机。手写一个数据库连接池，可以吗？可以的，我的思路是这样的：数据
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 库连接池主要是为了避免每次操作数据库时都去创建连接，因为那样很浪费资源。所以我打算在初始化时预先创建好固定数量的连接，然后把它们放到一个线程安全的容器里，后续有请求的时候就从队列里拿，使用完后再归还到队列中JVMJVM，也就是 Java 虚拟机，它是 Java 实现跨平台的基石。程序运行之前，需要先通过编译器将 Java 源代码文件编译成 Java 字节码文件；程序运行时，JVM 会对字节码文件进行逐行解释，翻译成机器码指令，并交给对应的操作系统去执行说说 JVM 的其他特性？①、JVM 可以自动管理内存，通过垃圾回收器回收不再使用的对象并释放内存空间。②、JVM 包含一个即时编译器 JIT，它可以在运行时将热点代码缓存到codeCache 中，下次执行的时候不用再一行一行的解释，而是直接执行缓存后的机器码，执行效率会大幅提高说说 JVM 的组织架构（补充）JVM 大致可以划分为三个部分：类加载器、运行时数据区和执行引擎。① 类加载器，负责从文件系统、网络或其他来源加载 Class 文件，将 Class 文件中的二进制数据读入到内存当中。② 运行时数据区，JVM 在执行 Java 程序时，需要在内存中分配空间
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 来处理各种数据，这些内存区域按照 Java 虚拟机规范可以划分为方法区、堆、虚拟机栈、程序计数器和本地方法栈。③ 执行引擎，也是 JVM 的心脏，负责执行字节码。它包括一个虚拟处理器、即时编译器 JIT 和垃圾回收器能说一下 JVM 的内存区域吗？按照 Java 虚拟机规范，JVM 的内存区域可以细分为程序计数器、虚拟机栈、本地方法栈、堆和方法区。其中方法区和堆是线程共享的，虚拟机栈、本地方法栈和程序计数器是线程私有的。介绍一下程序计数器？程序计数器也被称为 PC 寄存器，是一块较小的内存空间。它可以看作是当前线程所执行的字节码行号指示器。介绍一下 Java 虚拟机栈？Java 虚拟机栈的生命周期与线程相同。当线程执行一个方法时，会创建一个对应的栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，然后栈帧会被压入虚拟机栈中。当方法执行完毕后，栈帧会从虚拟机栈中移除。介绍一下本地方法栈？本地方法栈与虚拟机栈相似，区别在于虚拟机栈是为 JVM 执行 Java 编写的方法服务的，而本地方法栈是为 Java 调用本地 native 方法服务的，通常由 C/C++编写。在本地方法栈中，主要存放了 native
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  方法的局部变量、动态链接和方法出口等信息。当一个 Java 程序调用一个 native 方法时，JVM 会切换到本地方法栈来执行这个方法介绍一下本地方法栈的运行场景？当 Java 应用需要与操作系统底层或硬件交互时，通常会用到本地方法栈。比如调用操作系统的特定功能，如内存管理、文件操作、系统时间、系统调用等。详细说明一下：比如说获取系统时间的 System.currentTimeMillis() 方法就是调用本地方法，来获取操作系统当前时间的。介绍一下 Java 堆？堆是 JVM 中最大的一块内存区域，被所有线程共享，在 JVM 启动时创建，主要用来存储 new 出来的对象。Java 中“几乎”所有的对象都会在堆中分配，堆也是垃圾收集器管理的目标区域。堆和栈的区别是什么？堆属于线程共享的内存区域，几乎所有 new 出来的对象都会堆上分配，生命周期不由单个方法调用所决定，可以在方法调用结束后继续存在，直到不再被任何变量引用，最后被垃圾收集器回收。栈属于线程私有的内存区域，主要存储局部变量、方法参数、对象引用等，通常随着方法调用的结束而自动释放，不需要垃圾收集器处理。介绍一下方法区？方法区并不真实存在，属于 J
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ava 虚拟机规范中的一个逻辑概念，用于存储已被JVM 加载的类信息、常量、静态变量、即时编译器编译后的代码缓存等。变量存在堆栈的什么位置？对于局部变量，它存储在当前方法栈帧中的局部变量表中。当方法执行完毕，栈帧被回收，局部变量也会被释放。对于静态变量来说，它存储在 Java 虚拟机规范中的方法区中对象创建的过程了解吗？当我们使用 new 关键字创建一个对象时，JVM 首先会检查 new 指令的参数是否能在常量池中定位到类的符号引用，然后检查这个符号引用代表的类是否已被加载、解析和初始化。如果没有，就先执行类加载。如果已经加载，JVM 会为对象分配内存完成初始化，比如数值类型的成员变量初始值是 0，布尔类型是 false，对象类型是 null。接下来会设置对象头，里面包含了对象是哪个类的实例、对象的哈希码、对象的GC 分代年龄等信息。最后，JVM 会执行构造方法 <init> 完成赋值操作，将成员变量赋值为预期的值，比如 int age = 18，这样一个对象就创建完成了对象的销毁过程了解吗？当对象不再被任何引用指向时，就会变成垃圾。垃圾收集器会通过可达性分析算法判断对象是否存活，如果对象不可达，就会被回收。
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 垃圾收集器通过标记清除、标记复制、标记整理等算法来回收内存，将对象占用的内存空间释放出来堆内存是如何分配的？在堆中为对象分配内存时，主要使用两种策略：指针碰撞和空闲列表。指针碰撞适用于管理简单、碎片化较少的内存区域，如年轻代；而空闲列表适用于内存碎片化较严重或对象大小差异较大的场景如老年代。什么是指针碰撞？假设堆内存是一个连续的空间，分为两个部分，一部分是已经被使用的内存，另一部分是未被使用的内存。在分配内存时，Java 虚拟机会维护一个指针，指向下一个可用的内存地址，每次分配内存时，只需要将指针向后移动一段距离，如果没有发生碰撞，就将这段内存分配给对象实例。什么是空闲列表？JVM 维护一个列表，记录堆中所有未占用的内存块，每个内存块都记录有大小和地址信息。当有新的对象请求内存时，JVM 会遍历空闲列表，寻找足够大的空间来存放新对象。分配后，如果选中的内存块未被完全利用，剩余的部分会作为一个新的内存块加入到空闲列表中。new 对象时，堆会发生抢占吗？new 对象时，指针会向右移动一个对象大小的距离，假如一个线程 A 正在给字符串对象 s 分配内存，另外一个线程 B 同时为 ArrayList 对象 l 分配内
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 存，两个线程就发生了抢占JVM 怎么解决堆内存分配的竞争问题？为了解决堆内存分配的竞争问题，JVM 为每个线程保留了一小块内存空间，被称为 TLAB，也就是线程本地分配缓冲区，用于存放该线程分配的对象。当线程需要分配对象时，直接从 TLAB 中分配。只有当 TLAB 用尽或对象太大需要直接在堆中分配时，才会使用全局分配指针。能说一下对象的内存布局吗？对象在内存中包括三部分：对象头、实例数据和对齐填充说说对象头的作用？对象头是对象存储在内存中的元信息，包含了 Mark Word、类型指针等信息。对齐填充了解吗？由于 JVM 的内存模型要求对象的起始地址是 8 字节对齐（64 位 JVM 中），因此对象的总大小必须是 8 字节的倍数。如果对象头和实例数据的总长度不是 8 的倍数，JVM 会通过填充额外的字节来对齐。比如说，如果对象头 + 实例数据 = 14 字节，则需要填充 2 个字节，使总长度变为 16 字节为什么非要进行 8 字节对齐呢？因为 CPU 进行内存访问时，一次寻址的指针大小是 8 字节，正好是 L1 缓存行的大小。如果不进行内存对齐，则可能出现跨缓存行访问，导致额外的缓存行加载，CPU 的访问效率
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 就会降低。new Object() 对象的内存大小是多少？一般来说，目前的操作系统都是 64 位的，并且 JDK 8 中的压缩指针是默认开启的，因此在 64 位的 JVM 上，new Object()的大小是 16 字节（12 字节的对象头 + 4 字节的对齐填充）。对象头的大小是固定的，在 32 位 JVM 上是 8 字节，在 64 位 JVM 上是 16字节；如果开启了压缩指针，就是 12 字节。实例数据的大小取决于对象的成员变量和它们的类型。对于 new Object()来说，由于默认没有成员变量，因此我们可以认为此时的实例数据大小是 0。对象的引用大小了解吗？在 64 位 JVM 上，未开启压缩指针时，对象引用占用 8 字节；开启压缩指针时，对象引用会被压缩到 4 字节。HotSpot 虚拟机默认是开启压缩指针的。JVM 怎么访问对象的？主流的方式有两种：句柄和直接指针。两种方式的区别在于，句柄是通过一个中间的句柄表来定位对象的，而直接指针则是通过引用直接指向对象的内存地址。优点是，对象被移动时只需要修改句柄表中的指针，而不需要修改对象引用本身。、在直接指针访问中，引用直接存储对象的内存地址；对象的实
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例数据和类型信息都存储在堆中固定的内存区域。说一下对象有哪几种引用？四种，分别是强引用、软引用、弱引用和虚引用。强引用是 Java 中最常见的引用类型。使用 new 关键字赋值的引用就是强引用，只要强引用关联着对象，垃圾收集器就不会回收这部分对象，即使内存不足。软引用于描述一些非必须对象，通过 SoftReference 类实现。软引用的对象在内存不足时会被回收弱引用用于描述一些短生命周期的非必须对象，如 ThreadLocal 中的 Entry，就是通过 WeakReference 类实现的。弱引用的对象会在下一次垃圾回收时会被回收，不论内存是否充足虚引用主要用来跟踪对象被垃圾回收的过程，通过 PhantomReference 类实现。虚引用的对象在任何时候都可能被回收Java 堆的内存分区了解吗？了解。Java 堆被划分为新生代和老年代两个区域。新生代又被划分为 Eden 空间和两个 Survivor 空间（From 和 To）。新创建的对象会被分配到 Eden 空间。当 Eden 区填满时，会触发一次 Minor GC，清除不再使用的对象。存活下来的对象会从 Eden 区移动到 Survivor 区。对
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象在新生代中经历多次 GC 后，如果仍然存活，会被移动到老年代。当老年代内存不足时，会触发 Major GC，对整个堆进行垃圾回收。对象什么时候会进入老年代？对象通常会在年轻代中分配，随着时间的推移和垃圾收集的进程，某些满足条件的对象会进入到老年代中，如长期存活的对象。长期存活的对象如何判断？JVM 会为对象维护一个“年龄”计数器，记录对象在新生代中经历 Minor GC 的次数。每次 GC 未被回收的对象，其年龄会加 1。当超过一个特定阈值，默认值是 15，就会被认为老对象了，需要重点关照。STW 了解吗？了解。JVM 进行垃圾回收的过程中，会涉及到对象的移动，为了保证对象引用在移动过程中不被修改，必须暂停所有的用户线程，像这样的停顿，我们称之为 Stop TheWorld。简称 STW。如何暂停线程呢？JVM 会使用一个名为安全点（Safe Point）的机制来确保线程能够被安全地暂停，其过程包括四个步骤：JVM 发出暂停信号；线程执行到安全点后，挂起自身并等待垃圾收集完成；垃圾回收器完成 GC 操作；线程恢复执行对象一定分配在堆中吗？不一定。默认情况下，Java 对象是在堆中分配的，但 JVM 会进行逃
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 逸分析，来判断对象的生命周期是否只在方法内部，如果是的话，这个对象可以在栈上分配。逃逸分析是一种 JVM 优化技术，用来分析对象的作用域和生命周期，判断对象是否逃逸出方法或线程逃逸分析会带来什么好处？主要有三个。第一，如果确定一个对象不会逃逸，那么就可以考虑栈上分配，对象占用的内存随着栈帧出栈后销毁，这样一来，垃圾收集的压力就降低很多。第二，线程同步需要加锁，加锁就要占用系统资源，如果逃逸分析能够确定一个对象不会逃逸出线程，那么这个对象就不用加锁，从而减少线程同步的开销。第三，如果对象的字段在方法中独立使用，JVM 可以将对象分解为标量变量，避免对象分配内存溢出和内存泄漏了解吗？内存溢出，俗称 OOM，是指当程序请求分配内存时，由于没有足够的内存空间，从而抛出 OutOfMemoryError。可能是因为堆、元空间、栈或直接内存不足导致的。可以通过优化内存配置、减少对象分配来解决。内存泄漏是指程序在使用完内存后，未能及时释放，导致占用的内存无法再被使用。随着时间的推移，内存泄漏会导致可用内存逐渐减少，最终导致内存溢出。内存泄漏通常是因为长期存活的对象持有短期存活对象的引用，又没有及时释放，从而导致短期存活对象
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 无法被回收而导致的内存泄漏可能由哪些原因导致呢？静态的集合中添加的对象越来越多，但却没有及时清理；静态变量的生命周期与应用程序相同，如果静态变量持有对象的引用，这些对象将无法被 GC 回收。单例模式下对象持有的外部引用无法及时释放；单例对象在整个应用程序的生命周期中存活，如果单例对象持有其他对象的引用，这些对象将无法被回收。数据库、IO、Socket 等连接资源没有及时关闭；ThreadLocal 的引用未被清理，线程退出后仍然持有对象引用；在线程执行完后，要调用 ThreadLocal 的 remove 方法进行清理。有没有处理过内存泄漏问题？当时在做技术派项目的时候，由于 ThreadLocal 没有及时清理导致出现了内存泄漏问题什么情况下会发生栈溢出？（补充）栈溢出发生在程序调用栈的深度超过 JVM 允许的最大深度时。栈溢出的本质是因为线程的栈空间不足，导致无法再为新的栈帧分配内存当一个方法被调用时，JVM 会在栈中分配一个栈帧，用于存储该方法的执行信息。如果方法调用嵌套太深，栈帧不断压入栈中，最终会导致栈空间耗尽，抛出StackOverflowError。讲讲 JVM 的垃圾回收机制（补充）垃圾回收就
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 是对内存堆中已经死亡的或者长时间没有使用的对象进行清除或回收。JVM 在做 GC 之前，会先搞清楚什么是垃圾，什么不是垃圾，通常会通过可达性分析算法来判断对象是否存活。垃圾回收的过程是什么？Java 的垃圾回收过程主要分为标记存活对象、清除无用对象、以及内存压缩/整理三个阶段。不同的垃圾回收器在执行这些步骤时会采用不同的策略和算法如何判断对象仍然存活？Java 通过可达性分析算法来判断一个对象是否还存活。通过一组名为 “GC Roots” 的根对象，进行递归扫描，无法从根对象到达的对象就是“垃圾”，可以被回收。这也是 G1、CMS 等主流垃圾收集器使用的主要算法。什么是引用计数法？每个对象有一个引用计数器，记录引用它的次数。当计数器为零时，对象可以被回收。引用计数法无法解决循环引用的问题。例如，两个对象互相引用，但不再被其他对象引用，它们的引用计数都不为零，因此不会被回收。做可达性分析的时候，应该有哪些前置性的操作？在进行垃圾回收之前，JVM 会暂停所有正在执行的应用线程。这是因为可达性分析过程必须确保在执行分析时，内存中的对象关系不会被应用线程修改。如果不暂停应用线程，可能会出现对象引用的改变，导致垃圾回收
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过程中判断对象是否可达的结果不一致，从而引发严重的内存错误或数据丢失Java 中可作为 GC Roots 的引用有哪几种？所谓的 GC Roots，就是一组必须活跃的引用，它们是程序运行时的起点，是一切引用链的源头。在 Java 中，GC Roots 包括以下几种虚拟机栈中的引用（方法的参数、局部变量等）本地方法栈中 JNI 的引用类静态变量运行时常量池中的常量（String 或 Class 类型）finalize()方法了解吗？垃圾回收就是古代的秋后问斩，finalize() 就是刀下留人，在人犯被处决之前，还要做最后一次审计，青天大老爷会看看有没有什么冤情，需不需要刀下留人。如果对象在进行可达性分析后发现没有与 GC Roots 相连接的引用链，那它将会被第一次标记，随后进行一次筛选。筛选的条件是对象是否有必要执行 finalize()方法。如果对象在 finalize() 中成功拯救自己——只要重新与引用链上的任何一个对象建立关联即可。垃圾收集算法了解吗？垃圾收集算法主要有三种，分别是标记-清除算法、标记-复制算法和标记-整理算法说说标记-清除算法？标记-清除算法分为两个阶段：标记：标记所有需要回收的对
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 象清除：回收所有被标记的对象优点是实现简单，缺点是回收过程中会产生内存碎片说说标记-复制算法？标记-复制算法可以解决标记-清除算法的内存碎片问题，因为它将内存空间划分为两块，每次只使用其中一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后清理掉这一块。缺点是浪费了一半的内存空间。说说标记-整理算法？标记-整理算法是标记-清除复制算法的升级版，它不再划分内存空间，而是将存活的对象向内存的一端移动，然后清理边界以外的内存。缺点是移动对象的成本比较高。说说分代收集算法？分代收集算法是目前主流的垃圾收集算法，它根据对象存活周期的不同将内存划分为几块，一般分为新生代和老年代。新生代用复制算法，因为大部分对象生命周期短。老年代用标记-整理算法，因为对象存活率较高。为什么要用分代收集呢？分代收集算法的核心思想是根据对象的生命周期优化垃圾回收。新生代的对象生命周期短，使用复制算法可以快速回收。老年代的对象生命周期长，使用标记-整理算法可以减少移动对象的成本Minor GC、Major GC、Mixed GC、Full GC 都是什么意思？Minor GC 也称为 Young GC，是指发生在年轻代的垃圾收
2025-08-11 10:56:08.176 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 集。年轻代包含 Eden 区以及两个 Survivor 区Major GC 也称为 Old GC，主要指的是发生在老年代的垃圾收集。是 CMS 的特有行为。Mixed GC 是 G1 垃圾收集器特有的一种 GC 类型，它在一次 GC 中同时清理年轻代和部分老年代。Full GC 是最彻底的垃圾收集，涉及整个 Java 堆和方法区。它是最耗时的 GC，通常在 JVM 压力很大时发生FULL gc 怎么去清理的？Full GC 会从 GC Root 出发，标记所有可达对象。新生代使用复制算法，清空Eden 区。老年代使用标记-整理算法，回收对象并消除碎片。Young GC 什么时候触发？如果 Eden 区没有足够的空间时，就会触发 Young GC 来清理新生代。什么时候会触发 Full GC？在进行 Young GC 的时候，如果发现老年代可用的连续内存空间 < 新生代历次Young GC 后升入老年代的对象总和的平均大小，说明本次 Young GC 后升入老年代的对象大小，可能超过了老年代当前可用的内存空间，就会触发 Full GC。执行 Young GC 后老年代没有足够的内存空间存放转入的对象，会立即触发
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一次Full GC。知道哪些垃圾收集器？JVM 的垃圾收集器主要分为两大类：分代收集器和分区收集器，分代收集器的代表是 CMS，分区收集器的代表是 G1 和 ZGC。说说 CMS 收集器？CMS 是一种低延迟的垃圾收集器，采用标记-清除算法，分为初始标记、并发标记、重新标记和并发清除四个阶段，优点是垃圾回收线程和应用线程同时运行，停顿时间短，适合延迟敏感的应用，但容易产生内存碎片，可能触发 Full GC能详细说一下 CMS 的垃圾收集过程吗？CMS 使用标记-清除算法进行垃圾收集，分 4 大步：初始标记：标记所有从 GC Roots 直接可达的对象，这个阶段需要 STW，但速度很快。并发标记：从初始标记的对象出发，遍历所有对象，标记所有可达的对象。这个阶段是并发进行的。重新标记：完成剩余的标记工作，包括处理并发阶段遗留下来的少量变动，这个阶段通常需要短暂的 STW 停顿。并发清除：清除未被标记的对象，回收它们占用的内存空间。三色标记法的工作流程：①、初始标记（Initial Marking）：从 GC Roots 开始，标记所有直接可达的对象为灰色。②、并发标记（Concurrent Marking）：在此
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 阶段，标记所有灰色对象引用的对象为灰色，然后将灰色对象自身标记为黑色。这个过程是并发的，和应用线程同时进行。此阶段的一个问题是，应用线程可能在并发标记期间修改对象的引用关系，导致一些对象的标记状态不准确。③、重新标记（Remarking）：重新标记阶段的目标是处理并发标记阶段遗漏的引用变化。为了确保所有存活对象都被正确标记，remark 需要在 STW 暂停期间执行。④、使用写屏障（Write Barrier）来捕捉并发标记阶段应用线程对对象引用的更新。通过遍历这些更新的引用来修正标记状态，确保遗漏的对象不会被错误地回收。说说 G1 收集器？G1 是一种面向大内存、高吞吐场景的垃圾收集器，它将堆划分为多个小的Region，通过标记-整理算法，避免了内存碎片问题。优点是停顿时间可控，适合大堆场景，但调优较复杂。G1 收集器的运行过程大致可划分为这几个步骤：①、并发标记，G1 通过并发标记的方式找出堆中的垃圾对象。并发标记阶段与应用线程同时执行，不会导致应用线程暂停。②、混合收集，在并发标记完成后，G1 会计算出哪些区域的回收价值最高（也就是包含最多垃圾的区域），然后优先回收这些区域。这种回收方式包括了部分新生代
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 区域和老年代区域。选择回收成本低而收益高的区域进行回收，可以提高回收效率和减少停顿时间。3 、可预测的停顿，G1 在垃圾回收期间仍然需要「Stop the World」。不过，G1 在停顿时间上添加了预测机制，用户可以 JVM 启动时指定期望停顿时间，G1会尽可能地在这个时间内完成垃圾回收。CMS 适用于对延迟敏感的应用场景，主要目标是减少停顿时间，但容易产生内存碎片。G1 则提供了更好的停顿时间预测和内存压缩能力，适用于大内存和多核处理器环境说说 ZGC 收集器？ZGC 是 JDK 11 时引入的一款低延迟的垃圾收集器，最大特点是将垃圾收集的停顿时间控制在 10ms 以内，即使在 TB 级别的堆内存下也能保持较低的停顿时间。它通过并发标记和重定位来避免大部分 Stop-The-World 停顿，主要依赖指针染色来管理对象状态。标记对象的可达性：通过在指针上增加标记位，不需要额外的标记位即可判断对象的存活状态。重定位状态：在对象被移动时，可以通过指针染色来更新对象的引用，而不需要等待全局同步垃圾回收器的作用是什么？垃圾回收器的核心作用是自动管理 Java 应用程序的运行时内存。它负责识别哪些内存是不再被应用程
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 序使用的，并释放这些内存以便重新使用。这一过程减少了程序员手动管理内存的负担，降低了内存泄漏和溢出错误的风险你们线上用的什么垃圾收集器？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，因为它不仅能满足低停顿的要求，而且解决了 CMS 的浮动垃圾问题、内存碎片问题。G1 非常适合大内存、多核处理器的环境。工作中项目使用的什么垃圾回收算法？我们生产环境中采用了设计比较优秀的 G1 垃圾收集器，G1 采用的是分区式标记-整理算法，将堆划分为多个区域，按需回收，适用于大内存和多核环境，能够同时考虑吞吐量和暂停时间JVM 调优37. 用过哪些性能监控的命令行工具？操作系统层面，我用过 top、vmstat、iostat、netstat 等命令，可以监控系统整体的资源使用情况，比如说内存、CPU、IO 使用情况、网络使用情况。JDK 自带的命令行工具层面，我用过 jps、jstat、jinfo、jmap、jhat、jstack、jcmd 等，可以查看 JVM 运行时信息、内存使用情况、堆栈信息等。你一般都怎么用 jmap？我一般会使用 jmap -heap <pid> 查看堆内存摘要，包括新生代、老年代、元空间等。
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 或者使用 jmap -histo <pid> 查看对象分布了解哪些可视化的性能监控工具？JConsole：JDK 自带的监控工具，可以用来监视 Java 应用程序的运行状态，包括内存使用、线程状态、类加载、GC 等。JVM 的常见参数配置知道哪些？配置堆内存大小的参数有哪些？-Xms：初始堆大小-Xmx：最大堆大小-XX:NewSize=n：设置年轻代大小-XX:NewRatio=n：设置年轻代和年老代的比值。如：n 为 3 表示年轻代和年老代比值为 1：3，年轻代占总和的 1/4-XX:SurvivorRatio=n：年轻代中 Eden 区与两个 Survivor 区的比值。如 n=3表示 Eden 占 3 Survivor 占 2，一个 Survivor 区占整个年轻代的 1/5做过 JVM 调优吗？JVM 调优是一个复杂的过程，调优的对象包括堆内存、垃圾收集器和 JVM 运行时参数等。如果堆内存设置过小，可能会导致频繁的垃圾回收。所以在技术派实战项目中，启动 JVM 的时候配置了 -Xms 和 -Xmx 参数，让堆内存最大可用内存为 2G（我用的丐版服务器）。在项目运行期间，我会使用 JVisualVM
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  定期观察和分析 GC 日志，如果发现频繁的 Full GC，我会特意关注一下老年代的使用情况。接着，通过分析 Heap dump 寻找内存泄漏的源头，看看是否有未关闭的资源，长生命周期的大对象等。之后进行代码优化，比如说减少大对象的创建、优化数据结构的使用方式、减少不必要的对象持有等CPU 占用过高怎么排查？首先，使用 top 命令查看 CPU 占用情况，找到占用 CPU 较高的进程 ID。接着，使用 jstack 命令查看对应进程的线程堆栈信息。然后再使用 top 命令查看进程中线程的占用情况，找到占用 CPU 较高的线程ID接着在 jstack 的输出中搜索这个十六进制的线程 ID，找到对应的堆栈信息。最后，根据堆栈信息定位到具体的业务方法，查看是否有死循环、频繁的垃圾回收、资源竞争导致的上下文频繁切换等问题内存飙高问题怎么排查？内存飚高一般是因为创建了大量的 Java 对象导致的，如果持续飙高则说明垃圾回收跟不上对象创建的速度，或者内存泄漏导致对象无法回收排查的方法主要分为以下几步：第一，先观察垃圾回收的情况，可以通过 jstat -gc PID 1000 查看 GC 次数和时间。或者使用 jmap 
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -histo PID | head -20 查看堆内存占用空间最大的前 20 个对象类型。第二步，通过 jmap 命令 dump 出堆内存信息第三步，使用可视化工具分析 dump 文件，比如说 VisualVM，找到占用内存高的对象，再找到创建该对象的业务代码位置，从代码和业务场景中定位具体问题。频繁 minor gc 怎么办？频繁的 Minor GC 通常意味着新生代中的对象频繁地被垃圾回收，可能是因为新生代空间设置的过小，或者是因为程序中存在大量的短生命周期对象（如临时变量）。可以使用 GC 日志进行分析，查看 GC 的频率和耗时，找到频繁 GC 的原因或者使用监控工具查看堆内存的使用情况，特别是新生代（Eden 和 Survivor 区）的使用情况。如果是因为新生代空间不足，可以通过 -Xmn 增加新生代的大小，减缓新生代的填满速度。如果对象需要长期存活，但频繁从 Survivor 区晋升到老年代，可以通过-XX:SurvivorRatio 参数调整 Eden 和 Survivor 的比例。默认比例是 8:1，表示 8 个空间用于 Eden，1 个空间用于 Survivor 区。调整为 6 的话，会减少
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Eden 区的大小，增加 Survivor 区的大小，以确保对象在 Survivor 区中存活的时间足够长，避免过早晋升到老年代。频繁 Full GC 怎么办？频繁的 Full GC 通常意味着老年代中的对象频繁地被垃圾回收，可能是因为老年代空间设置的过小，或者是因为程序中存在大量的长生命周期对象该怎么排查 Full GC 频繁问题？通过专门的性能监控系统，查看 GC 的频率和堆内存的使用情况，然后根据监控数据分析 GC 的原因。假如是因为大对象直接分配到老年代导致的 Full GC 频繁，可以通过-XX:PretenureSizeThreshold 参数设置大对象直接进入老年代的阈值。或者将大对象拆分成小对象，减少大对象的创建。比如说分页。假如是因为内存泄漏导致的频繁 Full GC，可以通过分析堆内存 dump 文件找到内存泄漏的对象，再找到内存泄漏的代码位置。假如是因为长生命周期的对象进入到了老年代，要及时释放资源，比如说ThreadLocal、数据库连接、IO 资源等。了解类的加载机制吗？（补充）JVM 的操作对象是 Class 文件，JVM 把 Class 文件中描述类的数据结构加载到内存中，并对数
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据进行校验、解析和初始化，最终转化成可以被 JVM 直接使用的类型，这个过程被称为类加载机制。其中最重要的三个概念就是：类加载器、类加载过程和双亲委派模型。类加载器：负责加载类文件，将类文件加载到内存中，生成 Class 对象。类加载过程：包括加载、验证、准备、解析和初始化等步骤。双亲委派模型：当一个类加载器接收到类加载请求时，它会把请求委派给父——类加载器去完成，依次递归，直到最顶层的类加载器，如果父——类加载器无法完成加载请求，子类加载器才会尝试自己去加载。能说一下类的生命周期吗？一个类从被加载到虚拟机内存中开始，到从内存中卸载，整个生命周期需要经过七个阶段：加载 、验证、准备、解析、初始化、使用和卸载。以下是整理自网络的一些 JVM 调优实例：网站流量浏览量暴增后，网站反应页面响很慢问题推测：在测试环境测速度比较快，但是一到生产就变慢，所以推测可能是因为垃圾收集导致的业务线程停顿。定位：为了确认推测的正确性，在线上通过 jstat -gc 指令 看到 JVM 进行 GC 次数频率非常高，GC 所占用的时间非常长，所以基本推断就是因为 GC 频率非常高，所以导致业务线程经常停顿，从而造成网页反应很慢。解决
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 方案：因为网页访问量很高，所以对象创建速度非常快，导致堆内存容易填满从而频繁 GC，所以这里问题在于新生代内存太小，所以这里可以增加 JVM 内存就行了，所以初步从原来的 2G 内存增加到 16G 内存。第二个问题：增加内存后的确平常的请求比较快了，但是又出现了另外一个问题，就是不定期的会间断性的卡顿，而且单次卡顿的时间要比之前要长很多问题推测：练习到是之前的优化加大了内存，所以推测可能是因为内存加大了，从而导致单次 GC 的时间变长从而导致间接性的卡顿。定位：还是通过 jstat -gc 指令 查看到 的确 FGC 次数并不是很高，但是花费在 FGC 上的时间是非常高的,根据 GC 日志 查看到单次 FGC 的时间有达到几十秒的。解决方案： 因为 JVM 默认使用的是 PS+PO 的组合，PS+PO 垃圾标记和收集阶段都是 STW，所以内存加大了之后，需要进行垃圾回收的时间就变长了，所以这里要想避免单次 GC 时间过长，所以需要更换并发类的收集器，因为当前的 JDK 版本为 1.7，所以最后选择 CMS 垃圾收集器，根据之前垃圾收集情况设置了一个预期的停顿的时间，上线后网站再也没有了卡顿问题。公司的后台系统
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ，偶发性的引发 OOM 异常，堆内存溢出。因为是偶发性的，所以第一次简单的认为就是堆内存不足导致，所以单方面的加大了堆内存从 4G 调整到 8G。但是问题依然没有解决，只能从堆内存信息下手，通过开启了-XX:+HeapDumpOnOutOfMemoryError 参数 获得堆内存的 dump 文件。VisualVM 对 堆 dump 文件进行分析，通过 VisualVM 查看到占用内存最大的对象是 String 对象，本来想跟踪着 String 对象找到其引用的地方，但 dump 文件太大，跟踪进去的时候总是卡死，而 String 对象占用比较多也比较正常，最开始也没有认定就是这里的问题，于是就从线程信息里面找突破点。通过线程进行分析，先找到了几个正在运行的业务线程，然后逐一跟进业务线程看了下代码，发现有个引起我注意的方法，导出订单信息。因为订单信息导出这个方法可能会有几万的数据量，首先要从数据库里面查询出来订单信息，然后把订单信息生成 excel，这个过程会产生大量的 String 对象为了验证自己的猜想，于是准备登录后台去测试下，结果在测试的过程中发现到处订单的按钮前端居然没有做点击后按钮置灰交互事件，结
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果按钮可以一直点，因为导出订单数据本来就非常慢，使用的人员可能发现点击后很久后页面都没反应，结果就一直点，结果就大量的请求进入到后台，堆内存产生了大量的订单对象和 EXCEL 对象，而且方法执行非常慢，导致这一段时间内这些对象都无法被回收，所以最终导致内存溢出jvm.gc.time：每分钟的 GC 耗时在 1s 以内，500ms 以内尤佳jvm.gc.meantime：每次 YGC 耗时在 100ms 以内，50ms 以内尤佳jvm.fullgc.count：FGC 最多几小时 1次，1天不到 1次尤佳jvm.fullgc.time：每次 FGC 耗时在 1s 以内，500ms 以内尤佳// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pidSpringSpring 是什么？Spring 是一个 Java 后端开发框架，其最核心的作用是帮我们管理 Java 对象其最重要的特性就是 IoC，也就是控制反转。以前我们要使用一个对象时，都要自己先 new 出来。但有了 Spring 之后，我们只需要告诉 Spr
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ing 我们需要什么对象，它就会自动帮我们创建好并注入到 Spring 容器当中。比如我在一个Service 类里需要用到 Dao 对象，只需要加个 @Autowired 注解，Spring 就会自动把 Dao 对象注入到 Spring 容器当中，这样就不需要我们手动去管理这些对象之间的依赖关系了。另外，Spring 还提供了 AOP，也就是面向切面编程，在我们需要做一些通用功能的时候特别有用，比如说日志记录、权限校验、事务管理这些，我们不用在每个方法里都写重复的代码，直接用 AOP 就能统一处理。Spring 的生态也特别丰富，像 Spring Boot 能让我们快速搭建项目，Spring MVC能帮我们处理 web 请求，Spring Data 能帮我们简化数据库操作，Spring Cloud能帮我们做微服务架构等等Spring 有哪些特性？首先最核心的就是 IoC 控制反转和 DI 依赖注入。这个我前面也提到了，就是Spring 能帮我们管理对象的创建和依赖关系。第二个就是 AOP 面向切面编程。这个在我们处理一些横切关注点的时候特别有用，比如说我们要给某些 Controller 方法都加上权限控制，如
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 果没有 AOP 的话，每个方法都要写一遍加权代码，维护起来很麻烦。用了 AOP 之后，我们只需要写一个切面类，定义好切点和通知，就能统一处理了。事务管理也是同样的道理，加个 @Transactional 注解就搞定了。简单说一下什么是 AOP 和 IoC？AOP 面向切面编程，简单点说就是把一些通用的功能从业务代码里抽取出来，统一处理。比如说技术派中的 @MdcDot 注解的作用是配合 AOP 在日志中加入 MDC信息，方便进行日志追踪。IoC 控制反转是一种设计思想，它的主要作用是将对象的创建和对象之间的调用过程交给 Spring 容器来管理。Spring 有哪些模块呢？首先是 Spring Core 模块，这是整个 Spring 框架的基础，包含了 IoC 容器和依赖注入等核心功能。还有 Spring Beans 模块，负责 Bean 的配置和管理。这两个模块基本上是其他所有模块的基础，不管用 Spring 的哪个功能都会用到。然后是 Spring Context 上下文模块，它在 Core 的基础上提供了更多企业级的功能，比如国际化、事件传播、资源加载这些。ApplicationContext 就是在这
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个模块里面的。Spring AOP 模块提供了面向切面编程的支持，我们用的@Transactional、自定义切面这些都是基于这个模块。Web 开发方面，Spring Web 模块提供了基础的 Web 功能，Spring WebMVC 就是我们常用的 MVC 框架，用来处理 HTTP 请求和响应。现在还有 Spring WebFlux，支持响应式编程。还有一些其他的模块，比如 Spring Security 负责安全认证，Spring Batch 处理批处理任务等等。现在我们基本都是用 Spring Boot 来开发，它把这些模块都整合好了，用起来更方便。Spring 有哪些常用注解呢？Spring 的注解挺多的，我按照不同的功能分类来说一下平时用得最多的那些。首先是 Bean 管理相关的注解。@Component 是最基础的，用来标识一个类是Spring 组件。像 @Service、@Repository、@Controller 这些都是 @Component的特化版本，分别用在服务层、数据访问层和控制器层。依赖注入方面，@Autowired 是用得最多的，可以标注在字段、setter 方法或者构造方法上。
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: @Qualifier 在有多个同类型 Bean 的时候用来指定具体注入哪一个。@Resource 和 @Autowired 功能差不多，不过它是按名称注入的。配置相关的注解也很常用。@Configuration 标识配置类，@Bean 用来定义 Bean，@Value 用来注入配置文件中的属性值。我们项目里的数据库连接信息、Redis 配置这些都是用 @Value 来注入的。@PropertySource 用来指定配置文件的位置。@RequestMapping 及其变体@GetMapping、@PostMapping、@PutMapping、@DeleteMapping 用来映射 HTTP 请求。@PathVariable 获取路径参数，@RequestParam 获取请求参数，@RequestBody 接收 JSON 数据。、AOP 相关的注解，@Aspect 定义切面，@Pointcut 定义切点，@Before、@After、@Around 这些定义通知类型不过我们用得最多的还是@Transactional，基本上 Service 层需要保证事务原子性的方法都会加上这个注解。Spring 用了哪些设计模
2025-08-11 10:56:08.177 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 式？Spring 框架里面确实用了很多设计模式，我从平时工作中能观察到的几个来说说。首先是工厂模式，这个在 Spring 里用得非常多。BeanFactory 就是一个典型的工厂，它负责创建和管理所有的 Bean 对象。我们平时用的 ApplicationContext其实也是 BeanFactory 的一个实现。当我们通过 @Autowired 获取一个 Bean的时候，底层就是通过工厂模式来创建和获取对象的。单例模式也是 Spring 的默认行为。默认情况下，Spring 容器中的 Bean 都是单例的，整个应用中只会有一个实例。这样可以节省内存，提高性能。当然我们也可以通过 @Scope 注解来改变 Bean 的作用域，比如设置为 prototype 就是每次获取都创建新实例。代理模式在 AOP 中用得特别多。Spring AOP 的底层实现就是基于动态代理的，对于实现了接口的类用 JDK 动态代理，没有实现接口的类用 CGLIB 代理。比如我们用 @Transactional 注解的时候，Spring 会为我们的类创建一个代理对象，在方法执行前后添加事务处理逻辑。Spring 如何实现单例模式？传统的
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 单例模式是在类的内部控制只能创建一个实例，比如用 private 构造方法加 static getInstance() 这种方式。但是 Spring 的单例是容器级别的，同一个 Bean 在整个 Spring 容器中只会有一个实例。具体的实现机制是这样的：Spring 在启动的时候会把所有的 Bean 定义信息加载进来，然后在 DefaultSingletonBeanRegistry 这个类里面维护了一个叫singletonObjects 的 ConcurrentHashMap，这个 Map 就是用来存储单例 Bean的。key 是 Bean 的名称，value 就是 Bean 的实例对象。当我们第一次获取某个 Bean 的时候，Spring 会先检查 singletonObjects 这个 Map 里面有没有这个 Bean，如果没有就会创建一个新的实例，然后放到 Map 里面。后面再获取同一个 Bean 的时候，直接从 Map 里面取就行了，这样就保证了单例。还有一个细节就是 Spring 为了解决循环依赖的问题，还用了三级缓存。除了singletonObjects 这个一级缓存，还有 earlySingl
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: etonObjects 二级缓存和singletonFactories 三级缓存。这样即使有循环依赖，Spring 也能正确处理Spring 容器和 Web 容器之间的区别知道吗？（补充）首先从概念上来说，Spring 容器是一个 IoC 容器，主要负责管理 Java 对象的生命周期和依赖关系。而 Web 容器，比如 Tomcat、Jetty 这些，是用来运行 Web应用的容器，负责处理 HTTP 请求和响应，管理 Servlet 的生命周期。从功能上看，Spring 容器专注于业务逻辑层面的对象管理，比如我们的 Service、Dao、Controller 这些 Bean 都是由 Spring 容器来创建和管理的。而 Web 容器主要处理网络通信，比如接收 HTTP 请求、解析请求参数、调用相应的 Servlet，然后把响应返回给客户端在实际项目中，这两个容器是相辅相成的。我们的 Web 项目部署在 Tomcat 上的时候，Tomcat 会负责接收 HTTP 请求，然后把请求交给 DispatcherServlet处理，而 DispatcherServlet 又会去 Spring 容器中查找相应的 Cont
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: roller来处理业务逻辑。现在我们都用 Spring Boot 了，Spring Boot 内置了 Tomcat，把 Web 容器和Spring 容器都整合在一起了，我们只需要运行一个 jar 包就可以了说一说什么是 IoC？IoC 的全称是 Inversion of Control，也就是控制反转。这里的“控制”指的是对象创建和依赖关系管理的控制权。以前我们写代码的时候，如果 A 类需要用到 B 类，我们就在 A 类里面直接 new一个 B 对象出来，这样 A 类就控制了 B 类对象的创建。有了 IoC 之后，这个控制权就“反转”了，不再由 A 类来控制 B 对象的创建，而是交给外部的容器来管理。DI 和 IoC 的区别了解吗？IoC 的思想是把对象创建和依赖关系的控制权由业务代码转移给 Spring 容器。这是一个比较抽象的概念，告诉我们应该怎么去设计系统架构。而 DI，也就是依赖注入，它是实现 IoC 这种思想的具体技术手段。在 Spring 里，我们用 @Autowired 注解就是在使用 DI 的字段注入方式。为什么要使用 IoC 呢？在日常开发中，如果我们需要实现某一个功能，可能至少需要两个以上
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的对象来协助完成，在没有 Spring 之前，每个对象在需要它的合作对象时，需要自己 new一个，比如说 A 要使用 B，A 就对 B 产生了依赖，也就是 A 和 B 之间存在了一种耦合关系能说一下 IoC 的实现机制吗？好的，Spring IoC 的实现机制还是比较复杂的，我尽量用比较通俗的方式来解释一下整个流程。第一步是加载 Bean 的定义信息。Spring 会扫描我们配置的包路径，找到所有标注了 @Component、@Service、@Repository 这些注解的类，然后把这些类的元信息封装成 BeanDefinition 对象。第二步是 Bean 工厂的准备。Spring 会创建一个 DefaultListableBeanFactory作为 Bean 工厂来负责 Bean 的创建和管理。第三步是 Bean 的实例化和初始化。这个过程比较复杂，Spring 会根据BeanDefinition 来创建 Bean 实例。对于单例 Bean，Spring 会先检查缓存中是否已经存在，如果不存在就创建新实例。创建实例的时候会通过反射调用构造方法，然后进行属性注入，最后执行初始化回调方法。依赖注入的实现主
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 要是通过反射来完成的。比如我们用 @Autowired 标注了一个字段，Spring 在创建 Bean 的时候会扫描这个字段，然后从容器中找到对应类型的 Bean，通过反射的方式设置到这个字段上。你是怎么理解 Spring IoC 的？IoC 本质上一个超级工厂，这个工厂的产品就是各种 Bean 对象。我们通过 @Component、@Service 这些注解告诉工厂：“我要生产什么样的产品，这个产品有什么特性，需要什么原材料”。然后工厂里各种生产线，在 Spring 中就是各种 BeanPostProcessor。比如AutowiredAnnotationBeanPostProcessor 专门负责处理 @Autowired 注解。工厂里还有各种缓存机制用来存放产品，比如说 singletonObjects 是成品仓库，存放完工的单例 Bean；earlySingletonObjects 是半成品仓库，用来解决循环依赖问题。说说 BeanFactory 和 ApplicantContext 的区别?BeanFactory 算是 Spring 的“心脏”，而 ApplicantContext 可以说是Spri
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ng 的完整“身躯”。BeanFactory 提供了最基本的 IoC 能力。它就像是一个 Bean 工厂，负责 Bean的创建和管理。他采用的是懒加载的方式，也就是说只有当我们真正去获取某个Bean 的时候，它才会去创建这个 Bean。ApplicationContext 是 BeanFactory 的子接口，在 BeanFactory 的基础上扩展了很多企业级的功能。它不仅包含了 BeanFactory 的所有功能，还提供了国际化支持、事件发布机制、AOP、JDBC、ORM 框架集成等等。ApplicationContext 采用的是饿加载的方式，容器启动的时候就会把所有的单例 Bean 都创建好，虽然这样会导致启动时间长一点，但运行时性能更好。另外一个重要的区别是生命周期管理。ApplicationContext 会自动调用 Bean的初始化和销毁方法，而 BeanFactory 需要我们手动管理。在 Spring Boot 项目中，我们可以通过 @Autowired 注入 ApplicationContext，或者通过实现 ApplicationContextAware 接口来获取 Applicatio
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: nContext。项目启动时 Spring 的 IoC 会做什么？第一件事是扫描和注册 Bean。IoC 容器会根据我们的配置，比如@ComponentScan 指定的包路径，去扫描所有标注了 @Component、@Service、@Controller 这些注解的类。然后把这些类的元信息包装成 BeanDefinition 对象，注册到容器的 BeanDefinitionRegistry 中。这个阶段只是收集信息，还没有真正创建对象。第二件事是 Bean 的实例化和注入。这是最核心的过程，IoC 容器会按照依赖关系的顺序开始创建 Bean 实例。对于单例 Bean，容器会通过反射调用构造方法创建实例，然后进行属性注入，最后执行初始化回调方法在依赖注入时，容器会根据 @Autowired、@Resource 这些注解，把相应的依赖对象注入到目标 Bean 中。比如 UserService 需要 UserDao，容器就会把UserDao 的实例注入到 UserService 中。说说 Spring 的 Bean 实例化方式？Spring 提供了 4 种方式来实例化 Bean，以满足不同场景下的需求第一种是通过
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法实例化，这是最常用的方式。当我们用 @Component、@Service 这些注解标注类的时候，Spring 默认通过无参构造器来创建实例的。如果类只有一个有参构造方法，Spring 会自动进行构造方法注入。第二种是通过静态工厂方法实例化。有时候对象的创建比较复杂，我们会写一个静态工厂方法来创建，然后用 @Bean 注解来标注这个方法。Spring 会调用这个静态方法来获取 Bean 实例。第三种是通过实例工厂方法实例化。这种方式是先创建工厂对象，然后通过工厂对象的方法来创建 Bean：第四种是通过 FactoryBean 接口实例化。这是 Spring 提供的一个特殊接口，当我们需要创建复杂对象的时候特别有用：你是怎么理解 Bean 的？在我看来，Bean 本质上就是由 Spring 容器管理的 Java 对象，但它和普通的Java 对象有很大区别。普通的 Java 对象我们是通过 new 关键字创建的。而Bean 是交给 Spring 容器来管理的，从创建到销毁都由容器负责。从实际使用的角度来说，我们项目里的 Service、Dao、Controller 这些都是Bean。比如 UserServ
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ice 被标注了 @Service 注解，它就成了一个 Bean，Spring会自动创建它的实例，管理它的依赖关系，当其他地方需要用到 UserService 的时候，Spring 就会把这个实例注入进去。这种依赖注入的方式让对象之间的关系变得松耦合。Spring 提供了多种 Bean 的配置方式，基于注解的方式是最常用的。@Component 和 @Bean 有什么区别？首先从使用上来说，@Component 是标注在类上的，而 @Bean 是标注在方法上的。@Component 告诉 Spring 这个类是一个组件，请把它注册为 Bean，而 @Bean 则告诉 Spring 请将这个方法返回的对象注册为 Bean。从控制权的角度来说，@Component 是由 Spring 自动创建和管理的。而 @Bean 则是由我们手动创建的，然后再交给 Spring 管理，我们对对象的创建过程有完全的控制权。能说一下 Bean 的生命周期吗？Bean 的生命周期可以分为 5 个主要阶段，我按照实际的执行顺序来说一下。第一个阶段是实例化。Spring 容器会根据 BeanDefinition，通过反射调用 Bean的
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 构造方法创建对象实例。如果有多个构造方法，Spring 会根据依赖注入的规则选择合适的构造方法。第二阶段是属性赋值。这个阶段 Spring 会给 Bean 的属性赋值，包括通过@Autowired、@Resource 这些注解注入的依赖对象，以及通过 @Value 注入的配置值第三阶段是初始化。这个阶段会依次执行：@PostConstruct 标注的方法InitializingBean 接口的 afterPropertiesSet 方法通过 @Bean 的 initMethod 指定的初始化方法初始化后，Spring 还会调用所有注册的 BeanPostProcessor 后置处理方法。这个阶段经常用来创建代理对象，比如 AOP 代理。第五阶段是使用 Bean。比如我们的 Controller 调用 Service，Service 调用DAO。最后是销毁阶段。当容器关闭或者 Bean 被移除的时候，会依次执行：@PreDestroy 标注的方法DisposableBean 接口的 destroy 方法通过 @Bean 的 destroyMethod 指定的销毁方法Aware 类型的接口有什么作用？Aware 
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 接口在 Spring 中是一个很有意思的设计，它们的作用是让 Bean 能够感知到 Spring 容器的一些内部组件。从设计理念来说，Aware 接口实现了一种“回调”机制。正常情况下，Bean 不应该直接依赖 Spring 容器，这样可以保持代码的独立性。但有些时候，Bean 确实需要获取容器的一些信息或者组件，Aware 接口就提供了这样一个能力。什么是自动装配？自动装配的本质就是让 Spring 容器自动帮我们完成 Bean 之间的依赖关系注入，而不需要我们手动去指定每个依赖。简单来说，就是“我们不用告诉 Spring具体怎么注入，Spring 自己会想办法找到合适的 Bean 注入进来”。自动装配的工作原理简单来说就是，Spring 容器在启动时自动扫描@ComponentScan 指定包路径下的所有类，然后根据类上的注解，比如@Autowired、@Resource 等，来判断哪些 Bean 需要被自动装配。之后分析每个 Bean 的依赖关系，在创建 Bean 的时候，根据装配规则自动找到合适的依赖 Bean，最后根据反射将这些依赖注入到目标 Bean 中Bean 的作用域有哪些Bean 的作用域决
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 定了 Bean 实例的生命周期和创建策略，singleton 是默认的作用域。整个 Spring 容器中只会有一个 Bean 实例。不管在多少个地方注入这个 Bean，拿到的都是同一个对象。生命周期和 Spring 容器相同，容器启动时创建，容器销毁时销毁。实际开发中，像 Service、Dao 这些业务组件基本都是单例的，因为单例既能节省内存，又能提高性能。当把 scope 设置为 prototype 时，每次从容器中获取 Bean 的时候都会创建一个新的实例。当需要处理一些有状态的 Bean 时会用到 prototype，比如每个订单处理器需要维护不同的状态信息如果作用于是 request，表示在 Web 应用中，每个 HTTP 请求都会创建一个新的 Bean 实例，请求结束后 Bean 就被销毁。如果作用于是 session，表示在 Web 应用中，每个 HTTP 会话都会创建一个新的 Bean 实例，会话结束后 Bean 被销毁。application 作用域表示在整个应用中只有一个 Bean 实例，类似于 singleton，但它的生命周期与 ServletContext 绑定。Spring 中的单
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 例 Bean 会存在线程安全问题吗？首先要明确一点。Spring 容器本身保证了 Bean 创建过程的线程安全，也就是说不会出现多个线程同时创建同一个单例 Bean 的情况。但是 Bean 创建完成后的使用过程，Spring 就不管了换句话说，单例 Bean 在被创建后，如果它的内部状态是可变的，那么在多线程环境下就可能会出现线程安全问题单例 Bean 的线程安全问题怎么解决呢？第一种，使用局部变量，也就是使用无状态的单例 Bean，把所有状态都通过方法参数传递：第二种，当确实需要维护线程相关的状态时，可以使用 ThreadLocal 来保存状态。ThreadLocal 可以保证每个线程都有自己的变量副本，互不干扰。第三种，如果需要缓存数据或者计数，使用 JUC 包下的线程安全类，比如说AtomicInteger、ConcurrentHashMap、CopyOnWriteArrayList 等。第四种，对于复杂的状态操作，可以使用 synchronized 或 Lock：第五种，如果 Bean 确实需要维护状态，可以考虑将其改为 prototype 作用域，这样每次注入都会创建一个新的实例，避免了多线程共享同
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个实例的问题。说说循环依赖?A 依赖 B，B 依赖 A，或者 C 依赖 C，就成了循环依赖。Spring 怎么解决循环依赖呢？Spring 通过三级缓存机制来解决循环依赖：一级缓存：存放完全初始化好的单例 Bean。二级缓存：存放正在创建但未完全初始化的 Bean 实例。三级缓存：存放 Bean 工厂对象，用于提前暴露 Bean。三级缓存解决循环依赖的过程是什么样的？实例化 Bean 时，将其早期引用放入三级缓存。其他依赖该 Bean 的对象，可以从缓存中获取其引用。初始化完成后，将 Bean 移入一级缓存。假如 A、B 两个类发生循环依赖：A 实例的初始化过程：1 、创建 A 实例，实例化的时候把 A 的对象⼯⼚放⼊三级缓存，表示 A 开始实例化了，虽然这个对象还不完整，但是先曝光出来让大家知道2 、A 注⼊属性时，发现依赖 B，此时 B 还没有被创建出来，所以去实例化 B。3 、同样，B 注⼊属性时发现依赖 A，它就从缓存里找 A 对象。依次从⼀级到三级缓存查询 A。4 、发现可以从三级缓存中通过对象⼯⼚拿到 A，虽然 A 不太完善，但是存在，就把 A 放⼊⼆级缓存，同时删除三级缓存中的 A，此时，B 
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 已经实例化并且初始化完成了，把 B 放入⼀级缓存5 、接着 A 继续属性赋值，顺利从⼀级缓存拿到实例化且初始化完成的 B 对象，A 对象创建也完成，删除⼆级缓存中的 A，同时把 A 放⼊⼀级缓存6 、最后，⼀级缓存中保存着实例化、初始化都完成的 A、B 对象。为什么要三级缓存？⼆级不⾏吗？不行，主要是为了 ⽣成代理对象。如果是没有代理的情况下，使用二级缓存解决循环依赖也是 OK 的。但是如果存在代理，三级没有问题，二级就不行了。因为三级缓存中放的是⽣成具体对象的匿名内部类，获取 Object 的时候，它可以⽣成代理对象，也可以返回普通对象。使⽤三级缓存主要是为了保证不管什么时候使⽤的都是⼀个对象。假设只有⼆级缓存的情况，往⼆级缓存中放的显示⼀个普通的 Bean 对象，Bean初始化过程中，通过 BeanPostProcessor 去⽣成代理对象之后，覆盖掉⼆级缓存中的普通 Bean 对象，那么可能就导致取到的 Bean 对象不一致了。@Autowired 的实现原理？实现@Autowired 的关键是：AutowiredAnnotationBeanPostProcessor在 Bean 的初始化阶段，会通过 
2025-08-11 10:56:08.178 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: Bean 后置处理器来进行一些前置和后置的处理。实现@Autowired 的功能，也是通过后置处理器来完成的。这个后置处理器就是AutowiredAnnotationBeanPostProcessor。Spring 在创建 bean 的过程中，最终会调用到 doCreateBean()方法，在doCreateBean()方法中会调用 populateBean()方法，来为 bean 进行属性填充，完成自动装配等工作。在 populateBean()方法中一共调用了两次后置处理器，第一次是为了判断是否需要属性填充，如果不需要进行属性填充，那么就会直接进行 return，如果需要进行属性填充，那么方法就会继续向下执行，后面会进行第二次后置处理器的调用，这个时候，就会调用到 AutowiredAnnotationBeanPostProcessor 的postProcessPropertyValues()方法，在该方法中就会进行@Autowired 注解的解析，然后实现自动装配。说说什么是 AOP？AOP，也就是面向切面编程，简单点说，AOP 就是把一些业务逻辑中的相同代码抽取到一个独立的模块中，让业务逻辑更加清爽。
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 业务代码不再关心这些通用逻辑，只需要关心自己的业务实现，这样就实现了业务逻辑和通用逻辑的分离。AOP 有哪些核心概念？切面（Aspect）：类是对物体特征的抽象，切面就是对横切关注点的抽象连接点（Join Point）：被拦截到的点，因为 Spring 只支持方法类型的连接点，所以在 Spring 中，连接点指的是被拦截到的方法，实际上连接点还可以是字段或者构造方法切点（Pointcut）：对连接点进行拦截的定位通知（Advice）：指拦截到连接点之后要执行的代码，也可以称作增强目标对象 （Target）：代理的目标对象引介（introduction）：一种特殊的增强，可以动态地为类添加一些属性和方法织入（Weabing）：织入是将增强添加到目标类的具体连接点上的过程。Spring AOP 发生在什么时候？Spring AOP 基于运行时代理机制，这意味着 Spring AOP 是在运行时通过动态代理生成的，而不是在编译时或类加载时生成的。在 Spring 容器初始化 Bean的过程中，Spring AOP 会检查 Bean 是否需要应用切面。如果需要，Spring 会为该 Bean 创建一个代理对象，并在代
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 理对象中织入切面逻辑。这一过程发生在Spring 容器的后处理器（BeanPostProcessor）阶段。简单总结一下 AOPAOP，也就是面向切面编程，是一种编程范式，旨在提高代码的模块化。比如说可以将日志记录、事务管理等分离出来，来提高代码的可重用性。AOP 的核心概念包括切面（Aspect）、连接点（Join Point）、通知（Advice）、切点（Pointcut）和织入（Weaving）等。① 像日志打印、事务管理等都可以抽离为切面，可以声明在类的方法上。像@Transactional 注解，就是一个典型的 AOP 应用，它就是通过 AOP 来实现事务管理的。我们只需要在方法上添加 @Transactional 注解，Spring 就会在方法执行前后添加事务管理的逻辑。② Spring AOP 是基于代理的，它默认使用 JDK 动态代理和 CGLIB 代理来实现AOP。③ Spring AOP 的织入方式是运行时织入，而 AspectJ 支持编译时织入、类加载时织入。AOP 的使用场景有哪些？AOP 的使用场景有很多，比如说日志记录、事务管理、权限控制、性能监控等。第一步，自定义注解作为切点第二
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 步，配置 AOP 切面：@Aspect：标识切面@Pointcut：设置切点，这里以自定义注解为切点@Around：环绕切点，打印方法签名和执行时间第三步，在使用的地方加上自定义注解第四步，当接口被调用时，就可以看到对应的执行日志。说说 JDK 动态代理和 CGLIB 代理？AOP 是通过动态代理实现的，代理方式有两种：JDK 动态代理和 CGLIB 代理。①、JDK 动态代理是基于接口的代理，只能代理实现了接口的类。使用 JDK 动态代理时，Spring AOP 会创建一个代理对象，该代理对象实现了目标对象所实现的接口，并在方法调用前后插入横切逻辑。优点：只需依赖 JDK 自带的 java.lang.reflect.Proxy 类，不需要额外的库；缺点：只能代理接口，不能代理类本身CGLIB 动态代理是基于继承的代理，可以代理没有实现接口的类。使用 CGLIB 动态代理时，Spring AOP 会生成目标类的子类，并在方法调用前后插入横切逻辑优点：可以代理没有实现接口的类，灵活性更高；缺点：需要依赖 CGLIB 库，创建代理对象的开销相对较大。说说 Spring AOP 和 AspectJ AOP 区别?说
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 说 AOP 和反射的区别？（补充）反射：用于检查和操作类的方法和字段，动态调用方法或访问字段。反射是 Java提供的内置机制，直接操作类对象。动态代理：通过生成代理类来拦截方法调用，通常用于 AOP 实现。动态代理使用反射来调用被代理的方法。反射：运行时操作类的元信息的底层能力动态代理：基于反射实现方法拦截的设计模式Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，Spring 是无法提供事务功能的。Spring 只提供统一事务管理接口，具体实现都是由各数据库自己实现，数据库事务的提交和回滚是通过数据库自己的事务机制实现Spring 事务的种类？在 Spring 中，事务管理可以分为两大类：声明式事务管理和编程式事务管理。介绍一下编程式事务管理？编程式事务可以使用 TransactionTemplate 和 PlatformTransactionManager来实现，需要显式执行事务。允许我们在代码中直接控制事务的边界，通过编程方式明确指定事务的开始、提交和回滚.我们使用了 TransactionTemplate 来实现编程式事务，通过 execute 方法来执行事务，这样就可以在方法
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部实现事务的控制。介绍一下声明式事务管理？声明式事务是建立在 AOP 之上的。其本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前启动一个事务，在目标方法执行完之后根据执行情况提交或者回滚事务。相比较编程式事务，优点是不需要在业务逻辑代码中掺杂事务管理的代码，Spring 推荐通过 @Transactional 注解的方式来实现声明式事务管理，也是日常开发中最常用的。不足的地方是，声明式事务管理最细粒度只能作用到方法级别，无法像编程式事务那样可以作用到代码块级别。说说两者的区别？编程式事务管理：需要在代码中显式调用事务管理的 API 来控制事务的边界，比较灵活，但是代码侵入性较强，不够优雅。声明式事务管理：这种方式使用 Spring 的 AOP 来声明事务，将事务管理代码从业务代码中分离出来。优点是代码简洁，易于维护。但缺点是不够灵活，只能在预定义的方法上使用事务说说 Spring 的事务隔离级别？好，事务的隔离级别定义了一个事务可能受其他并发事务影响的程度。SQL 标准定义了四个隔离级别，Spring 都支持，并且提供了对应的机制来配置它们，定义在 
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: TransactionDefinition 接口中。①、ISOLATION_DEFAULT：使用数据库默认的隔离级别（你们爱咋咋滴 ），MySQL默认的是可重复读，Oracle 默认的读已提交。②、ISOLATION_READ_UNCOMMITTED：读未提交，允许事务读取未被其他事务提交的更改。这是隔离级别最低的设置，可能会导致“脏读”问题。③、ISOLATION_READ_COMMITTED：读已提交，确保事务只能读取已经被其他事务提交的更改。这可以防止“脏读”，但仍然可能发生“不可重复读”和“幻读”问题。④、ISOLATION_REPEATABLE_READ：可重复读，确保事务可以多次从一个字段中读取相同的值，即在这个事务内，其他事务无法更改这个字段，从而避免了“不可重复读”，但仍可能发生“幻读”问题。⑤、ISOLATION_SERIALIZABLE：串行化，这是最高的隔离级别，它完全隔离了事务，确保事务序列化执行，以此来避免“脏读”、“不可重复读”和“幻读”问题，但性能影响也最大。Spring 的事务传播机制？事务的传播机制定义了方法在被另一个事务方法调用时的事务行为，这些行为定义了事务的边界和事务上
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 下文如何在方法调用链中传播。Spring 的默认传播行为是 PROPAGATION_REQUIRED，即如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。事务传播机制是使用ThreadLocal 实现的，所以，如果调用的方法是在新线程中，事务传播会失效。如果在 protected、private 方法上使用@Transactional，这些事务注解将不会生效，原因：Spring 默认使用基于 JDK 的动态代理（当接口存在时）或基于CGLIB 的代理（当只有类时）来实现事务。这两种代理机制都只能代理公开的方法。声明式事务实现原理了解吗？Spring 的声明式事务管理是通过 AOP（面向切面编程）和代理机制实现的。第一步，在 Bean 初始化阶段创建代理对象：Spring 容器在初始化单例 Bean 的时候，会遍历所有的 BeanPostProcessor 实现类，并执行其 postProcessAfterInitialization 方法。在执行 postProcessAfterInitialization 方法时会遍历容器中所有的切面，查找与当前 Bean 匹配的切面，这里会获取事务的属
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性切面，也就是@Transactional 注解及其属性值。然后根据得到的切面创建一个代理对象，默认使用 JDK 动态代理创建代理，如果目标类是接口，则使用 JDK 动态代理，否则使用 Cglib。第二步，在执行目标方法时进行事务增强操作：当通过代理对象调用 Bean 方法的时候，会触发对应的 AOP 增强拦截器，声明式事务是一种环绕增强，对应接口为 MethodInterceptor，事务增强对该接口的实现为 TransactionInterceptor，@Transactional 应用在非 public 修饰的方法上如果 Transactional 注解应用在非 public 修饰的方法上，Transactional 将会失效。Spring MVC 的核心组件？DispatcherServlet：前置控制器，是整个流程控制的核心，控制其他组件的执行，进行统一调度，降低组件之间的耦合性，相当于总指挥。Handler：处理器，完成具体的业务逻辑，相当于 Servlet 或 Action。HandlerMapping：DispatcherServlet 接收到请求之后，通过 HandlerMapping将不同
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的请求映射到不同的 Handler。HandlerInterceptor：处理器拦截器，是一个接口，如果需要完成一些拦截处理，可以实现该接口。HandlerExecutionChain：处理器执行链，包括两部分内容：Handler 和HandlerInterceptor（系统会有一个默认的 HandlerInterceptor，如果需要额外设置拦截，可以添加拦截器）。HandlerAdapter：处理器适配器，Handler 执行业务方法之前，需要进行一系列的操作，包括表单数据的验证、数据类型的转换、将表单数据封装到 JavaBean 等，这些操作都是由 HandlerApater 来完成，开发者只需将注意力集中业务逻辑的处理上，DispatcherServlet 通过 HandlerAdapter 执行不同的 Handler。ModelAndView：装载了模型数据和视图信息，作为 Handler 的处理结果，返回给 DispatcherServlet。ViewResolver：视图解析器，DispatcheServlet 通过它将逻辑视图解析为物理视图，最终将渲染结果响应给客户端。Spring MVC 的
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 工作流程？首先，客户端发送请求，DispatcherServlet 拦截并通过 HandlerMapping 找到对应的控制器。DispatcherServlet 使用 HandlerAdapter 调用控制器方法，执行具体的业务逻辑，返回一个 ModelAndView 对象。然后 DispatcherServlet 通过 ViewResolver 解析视图。最后，DispatcherServlet 渲染视图并将响应返回给客户端①、发起请求：客户端通过 HTTP 协议向服务器发起请求。②、前端控制器：这个请求会先到前端控制器 DispatcherServlet，它是整个流程的入口点，负责接收请求并将其分发给相应的处理器。③、处理器映射：DispatcherServlet 调用 HandlerMapping 来确定哪个Controller 应该处理这个请求。通常会根据请求的 URL 来确定。④、处理器适配器：一旦找到目标 Controller，DispatcherServlet 会使用HandlerAdapter 来调用 Controller 的处理方法。⑤、执行处理器：Controller 处理请求，处理完后
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 返回一个 ModelAndView 对象，其中包含模型数据和逻辑视图名。⑥、视图解析器：DispatcherServlet 接收到 ModelAndView 后，会使用ViewResolver 来解析视图名称，找到具体的视图页面。⑦、渲染视图：视图使用模型数据渲染页面，生成最终的页面内容。⑧、响结果：DispatcherServlet 将视图结果返回给客户端。Spring MVC 虽然整体流程复杂，但是实际开发中很简单，大部分的组件不需要我们开发人员创建和管理，真正需要处理的只有 Controller 、View 、Model。在前后端分离的情况下，步骤 ⑥、⑦、⑧ 会略有不同，后端通常只需要处理数据，并将 JSON 格式的数据返回给前端就可以了，而不是返回完整的视图页面。这个 Handler 是什么东西啊？为什么还需要 HandlerAdapterHandler 一般就是指 Controller，Controller 是 Spring MVC 的核心组件，负责处理请求，返回响应。Spring MVC 允许使用多种类型的处理器。不仅仅是标准的@Controller 注解的类，还可以是实现了特定接口的其他类（如
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块:  HttpRequestHandler 或SimpleControllerHandlerAdapter 等）。这些处理器可能有不同的方法签名和交互方式。HandlerAdapter 的主要职责就是调用 Handler 的方法来处理请求，并且适配不同类型的处理器。HandlerAdapter 确保 DispatcherServlet 可以以统一的方式调用不同类型的处理器，无需关心具体的执行细节。SpringMVC Restful 风格的接口的流程是什么样的呢？我们都知道 Restful 接口，响应格式是 json，这就用到了一个常用注解：@ResponseBody加入了这个注解后，整体的流程上和使用 ModelAndView 大体上相同，但是细节上有一些不同：客户端向服务端发送一次请求，这个请求会先到前端控制器 DispatcherServletDispatcherServlet 接收到请求后会调用 HandlerMapping 处理器映射器。由此得知，该请求该由哪个 Controller 来处理DispatcherServlet 调用 HandlerAdapter 处理器适配器，告诉处理器适配器应该要去执行哪
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个 ControllerController 被封装成了 ServletInvocableHandlerMethod，HandlerAdapter 处理器适配器去执行 invokeAndHandle 方法，完成对 Controller 的请求处理HandlerAdapter 执行完对 Controller 的请求，会调用HandlerMethodReturnValueHandler 去处理返回值，主要的过程：5.1. 调用 RequestResponseBodyMethodProcessor，创建ServletServerHttpResponse（Spring 对原生 ServerHttpResponse 的封装）实例5.2.使用 HttpMessageConverter 的 write 方法，将返回值写入ServletServerHttpResponse 的 OutputStream 输出流中5.3.在写入的过程中，会使用 JsonGenerator（默认使用 Jackson 框架）对返回值进行 Json 序列化执行完请求后，返回的 ModealAndView 为 null，ServletServerHtt
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: pResponse里也已经写入了响应，所以不用关心 View 的处理介绍一下 SpringBoot，有哪些优点？Spring Boot 提供了一套默认配置，它通过约定大于配置的理念，来帮助我们快速搭建 Spring 项目骨架Spring Boot 的优点非常多，比如说：Spring Boot 内嵌了 Tomcat、Jetty、Undertow 等容器，直接运行 jar 包就可以启动项目。Spring Boot 内置了 Starter 和自动装配，避免繁琐的手动配置。例如，如果项目中添加了 spring-boot-starter-web，Spring Boot 会自动配置 Tomcat 和Spring MVC。Spring Boot 内置了 Actuator 和 DevTools，便于调试和监控Spring Boot 常用注解有哪些？@SpringBootApplication：Spring Boot 应用的入口，用在启动类上。还有一些 Spring 框架本身的注解，比如 @Component、@RestController、@Service、@ConfigurationProperties、@Transact
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ional 等。SpringBoot 自动配置原理了解吗？在 Spring 中，自动装配是指容器利用反射技术，根据 Bean 的类型、名称等自动注入所需的依赖。在 Spring Boot 中，开启自动装配的注解是@EnableAutoConfiguration。Spring Boot 为了进一步简化，直接通过 @SpringBootApplication 注解一步搞定，该注解包含了 @EnableAutoConfiguration 注解。SpringBoot 的自动装配机制主要通过 @EnableAutoConfiguration 注解实现，这个注解是 @SpringBootApplication 注解的一部分，后者是一个组合注解，包括 @SpringBootConfiguration、@ComponentScan 和@EnableAutoConfiguration。@EnableAutoConfiguration 注解通过AutoConfigurationImportSelector 类来加载自动装配类，这个类实现了ImportSelector 接口的 selectImports 方法，该方法负责获取所有符
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。Spring Boot 的自动装配原理依赖于 Spring 框架的依赖注入和条件注册，通过这种方式，Spring Boot 能够智能地配置 bean，并且只有当这些 bean 实际需要时才会被创建和配置。Spring Boot Starter 的原理了解吗？Spring Boot Starter 主要通过起步依赖和自动配置机制来简化项目的构建和配置过程。起步依赖是 Spring Boot 提供的一组预定义依赖项，它们将一组相关的库和模块打包在一起。比如 spring-boot-starter-web 就包含了 Spring MVC、Tomcat和 Jackson 等依赖。自动配置机制是 Spring Boot 的核心特性，通过自动扫描类路径下的类、资源文件和配置文件，自动创建和配置应用程序所需的 Bean 和组件。为什么使用 Spring Boot？Spring Boot 解决了传统 Spring 开发的三大痛点：简化配置：自动装配 + 起步依赖，告别 XML 配置地狱快速启动：内嵌 Tomcat/Jetty，一键启动独立运行应用生产就绪：Actua
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: tor 提供健康检查、监控等运维能力@Import 的作用实现模块化配置导入，三种用法导入普通配置类：@Import(MyConfig.class)导入 ImportSelector 实现（自动装配核心）：@Import(AutoConfigurationImportSelector.class)导入 ImportBeanDefinitionRegistrar 实现（动态注册 Bean）Spring Boot 启动原理了解吗？Spring Boot 的启动由 SpringApplication 类负责：第一步，创建 SpringApplication 实例，负责应用的启动和初始化；第二步，从 application.yml 中加载配置文件和环境变量；第三步，创建上下文环境 ApplicationContext，并加载 Bean，完成依赖注入；第四步，启动内嵌的 Web 容器。第五步，发布启动完成事件 ApplicationReadyEvent，并调用ApplicationRunner 的 run 方法完成启动后的逻辑。了解@SpringBootApplication 注解吗？@SpringBootApplic
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ation 是 Spring Boot 的核心注解，经常用于主类上，作为项目启动入口的标识。它是一个组合注解：@SpringBootConfiguration：继承自 @Configuration，标注该类是一个配置类，相当于一个 Spring 配置文件。@EnableAutoConfiguration：告诉 Spring Boot 根据 pom.xml 中添加的依赖自动配置项目。例如，如果 spring-boot-starter-web 依赖被添加到项目中，Spring Boot 会自动配置 Tomcat 和 Spring MVC。@ComponentScan：扫描当前包及其子包下被@Component、@Service、@Controller、@Repository 注解标记的类，并注册为 Spring Bean为什么 Spring Boot 在启动的时候能够找到 main 方法上的@SpringBootApplication 注解？Spring Boot 在启动时能够找到主类上的@SpringBootApplication 注解，是因为它利用了 Java 的反射机制和类加载机制，结合 Spring 框架
2025-08-11 10:56:08.179 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 内部的一系列处理流程Spring Boot 利用 Java 反射机制来读取传递给 run 方法的类（MyApplication.class）。它会检查这个类上的注解，包括@SpringBootApplication@SpringBootApplication 是一个组合注解，它里面的@ComponentScan 注解可以指定要扫描的包路径，默认扫描启动类所在包及其子包下的所有组件。比如说带有 @Component、@Service、@Controller、@Repository 等注解的类都会被 Spring Boot 扫描到，并注册到 Spring 容器中。如果需要自定义包扫描路径，可以在@SpringBootApplication 注解上添加@ComponentScan 注解，指定要扫描的包路径。这种方式会覆盖默认的包扫描路径，只扫描 com.github.paicoding.forum 包及其子包下的所有组件。SpringBoot 和 SpringMVC 的区别？（补充）Spring MVC 是基于 Spring 框架的一个模块，提供了一种Model-View-Controller（模型-视图-控制器）
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 的开发模式。Spring Boot 旨在简化 Spring 应用的配置和部署过程，提供了大量的自动配置选项，以及运行时环境的内嵌 Web 服务器，这样就可以更快速地开发一个SpringMVC 的 Web 项目。Spring Boot 和 Spring 有什么区别？（补充）Spring Boot 是 Spring Framework 的一个扩展，提供了一套快速配置和开发的机制，可以帮助我们快速搭建 Spring 项目的骨架，提高生产效率。对 SpringCloud 了解多少？Spring Cloud 是一个基于 Spring Boot，提供构建分布式系统和微服务架构的工具集。用于解决分布式系统中的一些常见问题，如配置管理、服务发现、负载均衡等等。微服务化的核心就是将传统的一站式应用，根据业务拆分成一个一个的服务，彻底地去耦合，每一个微服务提供单个业务功能的服务，一个服务做一件事情，从技术角度看就是一种小而独立的处理过程，类似进程的概念，能够自行单独启动或销毁，拥有自己独立的数据库。微服务架构主要要解决哪些问题？服务很多，客户端怎么访问，如何提供对外网关?这么多服务，服务之间如何通信? HTTP 还是 RPC?这
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 么多服务，如何治理? 服务的注册和发现。服务挂了怎么办？熔断机制SpringTask 了解吗？SpringTask 是 Spring 框架提供的一个轻量级的任务调度框架，它允许我们开发者通过简单的注解来配置和管理定时任务@Scheduled：最常用的注解，用于标记方法为计划任务的执行点。技术派实战项目中，就使用该注解来定时刷新 sitemap.xml：用 SpringTask资源占用太高，有什么其他的方式解决？（补充）第一，使用消息队列，如 RabbitMQ、Kafka、RocketMQ 等，将任务放到消息队列中，然后由消费者异步处理这些任务。第二，使用数据库调度器（如 Quartz）Spring Cache 了解吗？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过统一的接口来支持多种缓存实现（如 Redis、Caffeine 等）。Spring Cache 和 Redis 有什么区别？Spring Cache 是 Spring 框架提供的一个缓存抽象，它通过注解来实现缓存管理，支持多种缓存实现（如 Redis、Caffeine 等）。Redis 是一个分布式的缓存中间件，支持多种数
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 据类型（如 String、Hash、List、Set、ZSet），还支持持久化、集群、主从复制等。Spring Cache 适合用于单机、轻量级和短时缓存场景，能够通过注解轻松控制缓存管理。Redis 是一种分布式缓存解决方案，支持多种数据结构和高并发访问，适合分布式系统和高并发场景，可以提供数据持久化和多种淘汰策略。在实际开发中，Spring Cache 和 Redis 可以结合使用，Spring Cache 提供管理缓存的注解，而 Redis 则作为分布式缓存的实现，提供共享缓存支持。有了 Redis 为什么还需要 Spring Cache？虽然 Redis 非常强大，但 Spring Cache 提供了一层缓存抽象，简化了缓存的管理。我们可以直接在方法上通过注解来实现缓存逻辑，减少了手动操作 Redis 的代码量。Spring Cache 还能灵活切换底层缓存实现。此外，Spring Cache 支持事务性缓存和条件缓存，便于在复杂场景中确保数据一致性。说说什么是 MyBatis?Mybatis 是一个半 ORM（对象关系映射）框架，它内部封装了 JDBC，开发时只需要关注 SQL 语句本身，不需要花费
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 精力去处理加载驱动、创建连接、创建statement 等繁杂的过程。程序员直接编写原生态 sql，可以严格控制 sql 执行性能，灵活度高。MyBatis 可以使用 XML 或注解来配置和映射原生信息，将 POJO 映射成数据库中的记录，避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。SQL 语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写 SQL语句的功底有一定要求SQL 语句依赖于数据库，导致数据库移植性差，不能随意更换数据库ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单 Java 对象（POJO）的映射关系的技术。简单来说，ORM 是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中JDBC 编程有哪些不足之处，MyBatis 是如何解决的？1、数据连接创建、释放频繁造成系统资源浪费从而影响系统性能，在mybatis-config.xml 中配置数据链接池，使用连接池统一管理数据库连接。2、sql 语句写在代码中造成代码不易维护，将 sql 语句配置在 XXXXmapper.xm
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: l文件中与 java 代码分离。3、向 sql 语句传参数麻烦，因为 sql 语句的 where 条件不一定，可能多也可能少，占位符需要和参数一一对应。Mybatis 自动将 java 对象映射至 sql 语句。4、对结果集解析麻烦，sql 变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成 pojo 对象解析比较方便。Mybatis 自动将 sql 执行结果映射至 java 对象。Hibernate 和 MyBatis 有什么区别？不同点1）映射关系MyBatis 是一个半自动映射的框架，配置 Java 对象与 sql 语句执行结果的对应关系，多表关联关系配置简单Hibernate 是一个全表映射的框架，配置 Java 对象与数据库表的对应关系，多表关联关系配置复杂2）SQL 优化和移植性Hibernate 对 SQL 语句封装，提供了日志、缓存、级联（级联比 MyBatis 强大）等特性，此外还提供 HQL（Hibernate Query Language）操作数据库，数据库无关性支持好，但会多消耗性能。如果项目需要支持多种数据库，代码开发量少，但 SQL 语句优化困难。MyBatis 需要
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 手动编写 SQL，支持动态 SQL、处理列表、动态生成表名、支持存储过程。开发工作量相对大些。直接使用 SQL 语句操作数据库，不支持数据库无关性，但 sql 语句优化容易MyBatis 使用过程？生命周期？MyBatis 基本使用的过程大概可以分为这么几步：1）创建 SqlSessionFactory2）通过 SqlSessionFactory 创建 SqlSessionSqlSession（会话）可以理解为程序和数据库之间的桥梁3）通过 sqlsession 执行数据库操作，可以通过 SqlSession 实例来直接执行已映射的 SQL 语句：4）调用 session.commit()提交事务5）调用 session.close()关闭会话说说 MyBatis 生命周期？SqlSessionFactoryBuilder一旦创建了 SqlSessionFactory，就不再需要它了。 因此SqlSessionFactoryBuilder 实例的生命周期只存在于方法的内部。SqlSessionFactorySqlSessionFactory 是用来创建 SqlSession 的，相当于一个数据库连接池，每次创
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 建 SqlSessionFactory 都会使用数据库资源，多次创建和销毁是对资源的浪费。所以 SqlSessionFactory 是应用级的生命周期，而且应该是单例的。SqlSessionSqlSession 相当于 JDBC 中的 Connection，SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的生命周期是一次请求或一个方法。Mapper映射器是一些绑定映射语句的接口。映射器接口的实例是从 SqlSession 中获得的，它的生命周期在 sqlsession 事务方法之内，一般会控制在方法级。在 mapper 中如何传递多个参数？#{}和${}的区别?①、当使用 #{} 时，MyBatis 会在 SQL 执行之前，将占位符替换为问号 ?，并使用参数值来替代这些问号。由于 #{} 使用了预处理，所以能有效防止 SQL 注入，确保参数值在到达数据库之前被正确地处理和转义。<select id="selectUser" resultType="User">SELECT * FROM users WHERE id = #{id}</select>②、当使用 ${} 时，参数的值会
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 直接替换到 SQL 语句中去，而不会经过预处理。这就存在 SQL 注入的风险，因为参数值会直接拼接到 SQL 语句中，假如参数值是 1 or 1=1，那么 SQL 语句就会变成 SELECT * FROM users WHERE id = 1 or1=1，这样就会导致查询出所有用户的结果。${} 通常用于那些不能使用预处理的场合，比如说动态表名、列名、排序等，要提前对参数进行安全性校验。<select id="selectUsersByOrder" resultType="User">SELECT * FROM users ORDER BY ${columnName} ASC</select>模糊查询 like 语句该怎么写?CONCAT('%',#{question},'%') 使用 CONCAT()函数，（推荐 ✨）说说 Mybatis 的一级、二级缓存？一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为SqlSession，各个 SqlSession 之间的缓存相互隔离，当 Session flush 或close 之后，该 SqlSession 中的所有 Ca
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: che 就将清空，MyBatis 默认打开一级缓存。二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同之处在于其存储作用域为 Mapper(Namespace)，可以在多个 SqlSession之间共享，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现 Serializable 序列化接口(可用来保存对象的状态),可在它的映射文件中配置。能说说 MyBatis 的工作原理吗？按工作原理，可以分为两大步：生成会话工厂、会话运行构造会话工厂也可以分为两步：获取配置获取配置这一步经过了几步转化，最终由生成了一个配置类 Configuration 实例，这个配置类实例非常重要，主要作用包括：读取配置文件，包括基础配置文件和映射文件初始化基础配置，比如 MyBatis 的别名，还有其它的一些重要的类对象，像插件、映射器、ObjectFactory 等等提供一个单例，作为会话工厂构建的重要参数它的构建过程也会初始化一些环境变量，比如数据源构建 SqlSessionFactorySqlSessionFactory 只是一
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 个接口，构建出来的实际上是它的实现类的实例，一般我们用的都是它的实现类 DefaultSqlSessionFactory会话运行是 MyBatis 最复杂的部分，它的运行离不开四大组件的配合：Executor（执行器）Executor 起到了至关重要的作用，SqlSession 只是一个门面，相当于客服，真正干活的是是 Executor，就像是默默无闻的工程师。它提供了相应的查询和更新方法，以及事务方法StatementHandler（数据库会话器）StatementHandler，顾名思义，处理数据库会话的。我们以 SimpleExecutor 为例，看一下它的查询方法，先生成了一个 StatementHandler 实例，再拿这个handler 去执行 query。ParameterHandler （参数处理器）PreparedStatementHandler 里对 sql 进行了预编译处理ResultSetHandler（结果处理器）我们前面也看到了，最后的结果要通过 ResultSetHandler 来进行处理，handleResultSets 这个方法就是用来包装结果集的。Mybatis 为我们提供
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 了一个 DefaultResultSetHandler，通常都是用这个实现类去进行结果的处理的。读取 MyBatis 配置文件——mybatis-config.xml 、加载映射文件——映射文件即 SQL 映射文件，文件中配置了操作数据库的 SQL 语句。最后生成一个配置对象。构造会话工厂：通过 MyBatis 的环境等配置信息构建会话工厂SqlSessionFactory。创建会话对象：由会话工厂创建 SqlSession 对象，该对象中包含了执行 SQL 语句的所有方法。Executor 执行器：MyBatis 底层定义了一个 Executor 接口来操作数据库，它将根据 SqlSession 传递的参数动态地生成需要执行的 SQL 语句，同时负责查询缓存的维护。StatementHandler：数据库会话器，串联起参数映射的处理和运行结果映射的处理。参数处理：对输入参数的类型进行处理，并预编译。结果处理：对返回结果的类型进行处理，根据对象映射规则，返回相应的对象。MyBatis 的功能架构是什么样的？我们一般把 Mybatis 的功能架构分为三层：API 接口层：提供给外部使用的接口 API，开发人员通
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 过这些本地 API 来操纵数据库。接口层一接收到调用请求就会调用数据处理层来完成具体的数据处理。数据处理层：负责具体的 SQL 查找、SQL 解析、SQL 执行和执行结果映射处理等。它主要的目的是根据调用的请求完成一次数据库操作。基础支撑层：负责最基础的功能支撑，包括连接管理、事务管理、配置加载和缓存处理，这些都是共用的东西，将他们抽取出来作为最基础的组件。为上层的数据处理层提供最基础的支撑Mybatis 都有哪些 Executor 执行器？Mybatis 有三种基本的 Executor 执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。SimpleExecutor：每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。ReuseExecutor：执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map<String, Statement>内，供下一次使用。简言之，就是重复使
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 用Statement 对象。BatchExecutor：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。说说 JDBC 的执行步骤？Java 数据库连接（JDBC）是一个用于执行 SQL 语句的 Java API，它为多种关系数据库提供了统一访问的机制。使用 JDBC 操作数据库通常涉及以下步骤：在与数据库建立连接之前，首先需要通过 Class.forName()方法加载对应的数据库驱动。这一步确保 JDBC 驱动注册到了 DriverManager 类中。Class.forName("com.mysql.cj.jdbc.Driver");第二步，建立数据库连接使用 DriverManager.getConnection()方法建立到数据库的连接。这一步需要提供数据库 URL、用户名和密码作为参数C
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: onnection conn = DriverManager.getConnection("jdbc:mysql://localhost:3306/databaseName", "username","password");第三步，创建 Statement 对象通过建立的数据库连接对象 Connection 创建 Statement、PreparedStatement或 CallableStatement 对象，用于执行 SQL 语句Statement stmt = conn.createStatement();第四步，执行 SQL 语句使用 Statement 或 PreparedStatement 对象执行 SQL 语句。执行查询（SELECT）语句时，使用 executeQuery()方法，它返回 ResultSet 对象；执行更新（INSERT、UPDATE、DELETE）语句时，使用 executeUpdate()方法，它返回一个整数表示受影响的行数。ResultSet rs = stmt.executeQuery("SELECT * FROM tableName");int affectedRow
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: s = stmt.executeUpdate("UPDATE tableName SET column ='value' WHERE condition");第五步，处理结果集如果执行的是查询操作，需要处理 ResultSet 对象来获取数据第六步，关闭资源最后，需要依次关闭 ResultSet、Statement 和 Connection 等资源，释放数据库连接等资源创建连接拿到的是什么对象？在 JDBC 的执行步骤中，创建连接后拿到的对象是 java.sql.Connection 对象。这个对象是 JDBC API 中用于表示数据库连接的接口，它提供了执行 SQL 语句、管理事务等一系列操作的方法。Connection 对象代表了应用程序和数据库的一个连接会话。通过调用 DriverManager.getConnection()方法并传入数据库的 URL、用户名和密码等信息来获得这个对象。一旦获得 Connection 对象，就可以使用它来创建执行 SQL 语句的 Statement、PreparedStatement 和 CallableStatement 对象，以及管理事务等。什么是 SQL 注入？如
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 何防止 SQL 注入？SQL 注入是一种代码注入技术，通过在输入字段中插入专用的 SQL 语句，从而欺骗数据库执行恶意 SQL，以获取敏感数据、修改数据，或者删除数据等。为了防止 SQL 注入，可以采取以下措施：①、使用参数化查询使用参数化查询，即使用 PreparedStatement 对象，通过 setXxx 方法设置参数值，而不是通过字符串拼接 SQL 语句。这样可以有效防止 SQL 注入。②、限制用户输入对用户输入进行验证和过滤，只允许输入预期的数据，不允许输入特殊字符或SQL 关键字。③、使用 ORM 框架比如，在 MyBatis 中，使用#{}占位符来代替直接拼接 SQL 语句，MyBatis 会自动进行参数化处理。<select id="selectUser" resultType="User">SELECT * FROM users WHERE username = #{userName}</select>分布式说说 CAP 原则？、CAP 原则又称 CAP 定理，指的是在一个分布式系统中，Consistency（一致性）、Availability（可用性）、Partition toleran
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ce（分区容错性）这 3 个基本需求，最多只能同时满足其中的 2 个。为什么 CAP 不可兼得呢？首先对于分布式系统，分区是必然存在的，所谓分区指的是分布式系统可能出现的字区域网络不通，成为孤立区域的的情况。那么分区容错性（P）就必须要满足，因为如果要牺牲分区容错性，就得把服务和资源放到一个机器，或者一个“同生共死”的集群，那就违背了分布式的初衷。假如现在有这样的场景：用户访问了 N1，修改了 D1 的数据。用户再次访问，请求落在了 N2。此时 D1 和 D2 的数据不一致。接下来：保证一致性：此时 D1 和 D2 数据不一致，要保证一致性就不能返回不一致的数据，可用性无法保证。保证可用性：立即响应，可用性得到了保证，但是此时响应的数据和 D1 不一致，一致性无法保证。所以，可以看出，分区容错的前提下，一致性和可用性是矛盾的。ASE 理论了解吗？BASE（Basically Available、Soft state、Eventual consistency）是基于 CAP 理论逐步演化而来的，核心思想是即便不能达到强一致性（Strong consistency），也可以根据应用特点采用适当的方式来达到最终一致
2025-08-11 10:56:08.180 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 性（Eventual consistency）的效果。BASE 的主要含义：Basically Available（基本可用）什么是基本可用呢？假设系统出现了不可预知的故障，但还是能用，只是相比较正常的系统而言，可能会有响应时间上的损失，或者功能上的降级。Soft State（软状态）什么是硬状态呢？要求多个节点的数据副本都是一致的，这是一种“硬状态”。软状态也称为弱状态，相比较硬状态而言，允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。Eventually Consistent（最终一致性）上面说了软状态，但是不应该一直都是软状态。在一定时间后，应该到达一个最终的状态，保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间取决于网络延时、系统负载、数据复制方案设计等等因素有哪些分布式锁的实现方案呢？常见的分布式锁实现方案有三种：MySQL 分布式锁、ZooKepper 分布式锁、Redis分布式锁。MySQL 分布式锁如何实现呢？用数据库实现分布式锁比较简单，就是创建一张锁表，数据库对字段作唯一性约束。加锁的时候，在锁表中增加一条记录
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 即可；释放锁的时候删除记录就行。如果有并发请求同时提交到数据库，数据库会保证只有一个请求能够得到锁。这种属于数据库 IO 操作，效率不高，而且频繁操作会增大数据库的开销，因此这种方式在高并发、高性能的场景中用的不多。ZooKeeper 如何实现分布式锁？ZooKeeper 也是常见分布式锁实现方法。ZooKeeper 的数据节点和文件目录类似，例如有一个 lock 节点，在此节点下建立子节点是可以保证先后顺序的，即便是两个进程同时申请新建节点，也会按照先后顺序建立两个节点。所以我们可以用此特性实现分布式锁。以某个资源为目录，然后这个目录下面的节点就是我们需要获取锁的客户端，每个服务在目录下创建节点，如果它的节点，序号在目录下最小，那么就获取到锁，否则等待。释放锁，就是删除服务创建的节点。基于 Redis 的分布式锁核心思想： 利用 Redis 单线程执行命令的特性以及其丰富的数据结构和命令（尤其是 SETNX, SET with NX/PX/EX, Lua 脚本）来实现高性能锁。当然，一般生产中都是使用 Redission 客户端，非常良好地封装了分布式锁的 api，而且支持 RedLock。什么是分布式事务
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: ?在分布式环境下，会涉及到多个数据库，比如说支付库、商品库、订单库。因此要保证跨服务的事务一致性就变得非常复杂。分布式事务其实就是将单一库的事务概念扩大到了多库，目的是为了保证跨服的数据一致性分布式事务有哪些常见的实现方案？二阶段提交（2PC）：通过准备和提交阶段保证一致性，但性能较差。三阶段提交（3PC）：在 2PC 的基础上增加了一个超时机制，降低了阻塞，但依旧存在数据不一致的风险。TCC：根据业务逻辑拆分为 Try、Confirm 和 Cancel 三个阶段，适合锁定资源的业务场景。本地消息表：在数据库中存储事务事件，通过定时任务处理消息。基于 MQ 的分布式事务：通过消息队列来实现异步确保，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。7.1 说说 2PC 两阶段提交？两阶段提交的思路可以概括为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情况决定各参与者是否要提交操作还是回滚操作。准备阶段：事务管理器要求每个涉及到事务的数据库预提交(precommit)此操作，并反映是否可以提交提交阶段：事务协调器要求每个数据库提交数据，或者回滚数据。优点：尽量保证了数据的强一致，实现成本
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 较低，在各大主流数据库都有自己实现，缺点:单点问题：事务管理器在整个流程中扮演的角色很关键，如果其宕机，比如在第一阶段已经完成，在第二阶段正准备提交的时候事务管理器宕机，资源管理器就会一直阻塞，导致数据库无法使用。同步阻塞：在准备就绪之后，资源管理器中的资源一直处于阻塞，直到提交完成，释放资源。数据不一致：两阶段提交协议虽然为分布式数据强一致性所设计，但仍然存在数据不一致性的可能，比如在第二阶段中，假设协调者发出了事务 commit 的通知，但是因为网络问题该通知仅被一部分参与者所收到并执行了 commit 操作，其余的参与者则因为没有收到通知一直处于阻塞状态，这时候就产生了数据的不一致性。3PC（三阶段提交）了解吗？三阶段提交（3PC）是二阶段提交（2PC）的一种改进版本 ，为解决两阶段提交协议的单点故障和同步阻塞问题。三阶段提交有这么三个阶段：CanCommit，PreCommit，DoCommit三个阶段CanCommit：准备阶段。协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。PreCommit：预提交阶段。协调者根据参与者在准备阶段的响应判断是
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 否执行事务还是中断事务，参与者执行完操作之后返回 ACK 响应，同时开始等待最终指令。DoCommit：提交阶段。协调者根据参与者在准备阶段的响应判断是否执行事务还是中断事务：如果所有参与者都返回正确的 ACK 响应，则提交事务如果参与者有一个或多个参与者收到错误的 ACK 响应或者超时，则中断事务可以看出，三阶段提交解决的只是两阶段提交中单体故障和同步阻塞的问题，因为加入了超时机制，这里的超时的机制作用于 预提交阶段 和 提交阶段。如果等待 预提交请求 超时，参与者直接回到准备阶段之前。如果等到提交请求超时，那参与者就会提交事务了。TCC 了解吗？TCC（Try Confirm Cancel） ，是两阶段提交的一个变种，针对每个操作，都需要有一个其对应的确认和取消操作，当操作成功时调用确认操作，当操作失败时调用取消操作，类似于二阶段提交，只不过是这里的提交和回滚是针对业务上的，所以基于 TCC 实现的分布式事务也可以看做是对业务的一种补偿机制。Try：尝试待执行的业务。订单系统将当前订单状态设置为支付中，库存系统校验当前剩余库存数量是否大于 1，然后将可用库存数量设置为库存剩余数量-1，。Confirm：确
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 认执行业务，如果 Try 阶段执行成功，接着执行 Confirm 阶段，将订单状态修改为支付成功，库存剩余数量修改为可用库存数量。Cancel：取消待执行的业务，如果 Try 阶段执行失败，执行 Cancel 阶段，将订单状态修改为支付失败，可用库存数量修改为库存剩余数量TCC 是业务层面的分布式事务，保证最终一致性，不会一直持有资源的锁。优点： 把数据库层的二阶段提交交给应用层来实现，规避了数据库的 2PC 性能低下问题缺点：TCC 的 Try、Confirm 和 Cancel 操作功能需业务提供，开发成本高。TCC对业务的侵入较大和业务紧耦合，需要根据特定的场景和业务逻辑来设计相应的操作本地消息表了解吗？本地消息表的核心思想是将分布式事务拆分成本地事务进行处理。例如，可以在订单库新增一个消息表，将新增订单和新增消息放到一个事务里完成，然后通过轮询的方式去查询消息表，将消息推送到 MQ，库存服务去消费 MQ。执行流程：订单服务，添加一条订单和一条消息，在一个事务里提交订单服务，使用定时任务轮询查询状态为未同步的消息表，发送到 MQ，如果发送失败，就重试发送库存服务，接收 MQ 消息，修改库存表，需要保证幂等
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 操作如果修改成功，调用 rpc 接口修改订单系统消息表的状态为已完成或者直接删除这条消息如果修改失败，可以不做处理，等待重试MQ 消息事务了解吗？基于 MQ 的分布式事务是指将两个事务通过消息队列进行异步解耦，利用重试机制保障最终一致性，适用于对实时性要求不高的场景。订单服务执行自己的本地事务，并发送消息到 MQ，库存服务接收到消息后，执行自己的本地事务，如果消费失败，可以利用重试机制确保最终一致性。延迟队列在分布式事务中通常用于异步补偿、定时校验和故障重试等场景，确保数据最终一致性。当主事务执行完成后，延迟队列会在一定时间后检查各子事务的状态，如果有失败的子事务，可以触发补偿操作，重试或回滚事务。当分布式锁因为某些原因未被正常释放时，可以通过延迟队列在超时后自动释放锁，防止死锁。分布式算法 paxos 了解么 ？Paxos 算法是什么？Paxos 算法是 基于消息传递 且具有 高效容错特性 的一致性算法，目前公认的解决 分布式一致性问题 最有效的算法之一在 Paxos 中有这么几个角色：Proposer（提议者） : 提议者提出提案，用于投票表决。Accecptor（接受者） : 对提案进行投票，并接受达成
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 共识的提案。Learner（学习者） : 被告知投票的结果，接受达成共识的提案Paxos 算法包含两个阶段，第一阶段 Prepare(准备) 、第二阶段 Accept(接受)Prepare(准备)阶段提议者提议一个新的提案 P[Mn,?]，然后向接受者的某个超过半数的子集成员发送编号为 Mn 的准备请求如果一个接受者收到一个编号为 Mn 的准备请求，并且编号 Mn 大于它已经响应的所有准备请求的编号，那么它就会将它已经批准过的最大编号的提案作为响应反馈给提议者，同时该接受者会承诺不会再批准任何编号小于 Mn 的提案总结一下，接受者在收到提案后，会给与提议者两个承诺与一个应答：两个承诺：承诺不会再接受提案号小于或等于 Mn 的 Prepare 请求承诺不会再接受提案号小于 Mn 的 Accept 请求一个应答：不违背以前作出的承诺的前提下，回复已经通过的提案中提案号最大的那个提案所设定的值和提案号 Mmax，如果这个值从来没有被任何提案设定过，则返回空值。如果不满足已经做出的承诺，即收到的提案号并不是决策节点收到过的最大的，那允许直接对此 Prepare 请求不予理会。Accept(接受)阶段如果提议者收到来自
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 半数以上的接受者对于它发出的编号为 Mn 的准备请求的响应，那么它就会发送一个针对[Mn,Vn]的接受请求给接受者，注意 Vn 的值就是收到的响应中编号最大的提案的值，如果响应中不包含任何提案，那么它可以随意选定一个值。如果接受者收到这个针对[Mn,Vn]提案的接受请求，只要该接受者尚未对编号大于 Mn 的准备请求做出响应，它就可以通过这个提案。当提议者收到了多数接受者的接受应答后，协商结束，共识决议形成，将形成的决议发送给所有学习节点进行学习Paxos 算法有什么缺点吗？怎么优化？前面描述的可以称之为 Basic Paxos 算法，在单提议者的前提下是没有问题的，但是假如有多个提议者互不相让，那么就可能导致整个提议的过程进入了死循环简单说就是在多个提议者的情况下，选出一个 Leader（领导者），由领导者作为唯一的提议者，这样就可以解决提议者冲突的问题笔试题1 为什么使用消息队列？消息队列（Message Queue，简称 MQ）是一种跨进程的通信机制，用于上下游传递消息。它在现代分布式系统中扮演着重要的角色，主要用于系统间的解耦、异步消息处理以及流量削峰。消息队列的使用场景解耦在没有消息队列的系统中，如果
2025-08-11 10:56:08.181 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 一个系统需要与多个系统交互，它们之间的耦合度会非常高。例如，系统 A直接调用系统 B 和 C的接口，如果未来需要接入系统 D或取消 B 系统，系统 A 需要修改代码，这增加了系统的风险。使用消息队列后，系统 A 只需将消息推送到队列，其他系统根据需要从队列中订阅消息。这样，系统 A 不需要做任何修改，也不需要考虑下游消费失败的情况，从而实现了系统间的解耦。异步处理在同步操作中，一些非关键的业务逻辑可能会消耗大量时间，导致用户体验不佳。例如，系统 A 在处理一个请求时，需要在多个系统中进行操作，这可能导致总延迟增加。通过使用消息队列，系统 A 可以将消息写入队列，而其他业务逻辑可以异步执行，从而显著减少总耗时。流量削峰对于面临突发流量的系统，如果直接将所有请求发送到数据库，可能会导致数据库连接异常或系统崩溃。消息队列可以帮助系统按照下游系统的处理能力从队列中慢慢拉取消息，从而避免因突发流量导致的系统崩溃。消息队列的优缺点优点解耦：使得系统间的依赖关系最小化，降低系统间的耦合度。异步处理：提高系统的响应速度和吞吐量。流量削峰：使系统能够应对高流量压力，避免系统因突发流量而崩溃2.简述数据库的事务，说出事务的特点？
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 在数据库管理系统中，事务是一个非常重要的概念，它指的是一系列的数据库操作，这些操作要么全部成功，要么全部失败，确保数据的完整性和一致性。事务的四大特性通常被称为 ACID属性，分别是原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability）。原子性（Atomicity）原子性确保事务中的所有操作要么全部成功，要么全部失败回滚，不会出现只执行了部分操作的情况。这意味着事务是一个不可分割的工作单位，例如，在银行转账的场景中，转账操作需要同时更新两个账户的余额，这两个操作必须要么都执行，要么都不执行一致性（Consistency）一致性意味着数据库在事务开始之前和结束之后，都必须保持一致状态。事务不会破坏数据的完整性和业务规则。例如，如果一个转账事务在执行过程中系统崩溃，事务没有提交，那么事务中所做的修改也不会保存到数据库中，保证了数据的一致性隔离性（Isolation）隔离性保证了当多个用户并发访问数据库时，数据库系统能够为每个用户的事务提供一个独立的运行环境，事务之间不会互相干扰。例如，当一个事务正在处理数据时，其他事务必须等待，直到该事务完成，
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 才能访问同样的数据持久性（Durability）持久性确保一旦事务提交，它对数据库的改变就是永久性的。即使发生系统故障，事务的结果也不会丢失。例如，一旦银行转账事务提交，转账的金额就会永久地反映在各个账户的余额中事务的四大特性是数据库管理系统设计的基础，它们确保了数据库操作的安全性和可靠性，使得用户可以信赖数据库处理复杂的业务逻辑。3. SOA 和微服务之间的区别？SOA（面向服务的架构）[&和微服务架构&]是两种常见的软件架构设计方法，它们在服务划分、通信方式和应用场景等方面存在显著差异。SOA 的特点 SOA 是一种高层次的架构设计理念，旨在通过服务接口实现系统间的松耦合和功能复用。服务通过企业服务总线（ESB）进行通信，ESB负责消息路由、协议转换和服务集成。SOA 的服务粒度较粗，适用于复杂的企业级系统，尤其是需要集成异构系统的场景。微服务的特点 微服务架构是对 SOA 的进一步演进，强调将单一业务系统拆分为多个独立的小型服务。每个服务独立开发、部署和运行，通常通过轻量级协议（如 HTTP/REST）进行通信。微服务更注重快速交付和自动化运维，适合快速变化的互联网系统。主要区别服务粒度：SOA 的服务
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 粒度较粗，通常是一个完整的业务模块；微服务的服务粒度较细，专注于单一功能。通信方式：SOA 依赖 ESB 进行服务间通信，支持多种协议；微服务使用轻量级协议（如 HTTP/REST），去掉了 ESB。部署方式：SOA 通常整体部署，微服务则支持独立部署，便于快速迭代。应用场景：SOA 适用于复杂的企业级系统，微服务更适合轻量级、基于 Web 的系统。总结 SOA 和微服务各有优劣，选择哪种架构取决于具体的业务需求和系统复杂性。SOA 更适合需要集成异构系统的大型企业应用，而微服务则适合快速变化的互联网应用。4.深拷贝和浅拷贝的区别？在编程中，深复制和浅复制是两种不同的对象复制方式。它们主要用于处理对象和数组等引用数据类型。浅复制浅复制只复制对象的引用，而不复制对象本身。也就是说，新旧对象共享同一块内存空间，对其中一个对象的修改会影响到另一个对象。浅复制适用于对象的属性是基本数据类型的情况，但如果属性是引用类型，则会出现共享内存的问题。深复制深复制会创建一个新的对象，并递归复制所有层级的属性和数组元素。新对象与原对象不共享内存，修改新对象不会影响到原对象。深复制适用于需要完全独立的对象副本的情况。5.有了关系型
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 数据库，为什么还需要 NOSQL 数据库？关系型数据库（RDBMS）在数据存储和管理方面表现出色，但在某些场景下存在局限性。随着互联网和大数据时代的到来，传统的关系型数据库在处理大量非结构化和半结构化数据时显得力不从心。为了解决这些问题，非关系型数据库（NoSQL）应运而生。关系型数据库的局限性扩展性：关系型数据库通常采用垂直扩展（Scale-Up）的方式，通过增加硬件资源来提升性能。然而，这种方式在高并发、大数据量的情况下成本高昂且效果有限。灵活性：关系型数据库要求预先定义数据模式（Schema），在需求频繁变化的应用场景中显得不够灵活。每次修改数据模式都需要停机或复杂的迁移操作。性能：在高并发读写、大规模数据处理的情况下，关系型数据库的性能可能无法满足需求，特别是在分布式环境下。成本：关系型数据库通常需要昂贵的硬件和专业的维护团队，对于中小型企业和初创公司来说，成本压力较大。非关系型数据库的优势水平扩展（Scale-Out）：NoSQL 数据库通常设计为支持水平扩展，通过增加更多的服务器节点来提升性能。这种方式在大规模数据和高并发场景下非常有效。灵活的数据模型：NoSQL数据库通常采用无模式（Schema
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: -less）或弱模式（Schema-flexible）的设计，允许数据以更灵活的方式存储。高性能：NoSQL 数据库针对特定的应用场景进行了优化，通常具有更高的读写性能。低成本：NoSQL数据库通常采用开源软件，硬件要求较低，适合在云环境中部署，降低了总体拥有成本（TCO）。分布式架构：NoSQL 数据库通常采用分布式架构，能够更好地处理大规模数据和高并发请求。关系型数据库与非关系型数据库的对比数据模型：关系型数据库以表格形式存储数据，而 NoSQL数据库以键值对、文档、列族或图的形式存储数据。扩展性：关系型数据库采用垂直扩展，而 NoSQL数据库采用水平扩展。数据一致性：关系型数据库强调强一致性（ACID），而 NoSQL 数据库通常采用最终一致性（BASE）。数据模式：关系型数据库需要固定模式，而 NoSQL 数据库通常无模式或弱模式。性能：关系型数据库适合事务处理和小规模数据，而 NoSQL数据库适合大规模数据和高并发。成本：关系型数据库成本较高，而 NoSQL数据库成本较低。结论NoSQL数据库的出现并不是为了取代关系型数据库，而是为了填补关系型数据库在某些场景下的不足。NoSQL 数据库提供了更高的
2025-08-11 10:56:08.182 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG com.yizhaoqi.smartpai.service.ParseService - 文本块: 扩展性、灵活性和性能，适合处理大规模数据和高并发请求。然而，关系型数据库在事务处理和企业应用中仍然具有不可替代的优势。
2025-08-11 10:56:08.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.service.ParseService - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:56:08.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.y.smartpai.consumer.FileProcessingConsumer - 文件解析完成，fileMd5: c8f8cebf90c764b93d862694096a2af9
2025-08-11 10:56:08.711 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  c.yizhaoqi.smartpai.service.VectorizationService - 开始向量化文件，fileMd5: c8f8cebf90c764b93d862694096a2af9, userId: 1, orgTag: PRIVATE_sy, isPublic: true
2025-08-11 10:56:08.734 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  com.yizhaoqi.smartpai.client.EmbeddingClient - 开始生成向量，文本数量: 1190
2025-08-11 10:56:08.735 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:08.736 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:08.982 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] [10d4b7b0-17] Response 400 BAD_REQUEST
2025-08-11 10:56:08.983 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] [10d4b7b0-17] Read 312 bytes
2025-08-11 10:56:09.985 [parallel-13] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:09.987 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:10.202 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] [10d4b7b0-18] Response 400 BAD_REQUEST
2025-08-11 10:56:10.203 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] [10d4b7b0-18] Read 312 bytes
2025-08-11 10:56:11.212 [parallel-14] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:11.213 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:11.422 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] [10d4b7b0-19] Response 400 BAD_REQUEST
2025-08-11 10:56:11.422 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] [10d4b7b0-19] Read 312 bytes
2025-08-11 10:56:12.435 [parallel-15] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] HTTP POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
2025-08-11 10:56:12.436 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] Encoding [{input=[community-社区论坛项目介绍一个基本功能完整的论坛项目。项目主要功能有：基于邮件激活的注册方式，基于 MD5 加密与加盐的密码存储方式，登陆功能加入了随机验证码的验证。实现登陆 (truncated)...]
2025-08-11 10:56:12.626 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] [10d4b7b0-20] Response 400 BAD_REQUEST
2025-08-11 10:56:12.626 [reactor-http-nio-2] DEBUG org.springframework.web.HttpLogging - [566b6efc] [10d4b7b0-20] Read 312 bytes
2025-08-11 10:56:12.627 [reactor-http-nio-2] DEBUG o.s.web.reactive.function.client.ExchangeFunctions - [566b6efc] Cancel signal (to close connection)
2025-08-11 10:56:12.627 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR com.yizhaoqi.smartpai.client.EmbeddingClient - 调用向量化 API 失败: Retries exhausted: 3/3
reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:12.630 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.yizhaoqi.smartpai.service.VectorizationService - 向量化失败，fileMd5: c8f8cebf90c764b93d862694096a2af9
java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 10:56:12.631 [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] ERROR c.y.smartpai.consumer.FileProcessingConsumer - Error processing task: FileProcessingTask(fileMd5=c8f8cebf90c764b93d862694096a2af9, filePath=http://localhost:9000/uploads/merged/%E7%89%9B%E5%AE%A2%E8%AE%BA%E5%9D%9B%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=minioadmin%2F20250811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250811T025536Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=cb3c8633c0c35116dced7e19a995ca636c7db533b76f233c28dd243c08e04d3f, fileName=牛客论坛项目总结.pdf, userId=1, orgTag=PRIVATE_sy, isPublic=true)
java.lang.RuntimeException: 向量化失败
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:79)
	at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
	at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
	at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
	at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
	at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
	at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
	at io.micrometer.observation.Observation.observe(Observation.java:564)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
	at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
	at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.RuntimeException: 向量生成失败
	at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:62)
	at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
	... 26 common frames omitted
Caused by: reactor.core.Exceptions$RetryExhaustedException: Retries exhausted: 3/3
	at reactor.core.Exceptions.retryExhausted(Exceptions.java:308)
	at reactor.util.retry.RetryBackoffSpec.lambda$static$0(RetryBackoffSpec.java:68)
	at reactor.util.retry.RetryBackoffSpec.lambda$null$4(RetryBackoffSpec.java:608)
	at reactor.core.publisher.FluxConcatMapNoPrefetch$FluxConcatMapNoPrefetchSubscriber.onNext(FluxConcatMapNoPrefetch.java:183)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.SinkManyEmitterProcessor.drain(SinkManyEmitterProcessor.java:476)
	at reactor.core.publisher.SinkManyEmitterProcessor.tryEmitNext(SinkManyEmitterProcessor.java:273)
	at reactor.core.publisher.SinkManySerialized.tryEmitNext(SinkManySerialized.java:100)
	at reactor.core.publisher.InternalManySink.emitNext(InternalManySink.java:27)
	at reactor.core.publisher.FluxRetryWhen$RetryWhenMainSubscriber.onError(FluxRetryWhen.java:194)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onError(Operators.java:2236)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onError(FluxOnAssembly.java:544)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onError(MonoFlatMap.java:180)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:106)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onError(FluxOnErrorResume.java:103)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.secondError(MonoFlatMap.java:241)
	at reactor.core.publisher.MonoFlatMap$FlatMapInner.onError(MonoFlatMap.java:315)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onError(MonoIgnoreThen.java:280)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:232)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.onComplete(MonoIgnoreThen.java:204)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onComplete(FluxOnErrorReturn.java:169)
	at reactor.core.publisher.MonoIgnoreElements$IgnoreElementsSubscriber.onComplete(MonoIgnoreElements.java:89)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onComplete(FluxMapFuseable.java:152)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onComplete(FluxOnAssembly.java:549)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.Operators.complete(Operators.java:137)
	at reactor.netty.channel.FluxReceive.startReceiver(FluxReceive.java:179)
	at reactor.netty.channel.FluxReceive.subscribe(FluxReceive.java:145)
	at reactor.core.publisher.InternalFluxOperator.subscribe(InternalFluxOperator.java:68)
	at reactor.netty.ByteBufFlux.subscribe(ByteBufFlux.java:340)
	at reactor.core.publisher.Mono.subscribe(Mono.java:4576)
	at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.subscribeNext(MonoIgnoreThen.java:265)
	at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:51)
	at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:165)
	at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:122)
	at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
	at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
	at reactor.core.publisher.FluxOnAssembly$OnAssemblySubscriber.onNext(FluxOnAssembly.java:539)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
	at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
	at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
	at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
	at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
	at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
	at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
	at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
	at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
	at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
	at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
	at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
	at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 common frames omitted
	Suppressed: java.lang.Exception: #block terminated with an error
		at reactor.core.publisher.BlockingSingleSubscriber.blockingGet(BlockingSingleSubscriber.java:146)
		at reactor.core.publisher.Mono.block(Mono.java:1807)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.callApiOnce(EmbeddingClient.java:80)
		at com.yizhaoqi.smartpai.client.EmbeddingClient.embed(EmbeddingClient.java:55)
		at com.yizhaoqi.smartpai.service.VectorizationService.vectorize(VectorizationService.java:57)
		at com.yizhaoqi.smartpai.consumer.FileProcessingConsumer.processTask(FileProcessingConsumer.java:61)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
		at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
		at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.base/java.lang.reflect.Method.invoke(Method.java:569)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:169)
		at org.springframework.kafka.listener.adapter.KotlinAwareInvocableHandlerMethod.doInvoke(KotlinAwareInvocableHandlerMethod.java:45)
		at org.springframework.messaging.handler.invocation.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:119)
		at org.springframework.kafka.listener.adapter.HandlerAdapter.invoke(HandlerAdapter.java:70)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invokeHandler(MessagingMessageListenerAdapter.java:420)
		at org.springframework.kafka.listener.adapter.MessagingMessageListenerAdapter.invoke(MessagingMessageListenerAdapter.java:384)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:85)
		at org.springframework.kafka.listener.adapter.RecordMessagingMessageListenerAdapter.onMessage(RecordMessagingMessageListenerAdapter.java:50)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeOnMessage(KafkaMessageListenerContainer.java:2800)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeOnMessage(KafkaMessageListenerContainer.java:2778)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.lambda$doInvokeRecordListener$53(KafkaMessageListenerContainer.java:2701)
		at io.micrometer.observation.Observation.observe(Observation.java:564)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeRecordListener(KafkaMessageListenerContainer.java:2699)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.doInvokeWithRecords(KafkaMessageListenerContainer.java:2541)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeRecordListener(KafkaMessageListenerContainer.java:2430)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeListener(KafkaMessageListenerContainer.java:2085)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.invokeIfHaveRecords(KafkaMessageListenerContainer.java:1461)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.pollAndInvoke(KafkaMessageListenerContainer.java:1426)
		at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.run(KafkaMessageListenerContainer.java:1296)
		at java.base/java.util.concurrent.CompletableFuture$AsyncRun.run(CompletableFuture.java:1804)
		... 1 common frames omitted
Caused by: org.springframework.web.reactive.function.client.WebClientResponseException$BadRequest: 400 Bad Request from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings
	at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
	Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: 
Error has been observed at the following site(s):
	*__checkpoint ⇢ 400 BAD_REQUEST from POST https://dashscope.aliyuncs.com/compatible-mode/v1/embeddings [DefaultWebClient]
Original Stack Trace:
		at org.springframework.web.reactive.function.client.WebClientResponseException.create(WebClientResponseException.java:321)
		at org.springframework.web.reactive.function.client.DefaultClientResponse.lambda$createException$1(DefaultClientResponse.java:214)
		at reactor.core.publisher.FluxMap$MapSubscriber.onNext(FluxMap.java:106)
		at reactor.core.publisher.FluxOnErrorReturn$ReturnSubscriber.onNext(FluxOnErrorReturn.java:162)
		at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:122)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:129)
		at reactor.core.publisher.FluxContextWrite$ContextWriteSubscriber.onNext(FluxContextWrite.java:107)
		at reactor.core.publisher.FluxMapFuseable$MapFuseableConditionalSubscriber.onNext(FluxMapFuseable.java:299)
		at reactor.core.publisher.FluxFilterFuseable$FilterFuseableConditionalSubscriber.onNext(FluxFilterFuseable.java:337)
		at reactor.core.publisher.Operators$BaseFluxToMonoOperator.completePossiblyEmpty(Operators.java:2097)
		at reactor.core.publisher.MonoCollect$CollectSubscriber.onComplete(MonoCollect.java:145)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.core.publisher.FluxPeek$PeekSubscriber.onComplete(FluxPeek.java:260)
		at reactor.core.publisher.FluxMap$MapSubscriber.onComplete(FluxMap.java:144)
		at reactor.netty.channel.FluxReceive.onInboundComplete(FluxReceive.java:413)
		at reactor.netty.channel.ChannelOperations.onInboundComplete(ChannelOperations.java:455)
		at reactor.netty.channel.ChannelOperations.terminate(ChannelOperations.java:509)
		at reactor.netty.http.client.HttpClientOperations.onInboundNext(HttpClientOperations.java:819)
		at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:115)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:107)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436)
		at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:346)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:318)
		at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:442)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1515)
		at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1378)
		at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1427)
		at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:530)
		at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:469)
		at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:290)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)
		at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1357)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)
		at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)
		at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:868)
		at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
		at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)
		at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)
		at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)
		at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
		at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
		at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
		at java.base/java.lang.Thread.run(Thread.java:840)
2025-08-11 11:05:42.407 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/users/login
2025-08-11 11:05:42.408 [http-nio-8081-exec-10] DEBUG o.s.s.w.a.AnonymousAuthenticationFilter - Set SecurityContextHolder to anonymous SecurityContext
2025-08-11 11:05:42.408 [http-nio-8081-exec-10] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/users/login
2025-08-11 11:05:42.408 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/users/login", parameters={}
2025-08-11 11:05:42.408 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#login(UserRequest)
2025-08-11 11:05:42.410 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.a.RequestResponseBodyMethodProcessor - Read "application/json;charset=UTF-8" to [UserRequest[username=testuser, password=test123]]
2025-08-11 11:05:42.415 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:05:42.416 [http-nio-8081-exec-10] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=401, message=Invalid username or password}]
2025-08-11 11:05:42.416 [http-nio-8081-exec-10] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 401 UNAUTHORIZED
2025-08-11 11:06:09.877 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Securing POST /api/v1/users/login
2025-08-11 11:06:09.878 [http-nio-8081-exec-2] DEBUG o.s.s.w.a.AnonymousAuthenticationFilter - Set SecurityContextHolder to anonymous SecurityContext
2025-08-11 11:06:09.878 [http-nio-8081-exec-2] DEBUG org.springframework.security.web.FilterChainProxy - Secured POST /api/v1/users/login
2025-08-11 11:06:09.878 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - POST "/api/v1/users/login", parameters={}
2025-08-11 11:06:09.878 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#login(UserRequest)
2025-08-11 11:06:09.878 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.a.RequestResponseBodyMethodProcessor - Read "application/json;charset=UTF-8" to [UserRequest[username=admin, password=admin123]]
2025-08-11 11:06:09.970 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Token cached: d4d97c1a246444a692370d7a2867ed3a for user: admin
2025-08-11 11:06:09.970 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Token generated and cached for user: admin, tokenId: d4d97c1a246444a692370d7a2867ed3a
2025-08-11 11:06:09.974 [http-nio-8081-exec-2] DEBUG com.yizhaoqi.smartpai.service.TokenCacheService - Refresh token cached: 0521146d60ef450daaa695ab267018ac for user: 1
2025-08-11 11:06:09.974 [http-nio-8081-exec-2] INFO  com.yizhaoqi.smartpai.utils.JwtUtils - Refresh token generated and cached for user: admin, refreshTokenId: 0521146d60ef450daaa695ab267018ac
2025-08-11 11:06:09.975 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:06:09.975 [http-nio-8081-exec-2] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={token=eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIi (truncated)...]
2025-08-11 11:06:09.975 [http-nio-8081-exec-2] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:06:10.287 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 11:06:10.297 [http-nio-8081-exec-1] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 11:06:10.297 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 11:06:10.297 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 11:06:10.303 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:06:10.303 [http-nio-8081-exec-1] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 11:06:10.305 [http-nio-8081-exec-1] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:06:10.551 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 11:06:10.560 [http-nio-8081-exec-3] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 11:06:10.560 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 11:06:10.560 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 11:06:10.563 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:06:10.563 [http-nio-8081-exec-3] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 11:06:10.564 [http-nio-8081-exec-3] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:06:10.630 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 11:06:10.636 [http-nio-8081-exec-4] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12
2025-08-11 11:06:10.636 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/conversation?start_date=2025-08-04&end_date=2025-08-12", parameters={masked}
2025-08-11 11:06:10.636 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.ConversationController#getConversations(String, String, String)
2025-08-11 11:06:10.645 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:06:10.645 [http-nio-8081-exec-4] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{code=200, data=[{role=user, content=解释一下社区论坛项目, timestamp=2025-08-10T10:40:56}, {role=assistant, co (truncated)...]
2025-08-11 11:06:10.646 [http-nio-8081-exec-4] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:18:56.698 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/users/me
2025-08-11 11:18:56.703 [http-nio-8081-exec-5] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/users/me
2025-08-11 11:18:56.703 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/users/me", parameters={}
2025-08-11 11:18:56.704 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.UserController#getCurrentUser(String)
2025-08-11 11:18:56.706 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:18:56.706 [http-nio-8081-exec-5] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={id=1, username=admin, role=ADMIN, orgTags=[default, admin], primaryOrg=default, createdAt=202 (truncated)...]
2025-08-11 11:18:56.707 [http-nio-8081-exec-5] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:18:56.961 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 11:18:56.964 [http-nio-8081-exec-6] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 11:18:56.964 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 11:18:56.965 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 11:18:56.971 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:18:56.973 [http-nio-8081-exec-6] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 11:18:56.974 [http-nio-8081-exec-6] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:18:57.038 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 11:18:57.038 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Securing GET /api/v1/admin/org-tags/tree
2025-08-11 11:18:57.044 [http-nio-8081-exec-8] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/users/list?page=1&size=20
2025-08-11 11:18:57.044 [http-nio-8081-exec-7] DEBUG org.springframework.security.web.FilterChainProxy - Secured GET /api/v1/admin/org-tags/tree
2025-08-11 11:18:57.044 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/org-tags/tree", parameters={}
2025-08-11 11:18:57.044 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - GET "/api/v1/admin/users/list?page=1&size=20", parameters={masked}
2025-08-11 11:18:57.044 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getUserList(String, String, String, Integer, int, int)
2025-08-11 11:18:57.044 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.a.RequestMappingHandlerMapping - Mapped to com.yizhaoqi.smartpai.controller.AdminController#getOrganizationTagTree(String)
2025-08-11 11:18:57.049 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:18:57.049 [http-nio-8081-exec-7] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data=[{tagId=admin, parentTag=null, name=管理员组织, description=管理员专用组织标签，具有管理权限}, {tagId=default, pare (truncated)...]
2025-08-11 11:18:57.049 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Using 'application/json', given [application/json, text/plain, */*] and supported [application/json, application/*+json]
2025-08-11 11:18:57.049 [http-nio-8081-exec-8] DEBUG o.s.w.s.m.m.annotation.HttpEntityMethodProcessor - Writing [{data={number=1, size=20, totalPages=1, content=[{primaryOrg=PRIVATE_sy, createdAt=2025-08-09T22:50: (truncated)...]
2025-08-11 11:18:57.051 [http-nio-8081-exec-8] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 11:18:57.051 [http-nio-8081-exec-7] DEBUG org.springframework.web.servlet.DispatcherServlet - Completed 200 OK
2025-08-11 20:55:40.436 [SpringApplicationShutdownHook] INFO  o.s.boot.web.embedded.tomcat.GracefulShutdown - Commencing graceful shutdown. Waiting for active requests to complete
2025-08-11 20:55:40.609 [tomcat-shutdown] INFO  o.s.boot.web.embedded.tomcat.GracefulShutdown - Graceful shutdown complete
2025-08-11 20:55:42.808 [SpringApplicationShutdownHook] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown initiated...
2025-08-11 20:55:42.811 [SpringApplicationShutdownHook] INFO  com.zaxxer.hikari.HikariDataSource - HikariPool-1 - Shutdown completed.
2025-08-11 20:55:42.820 [SpringApplicationShutdownHook] DEBUG o.s.w.s.handler.LoggingWebSocketHandlerDecorator - Transport error in StandardWebSocketSession[id=cd209248-acc0-e8ce-b521-37f7a1d71fa2, uri=ws://localhost:8081/chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM]
java.io.IOException: java.nio.channels.ClosedChannelException
	at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.sendMessageBlockInternal(WsRemoteEndpointImplBase.java:326)
	at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.sendMessageBlock(WsRemoteEndpointImplBase.java:266)
	at org.apache.tomcat.websocket.WsSession.sendCloseMessage(WsSession.java:792)
	at org.apache.tomcat.websocket.WsSession.doClose(WsSession.java:589)
	at org.apache.tomcat.websocket.WsSession.doClose(WsSession.java:556)
	at org.apache.tomcat.websocket.WsSession.close(WsSession.java:544)
	at org.apache.tomcat.websocket.WsWebSocketContainer.destroy(WsWebSocketContainer.java:1024)
	at org.apache.tomcat.websocket.server.WsContextListener.contextDestroyed(WsContextListener.java:46)
	at org.apache.catalina.core.StandardContext.listenerStop(StandardContext.java:4052)
	at org.apache.catalina.core.StandardContext.stopInternal(StandardContext.java:4662)
	at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:235)
	at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1219)
	at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1208)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.stopInternal(ContainerBase.java:814)
	at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:235)
	at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1219)
	at org.apache.catalina.core.ContainerBase$StopChild.call(ContainerBase.java:1208)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at org.apache.tomcat.util.threads.InlineExecutorService.execute(InlineExecutorService.java:75)
	at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:145)
	at org.apache.catalina.core.ContainerBase.stopInternal(ContainerBase.java:814)
	at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:235)
	at org.apache.catalina.core.StandardService.stopInternal(StandardService.java:471)
	at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:235)
	at org.apache.catalina.core.StandardServer.stopInternal(StandardServer.java:915)
	at org.apache.catalina.util.LifecycleBase.stop(LifecycleBase.java:235)
	at org.apache.catalina.startup.Tomcat.stop(Tomcat.java:447)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.stopTomcat(TomcatWebServer.java:301)
	at org.springframework.boot.web.embedded.tomcat.TomcatWebServer.destroy(TomcatWebServer.java:374)
	at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.doClose(ServletWebServerApplicationContext.java:177)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:1126)
	at org.springframework.boot.SpringApplicationShutdownHook.closeAndWait(SpringApplicationShutdownHook.java:147)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at org.springframework.boot.SpringApplicationShutdownHook.run(SpringApplicationShutdownHook.java:116)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.nio.channels.ClosedChannelException: null
	at org.apache.tomcat.util.net.NioChannel$1.write(NioChannel.java:273)
	at org.apache.tomcat.util.net.NioEndpoint$NioSocketWrapper$NioOperationState.run(NioEndpoint.java:1632)
	at org.apache.tomcat.util.net.SocketWrapperBase$OperationState.start(SocketWrapperBase.java:1051)
	at org.apache.tomcat.util.net.SocketWrapperBase.vectoredOperation(SocketWrapperBase.java:1440)
	at org.apache.tomcat.util.net.SocketWrapperBase.write(SocketWrapperBase.java:1366)
	at org.apache.tomcat.util.net.SocketWrapperBase.write(SocketWrapperBase.java:1337)
	at org.apache.tomcat.websocket.server.WsRemoteEndpointImplServer.doWrite(WsRemoteEndpointImplServer.java:171)
	at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.writeMessagePart(WsRemoteEndpointImplBase.java:521)
	at org.apache.tomcat.websocket.WsRemoteEndpointImplBase.sendMessageBlockInternal(WsRemoteEndpointImplBase.java:313)
	... 37 common frames omitted
2025-08-11 20:55:42.822 [SpringApplicationShutdownHook] DEBUG o.s.w.s.handler.LoggingWebSocketHandlerDecorator - StandardWebSocketSession[id=cd209248-acc0-e8ce-b521-37f7a1d71fa2, uri=ws://localhost:8081/chat/eyJhbGciOiJIUzI1NiJ9.eyJwcmltYXJ5T3JnIjoiZGVmYXVsdCIsIm9yZ1RhZ3MiOiJkZWZhdWx0LGFkbWluIiwicm9sZSI6IkFETUlOIiwidG9rZW5JZCI6ImFiNWM5YzlkYzg3YzRiOWZhYThhYjMzODhhNTBiMjQwIiwidXNlcklkIjoiMSIsInN1YiI6ImFkbWluIiwiZXhwIjoxNzU0ODg0NDEzfQ.tcwfeyrvH5GB67cS8dTnTDlHM-C9WP6KWW3UyFM0PIM] closed with CloseStatus[code=1001, reason=The web application is stopping]
2025-08-11 20:55:42.827 [SpringApplicationShutdownHook] INFO  c.yizhaoqi.smartpai.handler.ChatWebSocketHandler - WebSocket连接已关闭，用户ID: admin，会话ID: cd209248-acc0-e8ce-b521-37f7a1d71fa2，状态: CloseStatus[code=1001, reason=The web application is stopping]
